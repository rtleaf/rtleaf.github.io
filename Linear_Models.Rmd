---
output:
  html_document:
    toc: yes
    toc_depth: 1
    toc_float: yes
    number_sections: true
---

# Linear Regression

"Simple" linear regression with has a single predictor variable

A critical need is for us to understand the fit of a regression model, to do this we will evaluate the following quantities:

+ Total sum of squares
+ Model sum of squares
+ Residual sum of squares
+ R^2^

We will use these quantities to understand two things: 

1. Is $\beta_1 = 0$ (using the confidence intervals)? We would like to know if *X* is a good predictor of *Y* and this is determined by evaluating the null hypothesis, $\beta_1 = 0$.

2. Is the model significant (using an *F*-test)? We would like to know: Does the regression model fit the data better than the null model (more on this below).

## What is Regression?

+ A way of predicting the value of one variable from another.

+ It is a model of the relationship between two variables.

+ The modeled relationship is linear

+ Therefore, we describe the relationship using the equation of a straight line.

## Assumptions of Simple Linear Regression

+ For each value of *X*, *Y* are randomly sampled and independent.

+ For any value of *X* in the pop'l there exists a normal distribution of Y values

+ There is homogeneity of variance in the population. i.e. the variance of the normal distribution of *Y* values in population are equal for all of values of *X*.

+ X is measured without error

## Model for Linear Relationship

$Y_i=\beta_0 + \beta_1X_i + \varepsilon_i$

$\beta_1$

+ Regression coefficient for the predictor

+ Gradient (slope) of the regression line

+ Direction/strength of relationship

$\beta_0$

+ Intercept (value of *Y* when *X* = 0)

+ Point at which the regression line crosses the *Y*-axis (ordinate)

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,2,4)) # This tells R to put 1 row, 2 columns
x <- c(0,4,5,8,11,14)
y <- c(7,19,22,31,40,49)
plot(x,y, main = "(b) An example of a linear equation", xlab = "Variable X (arbitrary units)", ylab = "Variable y (arbitrary units)", xlim = c(0,20), ylim = c(0,60), type = 'l', pch=16, xaxt = "n", yaxt = "n",lwd = 2)
segments(x0 = 5, y0 = 22, x1 = 11, y1 = 22)
segments(x0 = 11, y0 = 22, x1 = 11, y1 = 40)
par(new=T)
box()
#needs formulas and labels of points

```

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,2,4)) # This tells R to put 1 row, 2 columns
x <- c(0,4,5,8,11,14,16)
y <- c(7,19,22,31,40,49,55)
plot(x,y, main = "(b) An example of a linear equation", xlab = "Variable x (arbitrary units)", ylab = "Variable y (arbitrary units)", xlim = c(0,20), ylim = c(0,60), type = 'b', pch=16)
segments(x0 = 0, y0 = 31, x1 = 8, y1 = 31, lty = 2)
segments(x0 = 8, y0 = 0, x1 = 8, y1 = 31, lty = 2)
segments(x0 = 11, y0 = 40, x1 = 14, y1 = 40, lty = 2)
segments(x0 = 14, y0 = 49, x1 = 14, y1 = 40, lty = 2)
par(new=T)
box()
#needs formulas and labels of points

```

## Intercepts and Gradients

$Y_i=\beta_0 + \beta_1X_i + \varepsilon_i$

```{r, results='hide',message=FALSE, echo=FALSE}
x1 <- c(0,3,8,12,15)
y1 <- c(0.71,1.97,4.07,5.75,7.01)
x2 <- c(0,3,8,12,16)
y2 <- c(0.71,1.52,2.87,3.95,5.03)
x3 <- c(0,3,8,12,17)
y3 <- c(0.71,0.59,0.39,0.23,0.03)

plot(x1, y1, xlim = c(0,18), ylim = c(0,8), pch = 16, type = 'l', xlab = "X", ylab = "Y")
lines(x2, y2, xlim = c(0,18), ylim = c(0,8), pch = 16, type = 'l', xlab = "X", ylab = "Y")
lines(x3, y3, xlim = c(0,18), ylim = c(0,8), pch = 16, type = 'l', xlab = "X", ylab = "Y")
#Equations needed on lines
```


```{r, results='hide',message=FALSE, echo=FALSE}
x1 <- c(0,2,4,6,8,10,12)
y1 <- c(70,75,80,85,90,95,100)
x2 <- c(0,2,4,6,8,10,12)
y2 <- c(40,45,50,55,60,65,70)
x3 <- c(0,2,4,6,8,10,12)
y3 <- c(25,30,35,40,45,50,55)

plot(x1, y1, xlim = c(0,12), ylim = c(0,100), pch = 16, type = 'l', xlab = "Predictor", ylab = "Outcome", xaxt = "n")
lines(x2, y2, xlim = c(0,12), ylim = c(0,100), pch = 16, type = 'l', xlab = "Predictor", ylab = "Outcome", xaxt = "n")
lines(x3, y3, xlim = c(0,12), ylim = c(0,100), pch = 16, type = 'l', xlab = "Predictor", ylab = "Outcome", xaxt = "n")

```

## The Method of Least Squares

This is the approach we will use for parameter estimation and inference in this course.

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(2,3,4,6,7,8,10,11,14,15,17,18,20,21,23)
y <- c(5,10,7,11,20,13,15,30,27,37,35,30,32,35,40)

plot(x, y, xlim = c(0,25), ylim = c(0,45), pch = 1, xlab = "Size of Spider", ylab = "Anxiety (GSR)")
segments(x0 = 0, y0 = 3, x1 = 23, y1 = 41)

```

This figure shows a scatterplot of some data with a line representing the general trend. The vertical lines (dotted) represent the differences (or residuals) between the line (predictions) and the observations. 


## Sums of Squares Components

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(2,2), mar = c(3,3,3,3)) 
x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
segments(x0 = 1, y0 = 2, x1 = 1, y1 = 10, lty = 2)
segments(x0 = 2, y0 = 8, x1 = 2, y1 = 10, lty = 2)
segments(x0 = 3, y0 = 5, x1 = 3, y1 = 10, lty = 2)
segments(x0 = 4, y0 = 8, x1 = 4, y1 = 10, lty = 2)
segments(x0 = 5, y0 = 12, x1 = 5, y1 = 10, lty = 2)
segments(x0 = 6, y0 = 11, x1 = 6, y1 = 10, lty = 2)
segments(x0 = 7, y0 = 18, x1 = 7, y1 = 10, lty = 2)
segments(x0 = 8, y0 = 15, x1 = 8, y1 = 10, lty = 2)
abline(h = 10)
legend('topleft',bty = 'n', legend = 'Total SS')
box()


x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
par(new=T)
abline(h = 10)
segments(x0 = 1, y0 = 3, x1 = 8, y1 = 17)
segments(x0 = 1, y0 = 2, x1 = 1, y1 = 3, lty = 2)
segments(x0 = 2, y0 = 5, x1 = 2, y1 = 8, lty = 2)
segments(x0 = 3, y0 = 5, x1 = 3, y1 = 7, lty = 2)
segments(x0 = 4, y0 = 8, x1 = 4, y1 = 9, lty = 2)
segments(x0 = 5, y0 = 12, x1 = 5, y1 = 11, lty = 2)
segments(x0 = 6, y0 = 11, x1 = 6, y1 = 13, lty = 2)
segments(x0 = 7, y0 = 18, x1 = 7, y1 = 15, lty = 2)
segments(x0 = 8, y0 = 15, x1 = 8, y1 = 16, lty = 2)
legend('topleft',bty = 'n', legend = 'Residual SS')
box()


x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
par(new=T)
abline(h = 10)
segments(x0 = 1, y0 = 3, x1 = 8, y1 = 17)
segments(x0 = 1, y0 = 3, x1 = 1, y1 = 10, lty = 2)
segments(x0 = 2, y0 = 5, x1 = 2, y1 = 10, lty = 2)
segments(x0 = 3, y0 = 7, x1 = 3, y1 = 10, lty = 2)
segments(x0 = 4, y0 = 9, x1 = 4, y1 = 10, lty = 2)
segments(x0 = 5, y0 = 10, x1 = 5, y1 = 11, lty = 2)
segments(x0 = 6, y0 = 10, x1 = 6, y1 = 13, lty = 2)
segments(x0 = 7, y0 = 10, x1 = 7, y1 = 15, lty = 2)
segments(x0 = 8, y0 = 10, x1 = 8, y1 = 17, lty = 2)
legend('topleft',bty = 'n', legend = 'Model SS')
box()
#Arrow and word boxes
```

## Total SS (SS~T~, SST)

+ Total variability (variability between scores and the mean).

+ TSS is the sum of the squared residuals when the most basic model is applied to the data. 

+ How good is the mean as a model to the observed data?

+ Lets consider the mean of *Y* to be the null model:

+ SS~T~ uses the differences between the observed data and the mean value of *Y* (the null model)

+ Total SS $=\sum(Y_i - \bar{Y})^2$

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,1,4)) # This tells R to put 1 row, 2 columns
x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
par(new=T)
abline(h = 10)
segments(x0 = 1, y0 = 2, x1 = 1, y1 = 10, lty = 2)
segments(x0 = 2, y0 = 8, x1 = 2, y1 = 10, lty = 2)
segments(x0 = 3, y0 = 5, x1 = 3, y1 = 10, lty = 2)
segments(x0 = 4, y0 = 8, x1 = 4, y1 = 10, lty = 2)
segments(x0 = 5, y0 = 12, x1 = 5, y1 = 10, lty = 2)
segments(x0 = 6, y0 = 11, x1 = 6, y1 = 10, lty = 2)
segments(x0 = 7, y0 = 18, x1 = 7, y1 = 10, lty = 2)
segments(x0 = 8, y0 = 15, x1 = 8, y1 = 10, lty = 2)
box()
#needs dotted lines

```

## Residual SS or Error SS (SS~R~)

+ SS~R~

+ Residual/error variability (variability between the regression model and the actual data).

+ Difference between the observed data and the model

+ Represents the degree of inaccuracy when fitting the *best fit* model to the data.

+ Residual/error variability (variability between the regression model and the actual data).

+ RSS $=\sum(Y_i - \hat{Y})^2$

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
par(new=T)
abline(h = 10)
segments(x0 = 1, y0 = 3, x1 = 8, y1 = 17)
segments(x0 = 1, y0 = 2, x1 = 1, y1 = 3, lty = 2)
segments(x0 = 2, y0 = 5, x1 = 2, y1 = 8, lty = 2)
segments(x0 = 3, y0 = 5, x1 = 3, y1 = 7, lty = 2)
segments(x0 = 4, y0 = 8, x1 = 4, y1 = 9, lty = 2)
segments(x0 = 5, y0 = 12, x1 = 5, y1 = 11, lty = 2)
segments(x0 = 6, y0 = 11, x1 = 6, y1 = 13, lty = 2)
segments(x0 = 7, y0 = 18, x1 = 7, y1 = 15, lty = 2)
segments(x0 = 8, y0 = 15, x1 = 8, y1 = 16, lty = 2)
box()
```

## Model SS or Regression SS (SS~M~)

+ SS~M~ 

+ Model variability (difference in variability between the model and the mean).

+ This is the improvement we get from fitting the model to the data relative to the null model.

+ regression SS $=\sum(\hat{Y}_i - \bar{Y})^2$

+ How do we get large SSM?

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
par(new=T)
abline(h = 10)
segments(x0 = 1, y0 = 3, x1 = 8, y1 = 17)
segments(x0 = 1, y0 = 3, x1 = 1, y1 = 10, lty = 2)
segments(x0 = 2, y0 = 5, x1 = 2, y1 = 10, lty = 2)
segments(x0 = 3, y0 = 7, x1 = 3, y1 = 10, lty = 2)
segments(x0 = 4, y0 = 9, x1 = 4, y1 = 10, lty = 2)
segments(x0 = 5, y0 = 10, x1 = 5, y1 = 11, lty = 2)
segments(x0 = 6, y0 = 10, x1 = 6, y1 = 13, lty = 2)
segments(x0 = 7, y0 = 10, x1 = 7, y1 = 15, lty = 2)
segments(x0 = 8, y0 = 10, x1 = 8, y1 = 17, lty = 2)
box()
#needs dotted lines

```

## SST = SSR + SSM

+ So, we can calculate the proportion of improvement due to the model.

+ SSM/SST, percentage of variation explained by the model.

## Evaluating the Quality of the Model: R^2^

R^2^

+ The proportion of variance accounted for by the regression model

+ The Pearson Correlation Coefficient Squared

$R^2=\frac{SS_M}{SS_T}$

## SS for Model Testing

Evaluate the amount of systematic variance (regression/model) divided by the amount of unsystematic (residual) variance.

*F* test - "termed variance ratio test"

1. Calculate quantities "mean squares"

2. Use the SSM and SSR

3. Divide the SSM and SSR by their 	respective degrees of freedom (DF). These terms can be expressed as averages, divided by DF terms.

+ DF for SSM (The number of parameters in the model - 1)

+ DF for SSR (number of obs - number of parameters in the model)

These are called mean squares, MS: MS *Model* and MS *Residual*

+ MS~M~ = SS~M~/DF~M~
+ MS~R~ = SS~R~/DF~R~

*F*-test is termed the "variance ratio test"

$F=\frac{MS_M}{MS_R}$

## Test $\beta_1 = 0$

To test whether $\beta_1 = 0$ we will use the t-distribution.

After determining the model parameters values we can estimate the standard error of the slope.

The approach is to determine the confidence interval of the slope for a given value of alpha.

Look at the following equation, what values will we need?

$SE_{\beta_1}=\frac {\sqrt{(\sum{{Y_i - \hat{Y_i}}})^2/(n - 2)}}{\sqrt{\sum{(X_i - \bar{X})^2}}}$  

Degrees of freedom. For simple linear regression (one independent and one dependent variable), the degrees of freedom (DF) is equal to, n - 2.

Our calculated test statistic is:

$t = \frac{\beta_1}{SE_{\beta_1}}$

We will compare this to the critical value:

$t_{\alpha, tailed, d.f.}$

## Worked Example

|Age (days) (X)|Wing Length (cm) (Y)|
|---   |--- |
|3.0   |1.4 |
|4.0   |1.5 |
|5.0   |2.2 |
|6.0   |2.4 |
|8.0   |3.1 |
|9.0   |3.2 |
|10.0  |3.2 |
|11.0  |3.9 |
|12.0  |4.1 |
|14.0  |4.7 |
|15.0  |4.5 |
|16.0  |5.2 |
|17.0  |5.0 |


```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(3,4,5,6,8,9,10,11,12,14,15,16,17)
y <- c(1.4,1.5,2.2,2.4,3.1,3.2,3.2,3.9,4.1,4.7,4.5,5.2,5.0)
plot(x,y, main = "", xlab = "Age. X. in days", ylab = "Wave length. Y. in cm", xlim = c(1,18), ylim = c(1,6), pch=16)
segments(x0 = 3, y0 = 1.5, x1 = 17, y1 = 5.2)
```

We find that the TSS is = 19.65

We find that the Model SS = 19.13

|Source of Variation|Sum of Squares (SS)|DF|Mean Squares (MS)|
|---|---|---|---|
|Total |$\sum(Y_i-\bar{Y})^2$|  
|Model (or Regression) |$\sum(\hat{Y}_i-\bar{Y})^2$ |The number of parameters in the model - 1 |SS~M~/DF~M~
|Residual (or Error)| $\sum(Y_i-\hat{Y})^2$| The number of obs - the number of parameters in the model|SS~R~/DF~R~
 
### Summary of the Calculations for Model Fit

We are falsifying the null hypothesis: $H_0$: There is no statistically significant relationship between variables x and y.

|Source of Variance|SS|DF|MS|
|---|---|---|---|
|Total|19.65|12|   |
|Model (or Regression)|19.13|1|19.13|
|Residual (or Error)|0.52|11|0.047|


$F=\frac{19.132214}{0.047701}=401.1$

$F_{0.05(1),1.11} = 4.84$

Therefore, reject H~0~

P < 0.0005

## Regression: An Example

A record company boss was interested in predicting record sales from advertising.

Data

+ 200 different album releases

Outcome variable:

+ Sales (CDs and downloads) in the week after release

Predictor variable:

+ The amount (in units of ?1000) spent promoting the record before release. 

## Output of a Simple Regression 

In R:

|Coefficients:|Estimate|Std. Error|t value|Pr(>|t|)|
|---        |---      |---|---|---|
|(intercept)|1.341e+02|7.537e+00|17.799|<2e-16 ***|
|Adverts    |9.612e-02|9.632e-03|9.979 |<2e-16 ***|

+ Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
+ Residual standard error: 65.99 on 198 degrees of freedom
+ Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 
+ F-statistic: 99.59 on 1 and 198 DF,  p-value: < 2.2e-16

## Using the Model

\begin{align}
Record \: Sales_i&=b_0+b_1Advertising \: Budget_i\\

&=134.14+(0.09612\times Advertising \: Budget_i)
\end{align}

<Br>
<Br>

Record Sales~i~ = 134.14 + (0.09612 x Advertising Budget~i~)

= 134.14 + (0.09612 x 100)

= 143.75

# Multiple Linear Regression

Model is fit by minimizing the sum of squared differences between the line and the actual data points - method of least squares.

We still use our base equation:

$outcome_i=model_i+error_i$

But this time the model it is a bit more complicated. 

When we add predictors, we add coefficients ($\beta$ terms). 

So each predictor variable has its own coefficient and the outcome variable is predicted from a combination of all variables multiplied by their respective coefficients plus a residual term.

## Multiple Predictors

*Y* is the outcome variable, $\beta_1$ is the coefficient of X1,$\beta_n$ is the coefficient of $X_n$.

We seek to find combinations of $\beta$ values (paramteers).

$outcome_i=model_i+error_i$

$Y_i=(b_0 + b_1X_{1i}+b_2X_{2i}+\ldots+b_nX_{ni})+\varepsilon_i$

## Album Sales Model

From our record sales data we know that advertising accounts for 33% of the variation in album sales.

Remember: 

$R^2=\frac{SS_M}{SS_T}$

$R^2=0.3313$

Therefore a large proportion of variation remains unexplained.

Lets bring a new predictor variable into the mix: 

+ How many times the song was played on the radio during the week prior to its release. 

## Multiple Predictors Model

Incorporate "airplay"

So we have a model with three parameters and two slope coefficients. Because there are two predictors, so we can view the model in two dimensions:

$Album \:Sales_i=(b_0+b_1advertising \: budget_i+b_2 airplay_i)+\varepsilon_i$

<Br>

## Regression "Plane"

``````{r, results='hide',message=FALSE, echo=FALSE}
#install.packages("scatterplot3d")
library(scatterplot3d)
library(MASS)
mu <- c(250,35,1000)
stddev <- c(100, 35, 500)

corMat <- matrix(c(1, 0.78, 0.63,
                   0.78, 1, 0.27,
                   0.63, 0.27, 1),
                 ncol = 3)
covMat <- stddev %*% t(stddev) * corMat
covMat
out <- as.data.frame(mvrnorm(n = 212, mu = mu, Sigma = covMat, empirical = FALSE))

out$V1.s <- (out$V1 - min(out$V1)) #*1000+10 #x
out$V2.s <- (out$V2 - min(out$V2))/10+45 #y
out$V3.s <- (out$V3 - min(out$V3)) #*200+30 #z

plot3d <- scatterplot3d(out$V2.s, out$V3.s, out$V1.s, pch = 16, xlab = "No. of Plays on Radio 1 per Week", ylab = "Album Sales (Thousands)", zlab = "Advertising Budget (Thousands of Pounds)", angle = 55)
my.lm <- lm(V1.s ~ V2.s + V3.s, data = out)
plot3d$plane3d(my.lm, lty.box = "solid")

```

## 2.6 Multiple Linear Regression

|Cities               |Y  |X~1~|X~2~|X~3~|X~4~|X~5~ |X~6~|
|---                  |---|--- |--- |--- |--- |---  |--- |
|Pheonix              |10 |70.3|213 |582 |6.0 |7.05 |36  |
|Little Rock          |13 |61.0|91  |132 |8.2 |48.52|100 | 
|San Francisco        |12 |56.7|453 |716 |8.7 |20.66|67  |
|Denver               |17 |51.9|454 |515 |9.0 |12.95|86  |
|Hartford             |56 |49.1|412 |158 |9.0 |43.37|127 |
|Wilmington           |36 |54.0|80  |80  |9.0 |40.25|114 |
|Washington           |29 |57.3|434 |757 |9.3 |38.89|111 |
|Jacksonville         |14 |68.4|136 |529 |8.8 |54.57|116 |
|Miami                |10 |75.5|207 |335 |9.0 |59.80|128 |
|Atlanta              |24 |61.5|368 |497 |9.1 |48.34|115 |
|Chicago              |110|50.6|3344|3369|10.4|34.44|122 |
|Indianapolis         |28 |52.3|361 |746 |9.7 |38.74|121 |
|Des Moines           |17 |49.0|104 |201 |11.2|30.85|103 |
|Wichita              |8  |56.6|125 |277 |12.7|30.58|82  |
|Louisville           |30 |55.6|291 |593 |8.3 |43.11|123 |
|New Orleans          |9  |68.3|204 |361 |8.4 |56.77|113 |
|Baltimore            |47 |55.0|625 |905 |9.6 |41.31|111 |
|Detroit              |35 |49.9|1064|1513|10.1|30.96|129 |
|Minneapolis- St. Paul|29 |43.5|699 |744 |10.6|25.94|137 |
|Kansas City          |14 |54.5|381 |507 |10.0|37.00|99  |
|St. Louis            |56 |55.9|775 |622 |9.5 |35.89|105 |
|Omaha                |14 |51.5|181 |347 |10.9|30.18|98  |
|Alburquerque         |11 |56.8|46  |244 |8.9 |7.77 |58  |
|Albany               |46 |47.6|44  |116 |8.8 |33.36|135 |
|Buffalo              |11 |47.1|391 |463 |12.4|36.11|166 |
|Cincinnati           |23 |54.0|462 |453 |7.1 |39.04|132 | 

*Table 2.6.1: \ Air pollution in 41 U.S. cities associatied with six environmental vairables*

<Br>

## Model Interpretation

We regress SO~2~ content in the air on average temperature X~1~ and the number of manufacturing enterprises, X~2~

$\bar{y} = 77.231 + 1.0480X_1 + 0.02431X_2$

A one unit change in X2 results in a 0.02431 increase in Y

A one unit change in X1 results in a 1.048 increase in Y

## Test of Null Hypothesis

We can analyze the null hypothesis that all of the regression coefficients are equal to zero using an ANOVA analysis 

+ analogous to that of the simple linear regression.

## Significance Testing

In general, a significant *F* value will be associated with the rejection of the null hypothesis for some regression coefficients. 

+ Sometimes it is possible to have a significant *F* without significant regression coefficients - this situation occurs when there is multicollinearity among variables.

+ This happens when two variables are highly correlated. 

+ Singularity is an extreme amount of multicollinearity: there is perfect correlation between two or more variables.

## Multicollinearity

Result in untrustworthy coefficients - serves to increase the variance of the estimate of the mean coefficient value.

Limits the magnitude of the coefficient of determination - 

+ Example: inclusion of one predictor results in R2 = 0.80. 
+ When you add a highly correlated variable, the variance it accounts for is already described by the first variable - it does not account for unique variance. 
+ So we only get a slight increase in our *R* value.

Importance of predictors - Difficult to discern which predictor is the most important.

+ We can deal with this by evaluating the variables prior to inclusion into the model.

+ Use stepwise inclusion or exclusion approaches.

## Determination of Predictors to Include?

One of the most widespread ways is to use "stepwise" methods, you specify a direction either forward or backward.

Forward style:

+ Initial model with only the constant b0 is made
+ Add single predictor that best predicts the outcome by selecting the one with the greatest correlation with the outcome
+ If fit is improved, then the predictor is retained
+ Repeat

Backward selection. 

+ As above but remove coefficients one at a time.

Computer intensive - simultaneous backward and forward.

In these approaches parsimony is generally ignored.

## Model Design Considerations 1

+ Predictor variables must be quantitative (continuous, unbounded, interval data) or categorical.

+ Predictors should have non-zero variance.

+ Predictors should not be highly correlated.

+ Predictors should not be correlated with external variables.

+ Avoid proxy variables and make direct and explicit relationships

## Model Design Considerations 2

It is desirable to employ a regression that is parsimonious - as few as necessary but as many as needed.

What strategies are available to get the best regression:

+ Fit all possible subsets - choose the one resulting in the lowest coefficient of determination.
+ Forward, backward, or "both" variable selection.
+ Use an independent inclusion criteria.

# Comparing Two Means

*t*-tests:

A common situation is the comparison of the means of two populations.

An example of a two sample *t*-test for the two tailed hypothesis is to evaluate the population of mean blood clotting times administered with drug B the same as the population mean for blood-clotting times of specimens given drug G.

+ Null hypothesis: $\mu_B = \mu_G$, where $\mu$ is the blood clotting time (minutes, hours)

## *t*-test

If the two samples came from two normally distributed populations and the variances of the populations are equal we can use a two sample *t*-test.

+ Null hypothesis: $\mu_1 = \mu_2$.

+ Two samples of data are collected and the sample means calculated. 

+ These means might differ by either a little or a lot.

+ If the samples come from the same population, then we expect their means to be roughly equal. 

+ Although it is possible (even likely) for their means to differ by chance alone, we would expect large differences between sample means to occur very infrequently.

+ We are interested in understanding the difference between the sample means that we would expect to obtain if there were no effect (i.e. if the null hypothesis were true). 

+ We use the standard error as a gauge of the variability between sample means. 

## Large differences in mean values

If the difference between the samples we have collected is larger than what we would expect based on the standard error then we can infer one of two things:

+ There is no effect and sample means in our population fluctuate a lot (sampling error).

+ By chance, collected two samples that are atypical of the population from which they came.
      
+ The two samples come from different populations but are typical of their respective parent population. 

+ In this scenario, the difference between samples represents a genuine difference between the samples (and so the null hypothesis is incorrect).

We are trying to infer - did the samples come from the same population?

If the observed difference between the sample means is large: The more confident we become that the second explanation is correct (i.e. that the null hypothesis should be rejected).

## Example

+ Is arachnophobia (fear of spiders) specific to real spiders or is a picture enough?

+ 24 arachnophobic individuals

Manipulation:

+ 12 participants were exposed to a real spider
+ 12 were exposed to a picture of the same spider

+ Outcome is the anxiety produced

## The *t*-test as a GLM

Outcome = model + error

Consider an experiment where Groups were exposed to a "Picture of a Spider" and an "Actual Spider"

The response variable is the level of Anxiety (A)

$A_i=b_0+b_1G_i+\varepsilon_i$

$Anxiety_i+=b_0+b_1group_i+\varepsilon_i$

The independent variable *G* has only two values "Group 1" and "Group 2" ie. The "real" and "picture" groups.

```{r, results='hide',message=FALSE, echo=FALSE}

df. <- data.frame(Group = c(rep("Picture", 25), rep("Real Spider", 25)), Anxiety = c(runif(25,25,55),runif(25,30,65)))
head(df.)
boxplot(Anxiety~Group, data = df.)

```

## Picture Group

We can code the "dummy" variable, *G* 

The group variable = 0
 
Intercept = mean of baseline group


$\bar{X}_{Picture}=b_0+(b_1\times0)$

$b_0=\bar{X}_{Picture}$

$b_0=40$


<Br>

```{r, results='hide',message=FALSE, echo=FALSE}

df. <- data.frame(Group = c(rep("Picture", 25), rep("Real Spider", 25)), Anxiety = c(runif(25,25,55),runif(25,30,65)))
head(df.)
boxplot(Anxiety~Group, data = df.)

```

## Real Spider Group

The group variable = 1

$b_1$ = Difference between means

$b_1 =\bar{Y}_{Real}=b_0+(b_1\times1)$

$b_1=\bar{Y}_{Real}=\bar{Y}_{Picture}+b_1$

$b_1=\bar{Y}_{Real}-\bar{Y}_{Picture}$

$b_1=47-40$

$b_1=7$


````{r, results='hide',message=FALSE, echo=FALSE}

df. <- data.frame(Group = c(rep("Picture", 25), rep("Real Spider", 25)), Anxiety = c(runif(25,25,55),runif(25,30,65)))
head(df.)
boxplot(Anxiety~Group, data = df.)

```

## Output from a Regression

```{r, echo=FALSE}
df. <- data.frame(Group = c(rep("Picture", 12), rep("Real Spider", 12)))
Anxiety <-  c(runif(12,25,55))
Anxiety <- c(Anxiety, Anxiety+7)
df.$Anxiety <- Anxiety

df.$Group <- as.factor(df.$Group)

summary(lm(df.$Anxiety~df.$Group))

anova(lm(df.$Anxiety~df.$Group))

```

## The Independent *t*

*Note: I am using the variable Y as the observation values - this is a departure from Zar's presentation*

$t=\frac{\bar{Y}_1-\bar{Y}_2}{\sqrt{\frac{S^2_p}{n_1}+\frac{S^2_p}{n_2}}}$

The numerator is the difference between sample means

Denominator is the standard error of the difference between the sample means

+ This quanitty is a measure of the variability of the data within the two samples.

## The Independent *t*-test 3

$S^2_p=\frac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}$

$S^2_p=\frac{SS_1+SS_2}{v_1+v_2}$

Where v~1~ and v~2~ are the degrees of freedom, v~1~ = n~1~ - 1 and v~2~ = n~2~ -1

The test value is compared to the critical value at a given $\alpha$

$t_\alpha(2),(v_1+v_2)$
  
  + Need to set Alpha vaule
  + (2): One or two-tailed test?
  + v~1~ = n~1~ - 1 and v~2~ = n~2~ - 1

## Zar Example

$H_0: \mu_1 = \mu_2$
$H_A: \mu_1\ne\mu_2$

|Given drug B|Given drug G|
|---|---|
|8.8|9.9|
|8.4|9.0|
|7.9|11.1|
|8.7|9.6|
|9.1|8.7|
|9.6|10.4|
|   |9.5|
|------|
|$n_1=6$|$n_2=7$|
|$\nu_1=5$|$\nu_2=6$|
|$\bar{Y}_1=$ 8.75 min | $\bar{Y}_2=$ 9.74 min|
|SS~1~ = 1.6950 min^2^ | \ SS~2~ = 4.0171 min^2^|

*Table 3.18.1: \ Zar example*

<Br>

$S^2_p=\frac{SS_1+SS_2}{v_1+v_2}=\frac{1.6950+4.0171}{5+6}=\frac{5.7121}{11}=0.5193 \: \mbox{min}^2$

<Br>


$s_{\bar{Y_1}-\bar{Y}_2}=\sqrt{\frac{S^2_p}{n_1}+\frac{S^2_p}{n_2}}=\sqrt{\frac{0.5193}{6}+\frac{0.5193}{7}}=\sqrt{0.0866+0.0742}$

<Br>

$=\sqrt{0.1608}=0.40\mbox{min}$


<Br>

$t=\frac{\bar{Y}_1-\bar{Y}_2}{s_{\bar{Y}_1-\bar{Y}_2}}$

<Br>

## Determine test value

$t=\frac{\bar{Y}_1-\bar{Y}_2}{s_{\bar{Y}_1-\bar{Y}_2}}=\frac{8.75-9.74}{0.40}=\frac{-0.99}{0.40}=-2.475$

$t_{0.05(2),v}=t_{0.05(2),11}=2.201$

+ Determine critical value

Therefore, reject H~0~

+ $0.02<P(|t|\ge2.175)<0.05 \: \: [P=0.031]$

## When Assumptions are Broken

+ Non-parameteric *t*-test Mann-Whitney "U" Test

+Do not require estimation of $\mu$ and $\sigma$.

+No assumptions about distributions.

+ Convert data to RANKS of data.

+ Two sample rank test

+ Rank from highest to lowest, the greatest value in either group is given a one, second given a two..

$U=n_1n_2+\frac{n_1(n_1+1)}{2}-R_1$

+ n~1~ and n~2~ are the number of observation sin samples 1 and 2. 
+ R~1~ is the sum of the ranks in sample 1 

H~0~: Male and female students are the same height.
H~A~: Male and female students are not the same height.

$\alpha=0.05$


|Height of males|Height of females|Ranks of male heights|Ranks of female heights|
|---|---|---|---|
|193 cm |178 cm |1   |6   |
|188   |173   |2   |8   |
|185   |168   |3   |10   |
|183   |165   |4   |11   |
|180   |163   |5   |12   |
|175   |   |7   |   |
|170   |   |9   |   |
|---|
|$n_1=7$|$n_2=5$|$R_1=31$|$R_2=47$|

$U=n_1n_2+\frac{n_1(n_1+1)}{2}-R_1$

$(7)(5)+\frac{(7)(8)}{2}-31$

$35+28-31$

$32$

$U^1=n_1n_2-U$

$(7)(5)-32$

$=3$

$U_{0.05(2),5,7}=30$

As 32>30, H~0~ is rejected.

Therefore, we conclude that height is different for male and female students. 

<Br>

# Comparing Several Means: One-Way ANOVA

Understand the basic principles of ANOVA

## When and why to use ANOVA designs

When we want to compare means we can use a *t*-test. This test has limitations:

+ You can test the equality of two mean values: often we would like to compare means from three or more groups.
+ It can be used only with one predictor/independent variable.

The ANOVA is an innovation that allows: 

+ Compares several means
+ Can be used when you have manipulated more than one independent variable
+ It is an extension of regression (and hence is a general linear model)

## 5.3 ANOVA

$H_0: \mu_1=\mu_2=\mu_3$

Fisher: British statistician and geneticists. Introduced this analysis 

Let us assume that we test four different feeds and want to see if it impacted the mean body weights in pigs.

We are going to test the effect of one factor - feed type. The analysis is termed a one-factor test or one-way ANOVA (Only a single thing, the factor, is different amont the groups).

A population of pigs is assigned, at random, to each of the four treatments (treatments are levels of the factor).

Parametric test

## Why Not Use Lots of *t*-tests?

If we want to compare several means why don't we compare pairs of means with *t*-tests?

+ Inflates the Type I error rate
+ Type one error rate = 1 - 0.95^n^

## What Does ANOVA Tell Us?

Null hypothesis:

+ Like a *t*-test, ANOVA tests the null hypothesis that the means are the same.

Experimental (alternative) hypothesis:

+ The means differ.

ANOVA is an omnibus test

+ It test for an overall test for evaluating the equality of means among groups.
+ It tells us that the group means are different.
+ It doesn't tell us which mean(s) differ.

$H_0: \mu_1=\mu_2=\mu_3=\mu_4$

H~A~: The mean weights of pigs on the four diets are not all equal 

If H~0~ is rejected, there is at least one difference among the four means.

## ANOVA as Regression

$\mbox{outcome}_i = (\mbox{model})+\mbox{error}_i$

$\mbox{illness}_i=b_0+b_2\mbox{high}_i+b_1\mbox{low}_i+\varepsilon_i$

|Group|Dummy variable 1 (High)|Dummy variable 2 (Low)|
|---|---|---|
|Placebo|0|0|
|Low dose medicine|0|1|
|High dose medicine|1|0|

## Placebo Group

$\mbox{illness}_i=b_0+b_2\mbox{high}_i+b_1\mbox{low}_i+\varepsilon_i$

$\mbox{illness}_i=b_0+(b_2\times 0)+(b_1\times 0)$

$\mbox{illness}_i=b_0$

$\bar{Y}_{placebo}=b_0$

## High Dose Group

$\mbox{illness}_i=b_0+b_2\mbox{high}_i+b_1\mbox{low}_i+\varepsilon_i$

$\mbox{illness}_i=b_0+(b_2\times 1)+(b_1\times 0)$

$\mbox{illness}_i=b_0+b_2$

$\bar{Y}_{high}= \bar{Y}_{placebo}+b_2$

$b_2=\bar{Y}_{high}-\bar{Y}_{placebo}$

## Low Dose Group

$\mbox{illness}_i=b_0+b_2\mbox{high}_i+b_1\mbox{low}_i+\varepsilon_i$

$\mbox{illness}_i=b_0+(b_2\times 0)+(b_1\times 1)$

$\mbox{illness}_i=b_0+b_1$

$\bar{Y}_{low}=\bar{Y}_{placebo}+b_1$

$b_1=\bar{Y}_{low}-\bar{Y}_{placebo}$

## Output from Regression

|Coefficients:|Estimate|Std. Error|t value|Pr(>|t|)|
|---|---|---|---|---|
|(intercept)   |2.2000   |0.6272   |3.508   |0.00432 **|
|dummy 1       |2.8000   |0.8869   |3.157   |0.00827 **|
|dummy 2       |1.0000   |0.8869   |1.127   |0.28158   |

Signif. Codes: \ 0 '***' 0.001 '**' 0.01 '*' 0.05'.' 0.1 '' 1

+ residual standard error: \ 1.402 on 12 degrees of freedom. 
+ Multiple R-squared: \ 0.4604, Adjusted R-squared: \ 0.3704
+ *F*-statistic: \ 5.119 on 2 and 12 DF, p-value: \ 0.02469

## Theory of ANOVA

We calculate how much variability there is between scores

+ Total sum of squares (SS~T~).

We then calculate how much of this variability can be explained by the model we fit to the data

+ How much variability is due to the experimental manipulation? This is evaluated using the model sum of squares (SS~M~)

+ How much variability is due to individual differences in performance, residual sum of squares (SS~R~). 

+ We compare the amount of variability explained by the model (experiment), to the error in the model (individual differences)

+ This ratio is called the *F*-ratio.

If the model explains a lot more variability than it can't explain, then the experimental manipulation has had a significant effect on the outcome.

+ SS~M~ will be greater than SS~R~

## ANOVA by Hand

Testing the effects of medicine on illness using three groups:

+ Placebo (sugar pill)
+ Low dose medicine
+ High dose medicine

The outcome/dependent variable (DV) was an objective measure of illness.

## The Data

|   |Placebo|Low Dose|High Dose|
|---|---|---|---|
|   |3  |5   |7 |
|   |2  |2   |4 |
|   |1  |4   |5 |
|   |1  |2   |3 |
|   |4  |3   |6 |
|$\bar{Y}$|2.20|3.20|5.00|
|*s*   |1.30|1.30|1.58|
|*s*^2^|1.70|1.70|2.50|

Grand Mean = 3.467, Grand *s* = 1.767, Grand *s*^2^ = 3.124

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(1,2,3,4,5)
y <- c(3,2,1,1,4)
plot(x,y, main = "", xlab = "Participant", ylab = "Illness", xlim = c(0,16), ylim = c(0,8), pch=16)
abline(h = 3.5)
segments(x0 = 1, y0 = 2.2, x1 = 5, y1 = 2.2)
segments(x0 = 1, y0 = 2.2, x1 = 1, y1 = 3, lty = 2)
segments(x0 = 2, y0 = 2.2, x1 = 2, y1 = 2, lty = 2)
segments(x0 = 3, y0 = 2.2, x1 = 3, y1 = 1, lty = 2)
segments(x0 = 4, y0 = 2.2, x1 = 4, y1 = 1, lty = 2)
segments(x0 = 5, y0 = 2.2, x1 = 5, y1 = 4, lty = 2)

x <- c(6,7,8,9,10)
y <- c(5,2,4,2,3)
points(x,y, main = "", xlab = "Participant", ylab = "Illness", xlim = c(0,16), ylim = c(0,8), pch=17)
segments(x0 = 6, y0 = 3.2, x1 = 10, y1 = 3.2)
segments(x0 = 6, y0 = 5, x1 = 6, y1 = 3.2, lty = 2)
segments(x0 = 7, y0 = 2, x1 = 7, y1 = 3.2, lty = 2)
segments(x0 = 8, y0 = 4, x1 = 8, y1 = 3.2, lty = 2)
segments(x0 = 9, y0 = 2, x1 = 9, y1 = 3.2, lty = 2)
segments(x0 = 10, y0 = 3, x1 = 10, y1 = 3.2, lty = 2)

x <- c(11,12,13,14,15)
y <- c(7,4,5,3,6)
points(x,y, main = "", xlab = "Participant", ylab = "Illness", xlim = c(0,16), ylim = c(0,8), pch=15)
segments(x0 = 11, y0 = 5, x1 = 15, y1 = 5)
segments(x0 = 11, y0 = 7, x1 = 11, y1 = 5, lty = 2)
segments(x0 = 12, y0 = 4, x1 = 12, y1 = 5, lty = 2)
segments(x0 = 14, y0 = 3, x1 = 14, y1 = 5, lty = 2)
segments(x0 = 15, y0 = 6, x1 = 15, y1 = 5, lty = 2)

arrows(x0 = 5.5, y0 = 2.2, x1 = 5.5, y1 = 3.2)
arrows(x0 = 10.5, y0 = 2.2, x1 = 10.5, y1 = 5)

legend("bottomright", legend = c("Placebo", "Low Dose", "Low Dose"), pch = c(16,17,15), lty = 1)
```

## Step 1: Calculate SS~T~

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,1,4)) # This tells R to put 1 row, 2 columns
x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
par(new=T)
abline(h = 10)
segments(x0 = 1, y0 = 2, x1 = 1, y1 = 10, lty = 2)
segments(x0 = 2, y0 = 8, x1 = 2, y1 = 10, lty = 2)
segments(x0 = 3, y0 = 5, x1 = 3, y1 = 10, lty = 2)
segments(x0 = 4, y0 = 8, x1 = 4, y1 = 10, lty = 2)
segments(x0 = 5, y0 = 12, x1 = 5, y1 = 10, lty = 2)
segments(x0 = 6, y0 = 11, x1 = 6, y1 = 10, lty = 2)
segments(x0 = 7, y0 = 18, x1 = 7, y1 = 10, lty = 2)
segments(x0 = 8, y0 = 15, x1 = 8, y1 = 10, lty = 2)
box()


```

*SS~T~ uses the differences between the observed data and the mean value of Y. Where the mean is the grand mean* 

## Total Sum of Squares (SS~T~)

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(1,2,3,4,5)
y <- c(3,2,1,1,4)
plot(x,y, main = "", xlab = "Participant", ylab = "Illness", xlim = c(0,16), ylim = c(0,8), pch=16)
abline(h = 3.5)
segments(x0 = 1, y0 = 2.2, x1 = 5, y1 = 2.2)
segments(x0 = 1, y0 = 2.2, x1 = 1, y1 = 3, lty = 2)
segments(x0 = 2, y0 = 2.2, x1 = 2, y1 = 2, lty = 2)
segments(x0 = 3, y0 = 2.2, x1 = 3, y1 = 1, lty = 2)
segments(x0 = 4, y0 = 2.2, x1 = 4, y1 = 1, lty = 2)
segments(x0 = 5, y0 = 2.2, x1 = 5, y1 = 4, lty = 2)

x <- c(6,7,8,9,10)
y <- c(5,2,4,2,3)
points(x,y, main = "", xlab = "Participant", ylab = "Illness", xlim = c(0,16), ylim = c(0,8), pch=17)
segments(x0 = 6, y0 = 3.2, x1 = 10, y1 = 3.2)
segments(x0 = 6, y0 = 5, x1 = 6, y1 = 3.2, lty = 2)
segments(x0 = 7, y0 = 2, x1 = 7, y1 = 3.2, lty = 2)
segments(x0 = 8, y0 = 4, x1 = 8, y1 = 3.2, lty = 2)
segments(x0 = 9, y0 = 2, x1 = 9, y1 = 3.2, lty = 2)
segments(x0 = 10, y0 = 3, x1 = 10, y1 = 3.2, lty = 2)

x <- c(11,12,13,14,15)
y <- c(7,4,5,3,6)
points(x,y, main = "", xlab = "Participant", ylab = "Illness", xlim = c(0,16), ylim = c(0,8), pch=15)
segments(x0 = 11, y0 = 5, x1 = 15, y1 = 5)
segments(x0 = 11, y0 = 7, x1 = 11, y1 = 5, lty = 2)
segments(x0 = 12, y0 = 4, x1 = 12, y1 = 5, lty = 2)
segments(x0 = 14, y0 = 3, x1 = 14, y1 = 5, lty = 2)
segments(x0 = 15, y0 = 6, x1 = 15, y1 = 5, lty = 2)


legend("bottomright", legend = c("Placebo", "Low Dose", "Low Dose"), pch = c(16,17,15), lty = 1)
```

|   |Placebo|Low Dose|High Dose|
|---|---|---|---|
|   |3  |5   |7 |
|   |2  |2   |4 |
|   |1  |4   |5 |
|   |1  |2   |3 |
|   |4  |3   |6 |
|$\bar{Y}$|2.20|3.20|5.00|
|*s*   |1.30|1.30|1.58|
|*s*^2^|1.70|1.70|2.50|

Grand Mean = 3.467, Grand *s* = 1.767, Grand *s*^2^ = 3.124

SS~T~ = sum((observed - Grand mean)^2^)

SS~T~ =S^2^(N-1) 

## Degrees of Freedom

Degrees of freedom (df) are the number of values that are free to vary.

In general, the df are one less than the number of values used to calculate the SS.

DF~Total~ =  N - 1 

## Model Sum of Squares (SS~M~)

Difference between the model estimate and the mean (or "Grand Mean")

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(1,2,3,4,5,6,7,8)
y <- c(2,8,5,8,12,11,18,15)
plot(x,y, main = "", xlab = "x", ylab = "y", xlim = c(1,8), ylim = c(1,18), pch=1)
par(new=T)
abline(h = 10)
segments(x0 = 1, y0 = 3, x1 = 8, y1 = 17)
segments(x0 = 1, y0 = 3, x1 = 1, y1 = 10, lty = 2)
segments(x0 = 2, y0 = 5, x1 = 2, y1 = 10, lty = 2)
segments(x0 = 3, y0 = 7, x1 = 3, y1 = 10, lty = 2)
segments(x0 = 4, y0 = 9, x1 = 4, y1 = 10, lty = 2)
segments(x0 = 5, y0 = 10, x1 = 5, y1 = 11, lty = 2)
segments(x0 = 6, y0 = 10, x1 = 6, y1 = 13, lty = 2)
segments(x0 = 7, y0 = 10, x1 = 7, y1 = 15, lty = 2)
segments(x0 = 8, y0 = 10, x1 = 8, y1 = 17, lty = 2)
box()
```

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(1,2,3,4,5)
y <- c(3,2,1,1,4)
plot(x,y, main = "", xlab = "", ylab = "", xlim = c(0,16), ylim = c(0,8), pch=16)
abline(h = 3.5)
segments(x0 = 1, y0 = 2.2, x1 = 5, y1 = 2.2)
segments(x0 = 1, y0 = 2.2, x1 = 1, y1 = 3.5)
segments(x0 = 2, y0 = 2.2, x1 = 2, y1 = 3.5)
segments(x0 = 3, y0 = 2.2, x1 = 3, y1 = 3.5)
segments(x0 = 4, y0 = 2.2, x1 = 4, y1 = 3.5)
segments(x0 = 5, y0 = 2.2, x1 = 5, y1 = 3.5)

x <- c(6,7,8,9,10)
y <- c(5,2,4,2,3)
points(x,y, pch=17)
segments(x0 = 6, y0 = 3.2, x1 = 10, y1 = 3.2)
segments(x0 = 6, y0 = 3.5, x1 = 6, y1 = 3.2)
segments(x0 = 7, y0 = 3.5, x1 = 7, y1 = 3.2)
segments(x0 = 8, y0 = 3.5, x1 = 8, y1 = 3.2)
segments(x0 = 9, y0 = 3.5, x1 = 9, y1 = 3.2)
segments(x0 = 10, y0 = 3.5, x1 = 10, y1 = 3.2)

x <- c(11,12,13,14,15)
y <- c(7,4,5,3,6)
points(x,y, pch=15)
segments(x0 = 11, y0 = 5, x1 = 15, y1 = 5)
segments(x0 = 11, y0 = 3.5, x1 = 11, y1 = 5)
segments(x0 = 12, y0 = 3.5, x1 = 12, y1 = 5)
segments(x0 = 13, y0 = 3.5, x1 = 13, y1 = 5)
segments(x0 = 14, y0 = 3.5, x1 = 14, y1 = 5)
segments(x0 = 15, y0 = 3.5, x1 = 15, y1 = 5)

```

## Step 2: Calculate SS~M~

$SS_M=\sum n_i(\bar{x}_i - \bar{x}_{grand})^2$

to 

$SS_M=5(2.2-3.467)^2+5(3.2-3.467)^2+5(5.0-3.467)^2$

$SS_M=5(-1.267)^2+5(-0.267)^2+5(1.533)^2$

$SS_M=8.025+0.355+11.755$

$SS_M=20.135$

## Model Degrees of Freedom

How many values did we use to calculate SS~M~?

+ We used the 3 means.

$df_M=k-1=3-1=2$

## Residual Sum of Squares (SS~R~)

```{r, results='hide',message=FALSE, echo=FALSE}
x <- c(1,2,3,4,5)
y <- c(3,2,1,1,4)
plot(x,y, main = "", xlab = "", ylab = "", xlim = c(0,16), ylim = c(0,8), pch=16)
abline(h = 3.5)
segments(x0 = 1, y0 = 2.2, x1 = 5, y1 = 2.2)
segments(x0 = 1, y0 = 2.2, x1 = 1, y1 = 3)
segments(x0 = 2, y0 = 2.2, x1 = 2, y1 = 2)
segments(x0 = 3, y0 = 2.2, x1 = 3, y1 = 1)
segments(x0 = 4, y0 = 2.2, x1 = 4, y1 = 1)
segments(x0 = 5, y0 = 2.2, x1 = 5, y1 = 4)

x <- c(6,7,8,9,10)
y <- c(5,2,4,2,3)
points(x,y, pch=17)
segments(x0 = 6, y0 = 3.2, x1 = 10, y1 = 3.2)
segments(x0 = 6, y0 = 5, x1 = 6, y1 = 3.2)
segments(x0 = 7, y0 = 2, x1 = 7, y1 = 3.2)
segments(x0 = 8, y0 = 4, x1 = 8, y1 = 3.2)
segments(x0 = 9, y0 = 2, x1 = 9, y1 = 3.2)
segments(x0 = 10, y0 = 3, x1 = 10, y1 = 3.2)

x <- c(11,12,13,14,15)
y <- c(7,4,5,3,6)
points(x,y, pch=15)
segments(x0 = 11, y0 = 5, x1 = 15, y1 = 5)
segments(x0 = 11, y0 = 7, x1 = 11, y1 = 5)
segments(x0 = 12, y0 = 4, x1 = 12, y1 = 5)
segments(x0 = 14, y0 = 3, x1 = 14, y1 = 5)
segments(x0 = 15, y0 = 6, x1 = 15, y1 = 5)

#needs df values

```

## Step 3: Calculate SS~R~


$SS_R=\mbox{sum}([x_i-\bar{x}_i]^2)$

$SS_R=S^2_{group1}(n_1-1)+S^2_{group2}(n_2-1)+S^2_{group3}(n_3-1)$

$SS_R=1.70(5-1)+1.70(5-1)+2.5(5-1)$

$SS_R=(1.70\times4)+(1.70\times4)+(2.50\times4)$

$SS_R=6.8+6.8+10$

$SS_R=23.60$


## Residual Degrees of Freedom

+ How many values did we use to calculate SS~R~?

+ We used the 5 scores for each of the SS for each group.

$df_R=df_{group1}+df_{group2}+df_{group3}$ 

$df_R=(n_1-1)+(n_2-1)+(n_3-1)$  

$df_R=(5-1)+(5-1)+(5-1)$

$df_R=12$

## Double Check

$SS_T=SS_R+SS_M$

$43.74=20.14+23.60$

$DF_T=DF_R+DF_M$

$14=2+12$

## Step 4: Calculate the Mean Squared Error

$MS_M=\frac{SS_M}{df_M}=\frac{20.135}{2}=10.067$

$MS_R=\frac{SS_R}[df_R]=\frac{23.60}{12}=1.967$

## Step 5: Calculate the *F*-Ratio

$F=\frac{MS_M}{MS_R}$

$F=\frac{MS_M}{MS_R}=\frac{10.067}{1.967}=5.12$

## Step 6: Construct a Summary Table

|Source|SS|df|MS|F|
|---|---|---|---|---|
|Model|20.14|2|10.067|5.12*|
|Residual|23.60|12|1.967|   |
|Total|43.74|14|   |   |

## Multiple-Comparison Tests

The ANOVA that you examined is used to test the hypothesis that there is no difference in the sample means among k treatment levels

However we cannot conclude, after doing the test, which of the mean values are different from one-another.

## Tukey Test

Tukey test - balanced, orthogonal designs

Step 1: is to arrange and number all five sample means in order of increasing magnitude

Calculate the pairwise difference in sample means.

We use a *t*-test "analog" to calculate a q-statistic

*S*^2^ is the error mean sqare by anova computation

*n* is the of data in each of groups B and A

Remember this is a completely balanced design.

$SE=\sqrt{\frac{s^2}{n}}$

$q=\frac{\bar{Y}_B-\bar{Y}_A}{SE}$

Start with the largest mean, vs. the smallest mean. Then when the first largest mean has been compared with increasingly large second means, use the second largest mean.

If the null hypothesis is accepted between two means then all other means within that range cannot be different.

# Two-Way Independent ANOVA

+ Rationale of factorial ANOVA
+ Partitioning variance
+ Interaction effects
+ Interaction graphs and their interpretation

## What is Two-Way Independent ANOVA?

Two independent variables

+ Two-way = 2 Independent variables
+ Three-way = 3 Independent variables

Several independent variables is known as a factorial design.

## Benefit of Factorial Designs

We can look at how variables interact.

Interactions

+ Show how the effects that one I.V. might depend on the effects of another
+ Are often more interesting than main effects.

Examples

+ Interaction between hangover and lecture topic on sleeping during lectures.

+ A hangover might have more effect on sleepiness during a stats lecture than during an ecology lecture.
      
## An Example

Field (2009): Testing the effects of the pH of soil and amount of sunlight exposure (full or partial) on plant height:

+ IV 1 (pH): 6, 7, 8
+ IV 2 (Sun): full, partial

Dependent variable (DV) was the height of the plant at the end of the experiment.

Data for the plant height (cm) 

<Br>

|pH |6|6|7|7|8|8|
|---     |---   |---   |---   |---   |---   |---   |
|Sun     |Full  |Partial  |Full |Partial  |Full |Partial  |
|        |65    |50    |70    |45    |55    |30    |
|        |70    |55    |65    |60    |65    |30    |
|        |60    |80    |60    |85    |70    |30    |
|        |60    |65    |70    |65    |55    |55    |
|        |60    |70    |65    |70    |55    |35    |
|        |55    |75    |60    |70    |60    |20    |
|        |60    |75    |60    |80    |50    |45    |
|        |55    |65    |50    |60    |50    |40    |
|Total   |485   |535   |500   |535   |460   |285   |
|Mean    |60.625|66.875|62.50 |66.875|57.50 |35.625|
|Variance|24.55 |106.70|42.86 |156.70|50.00 |117.41|


```{r echo=FALSE, warning=FALSE}

x <- c(50,25,75,10,25,50)
y = c(90,60,60,20,20,20)
Labs <- c(expression(atop("SS"[t], "Variance between all scores")), expression(atop(textstyle("SS"[M]), atop(textstyle("Variance explained by the"),textstyle("experimental manipulations")))), expression(atop("SS"[R], "Error Variance")), expression(atop(textstyle("SS"[A]), atop(textstyle("Effect of"), textstyle("pH")))), expression(atop(textstyle("SS"[B]), atop(textstyle("Effect of"), textstyle("sunlight")))), expression(atop(textstyle("SS"[A*B]), atop(textstyle("Effect of"), textstyle("interaction")))))
par(mar = rep(0,4), oma = rep(0,4))
plot(x,y, yaxt = 'n', xaxt = 'n', ylab = "", xlab = "", ylim = c(0,100), xlim = c(0,100), pch = 26, bty = "n")
text(x,y, labels = Labs)
arrows(x0 = 50, y0 = 80, x1 = 30, y1 = 70, lwd = 2)
arrows(x0 = 50, y0 = 80, x1 = 73, y1 = 70, lwd = 2)
arrows(x0 = 25, y0 = 50, x1 = 14, y1 = 27, lwd = 2)
arrows(x0 = 25, y0 = 50, x1 = 25, y1 = 30, lwd = 2)
arrows(x0 = 25, y0 = 50, x1 = 45, y1 = 27, lwd = 2)
```

## Step 1: Calculate SS~T~

|SS~T~|
|---|---|---|---|---|---|
|65   |50   |70   |45   |55   |30   |
|50   |55   |65   |60   |65   |30   |
|70   |80   |60   |85   |70   |30   |
|45   |65   |70   |65   |55   |55   |
|55   |70   |65   |70   |55   |35   |
|30   |75   |60   |70   |60   |20   |
|70   |75   |60   |80   |50   |45   |
|55   |65   |50   |60   |50   |40   |

$Grand \: Mean = 58.33$

$SS_T=\sum({observations_i}-\bar{observations})^2$

$SS_T=\sum({Y_i}-\bar{Y})^2$

$SS_T=8966.66$

## Step 2: Calculate SS~M~

$\mbox{SS}_\mbox{M}=\sum(\bar{Y}-\bar{Y}_{grand})^2$

$\mbox{SS}_\mbox{M}=8(60.625-58.33)^2+8(66.875-58.33)^2+8(62.5-58.33)^2+8(66.875-58.33)^2+8(57.5-58.33)^2+8(35.625-58.33)^2$

$\mbox{SS}_\mbox{M}=8(2.295)^2+8(8.545)^2+8(4.17)^2+8(8.545)^2+8(-0.83)^2+8(-22.705)^2$

$\mbox{SS}_\mbox{M}=42.1362+584.1362+139.1112+584.1362+5.5112+4124.1362$

$\mbox{SS}_\mbox{M}=5479.167$

## Step 2a: Calculate SS~A~

Mean "Full Sun": 60.21

|SS~A~|   |   |
|---|---|---|
|65 |70   |55   |
|70 |65   |65   |
|60 |60   |70   |
|60 |70   |55   |
|60 |65   |55   |
|55 |60   |60   |
|60 |60   |50   |
|55 |50   |50   |

Mean "Partial Sun": 56.46

|SS~A~   |   |   |
|---|---|---|
|50   |45   |30   |
|55   |60   |30   |
|80   |85   |30   |
|65   |65   |55   |
|70   |70   |35   |
|75   |70   |20   |
|75   |80   |45   |
|65   |60   |40   |

$\mbox{SS}_\mbox{Sun}=\sum^k_1(\bar{Y}_k-\bar{Y}_{Grand})^2n_k$

+ Where *k* is the number of levels of factor A (Sun)

$\mbox{SS}_\mbox{Sun}=24(60.21-58.33)^2+24(56.46-58.33)^2$

$\mbox{SS}_\mbox{Sun}=24(1.88)^2+24(-1.87)^2$

$\mbox{SS}_\mbox{Sun}=84.8256+83.9256$

$\mbox{SS}_\mbox{Sun}=168.75$

## Step 2b: Calculate SS~B~

$\mbox{SS}_\mbox{B}=\sum^k_1(\bar{Y}_k-\bar{Y}_{Grand})n_k$

+ Where *k* is the number of levels of factor B

+ Factor B is pH.

pH = 6

Mean $\mbox{pH}_6$ = 63.75

|SS~B~|   |
|---|---|
|65   |50   |
|70   |55   |
|60   |80   |
|60   |65   |
|60   |70   |
|55   |75   |
|60   |75   |
|55   |65   |

pH = 7

Mean $\mbox{pH}_7$ = 64.6875

|SS~B~   |   |
|---|---|
|70   |45   |
|65   |60   |
|60   |85   |
|70   |65   |
|65   |70   |
|60   |70   |
|60   |80   |
|50   |60   |

pH = 8

Mean $\mbox{pH}_8$ = 46.5625

|SS~B~   |   |
|---|---|
|55   |30   |
|65   |30   |
|70   |30   |
|55   |55   |
|55   |35   |
|60   |20   |
|50   |45   |
|50   |40   |


$\mbox{SS}_\mbox{pH}=16(63.75-58.33)^2+16(64.6875-58.33)^2+16(46.5625-58.33)^2$

$\mbox{SS}_\mbox{pH}=16(5.42)^2+16(6.3575)^2+16(-11.7675)^2$

$\mbox{SS}_\mbox{pH}=470.0224+646.6849+2215.5849$

$\mbox{SS}_\mbox{pH}=3332.292$


```{r echo=FALSE, warning=FALSE}

x <- c(50,25,50,75)
y = c(90,50,50,50)
Labs <- c(expression(atop(textstyle("SS"[M]), atop(textstyle("Variance explained by the"),textstyle("experimental manipulations")))), expression(atop(textstyle("SS"[A]), atop(textstyle("Effect of"), textstyle("pH")))), expression(atop(textstyle("SS"[B]), atop(textstyle("Effect of"), textstyle("sunlight")))), expression(atop(textstyle("SS"[A*B]), atop(textstyle("Effect of"), textstyle("interaction")))))
par(mar = rep(0,4), oma = rep(0,4))
plot(x,y, yaxt = 'n', xaxt = 'n', ylab = "", xlab = "", ylim = c(0,100), xlim = c(0,100), pch = 26, bty = "n")
text(x,y, labels = Labs)
arrows(x0 = 50, y0 = 80, x1 = 30, y1 = 60, lwd = 2)
arrows(x0 = 50, y0 = 80, x1 = 50, y1 = 60, lwd = 2)
arrows(x0 = 50, y0 = 80, x1 = 70, y1 = 60, lwd = 2)

```

## Step 2c: Calculate SS~(AxB)~
$\mbox{SS}_\mbox{M}=\mbox{SS}_{\mbox{A}{\times}\mbox{B}} + \mbox{SS}_\mbox{A}+\mbox{SS}_\mbox{B}$

$\mbox{SS}_{\mbox{A}{\times}\mbox{B}}=\mbox{SS}_\mbox{M}-\mbox{SS}_\mbox{A}-\mbox{SS}_\mbox{B}$

$\mbox{SS}_{\mbox{Sun}{\times}\mbox{pH}}=\mbox{SS}_\mbox{M}-\mbox{SS}_\mbox{Sun}-\mbox{SS}_\mbox{pH}$

$=5479.167-168.75-3332.292$

$=1978.125$

## Step 3: Calculate SS~R~

The residual sum of squares is calculated in the same way as for one-way ANOVA 

Represents individual differences in performance or the variance that can't be explained by factors that were systematically manipulated.

We saw in one-way ANOVA that the value is calculated by taking the squared error between each data point and its corresponding group mean. 

So, we use the individual variances of each group and multiply them by one less than the number of people within the group (n). 

We have the individual group variances: there were eight people in each group (therefore, n = 8).

The degrees of freedom for each group will be one less than the number of scores per group (i.e. 7). Therefore, if we add the sums of squares for each group, we get a total of 6 X 7 = 42.

$\mbox{SS}_\mbox{R}=s^2_{group1}(n_1-1)+s^2_{group2}(n_2-1)+s^2_{group3}(n_3-1)+s^2_{group \: n}(n_n-1)$

$\mbox{SS}_\mbox{R}=s^2_{group1}(n_1-1)+s^2_{group2}(n_2-1)+s^2_{group3}(n_3-1)+s^2_{group4}(n_4-1)+s^2_{group5}(n_5-1)+s^2_{group6}(n_6-1)$

$\mbox{SS}_\mbox{R}=(24.55\times7)+(106.7\times7)+(42.86\times7)+(156.7\times7)+(50\times7)+(117.41\times7)$

$\mbox{SS}_\mbox{R}=171.85+746.9+300+1096.9+350+821.87$

$\mbox{SS}_\mbox{R}=3487.52$

## Interpreting Factorial ANOVA 

|Response      |Attractiveness   |   |   |   |
|---           |---   |--- |---|---|
|              |Sum Sq|Df  |*F* value  |Pr(>*F*)   |
|Sunlight        |169   |1   |2.0323   |0.1614   |
|pH       |3332  |2   |20.0654  |7.649e-07   |
|Gender:pH|1978  |2   |11.9113  |7.987e-05   |
|residuals     |3488  |42  |         |   |


## Interpretation: Main Effect pH

```{r, echo = F}

ph <- data.frame(pH = c(6,7,8), mu = c(60.625,62.5,46.5625), sd = c(4.419417, 3.093592, 14.34326))
barCenters <- barplot(ph$mu, names.arg=ph$pH, col="gray", las=1, ylim=c(0,80), xlab = "pH", ylab = "Plant Height (cm)")
arrows(barCenters, ph$mu + ph$sd, barCenters, ph$mu - ph$sd, angle = 90, code = 3)
```

+ There was a significant main effect of the amount of pH in the soil on the height of the plant,  F(2, 42) = 20.07, p <  0.001

## Interpretation: Main Effect Sun

```{r, echo=F}
sun <- data.frame(sun = c("Full", "Partial"), mu = c(60.20833, 56.45833), sd = c( 2.525907, 18.0422))
barCenters <- barplot(sun$mu, names.arg=sun$sun, col="gray", las=1, ylim=c(0,80), xlab = "Sunlight Exposure", ylab = "Plant Height (cm)")
arrows(barCenters, sun$mu + sun$sd, barCenters, sun$mu - sun$sd, angle = 90, code = 3)

```

+ There was a non-significant main effect of sunlight exposure on the plant height, F(1, 42) = 2.03, p = .161.

## Interpretation: Interaction Effects

```{r, echo = F}
df <- data.frame(ph = rep(seq(6,8),each=2), sun = c(rep(c("Full", "Partial"), 3)), mean = c(60.625,66.875,62.50, 66.875,57.50, 35.625))


plot(x = unique(df$ph), y = df$mean[which(df$sun == "Full")], type = "l", xlab = "pH", ylab = "Plant Height (cm)", ylim = c(30,70))
lines(x = unique(df$ph), y = df$mean[which(df$sun == "Partial")], type = "l", lty = 2)
legend("topright", legend = c("Full Sunlight", "Partial Sulight"), lty = c(1,2), bty = "n")
```

+ There was a significant interaction between the amount of pH in the soil and the amount of sunlight exposure, on the height of the plant, F(2, 42) = 11.91, p < .001.
+ Non-parallel lines indicate such an interaction: For low pH full and partial sunlight, scores do not change much. 
+ At a high pH, partial sunlight scores plummet but full sunlight scores remain fairly high. So, the interaction is caused by a difference between sunlight exposure in the height of plants.

# ANOVA Part II 

## Comparison of Means Test

+ Multi-factorial ANOVA as a linear model

+ Hypotheses being tested

+ Interaction effects

+ Post-hoc tests

+ Non-parametric 

## Factorial ANOVA as Regression


|pH |6|6|7|7|8|8|
|---     |---   |---   |---   |---   |---   |---   |
|Sunlight  |Full|Partial  |Full|Partial  |Full|Partial  |
|        |65    |50    |70    |45    |55    |30    |
|        |70    |55    |65    |60    |65    |30    |
|        |60    |80    |60    |85    |70    |30    |
|        |60    |65    |70    |65    |55    |55    |
|        |60    |70    |65    |70    |55    |35    |
|        |55    |75    |60    |70    |60    |20    |
|        |60    |75    |60    |80    |50    |45    |
|        |55    |65    |50    |60    |50    |40    |
|Total   |485   |535   |500   |535   |460   |285   |
|Mean    |60.625|66.875|62.50 |66.875|57.50 |35.625|
|Variance|24.55 |106.70|42.86 |156.70|50.00 |117.41|

$outcome_i = (\mbox{model})+\mbox{error}_i$

$plant height_i=(b_0+b_{1}\mbox{Sunlight}_i+b_2\mbox{pH}_i)+\varepsilon_i$

$plant height_i=(b_0+b_1A_i+b_2B_i+b_3AB_i)+\varepsilon_i$

$plant height_i=(b_0+b_1Sunlight_i+b_2pH_i+b_3interaction_i)+\varepsilon_i$

+ How do we code the interaction term?

+ Multiply the variables

+ *A* x *B*

|Sunlight|pH|Dummy (Sunlight)|Dummy (pH)|Interaction|Mean|
|---|---|---|---|---|---|
|Partial   |6   |0   |0   |0   |66.875   |
|Partial   |8|0   |1   |0   |35.625   |
|Full   |6   |1   |0   |0   |60.625   |
|Full   |8|1   |1   |1   |57.500   |


$plant height_i=(b_0+b_1Sunlight_i+b_2pH_i+b_3interaction_i)+\varepsilon_i$

$\bar{Y}_{partial,6}=b_0+(b_1\times0)+(b_2\times0)+(b_3\times0)$

$b_0=\bar{Y}_{partial,6}$

$b_0=66.875$

|Sunlight|pH|Dummy (Sunlight)|Dummy (pH)|Interaction|Mean|
|---|---|---|---|---|---|
|Partial   |6   |0   |0   |0   |66.875   |
|Partial   |8|0   |1   |0   |35.625   |
|Full   |6   |1   |0   |0   |60.625   |
|Full   |8|1   |1   |1   |57.500   |

$\bar{Y}_{Full,6}=b_0+(b_1\times1)+(b_2\times0)+(b_3\times0)$

$\bar{Y}_{Full,6}=b_0+b_1$

$\bar{Y}_{Full,6}=\bar{Y}_{partial,6}+b_1$

$b_1=\bar{Y}_{Full,6}-\bar{Y}_{partial,6}$

$b_1=60.625-66.875$

$b_1=-6.25$


|Sunlight|pH|Dummy (Sunlight)|Dummy (pH)|Interaction|Mean|
|---|---|---|---|---|---|
|Partial   |6   |0   |0   |0   |66.875   |
|Partial   |8|0   |1   |0   |35.625   |
|Full   |6   |1   |0   |0   |60.625   |
|Full   |8|1   |1   |1   |57.500   |


$\bar{Y}_{partial,4 \: pH}=b_0+(b_1\times0)+(b_2\times1)+(b_3\times0)$

$\bar{Y}_{partial,4 \: pH}=b_0+b_2$

$\bar{Y}_{partial,4 \: pH}=\bar{Y}_{partial,6}+b_2$

$b_2=\bar{Y}_{partial,4 \: pH}-\bar{Y}_{partial,6}$


|Sunlight|pH|Dummy (Sunlight)|Dummy (pH)|Interaction|Mean|
|---|---|---|---|---|---|
|Partial   |6   |0   |0   |0   |66.875   |
|Partial   |8|0   |1   |0   |35.625   |
|Full   |6   |1   |0   |0   |60.625   |
|Full   |8|1   |1   |1   |57.500   |


$\bar{Y}_{Full,4 \: pH}=b_0+(b_1\times1)+(b_2\times1)+(b_3\times1)$

$\bar{Y}_{Full,4 \: pH}=b_0+b_1+b_2+b_3$

$\bar{Y}_{Full,4 \: pH}=\bar{Y}_{partial,6}+(\bar{Y}_{Full,6}-\bar{Y}_{partial,6})+(\bar{Y}_{partial,4 \: pH}-\bar{Y}_{partial,6})+b_3$

$\bar{Y}_{Full,4 \: pH}=\bar{Y}_{Full,6}+\bar{Y}_{partial,4 \: pH}-\bar{Y}_{partial,6}+b_3$

$b_3=\bar{Y}_{partial,6}-\bar{Y}_{Full,6}+\bar{Y}_{Full,4 \: pH}-\bar{Y}_{partial,4 \: pH}$

$b_3=66.875-60.625+57.500-35.625$

$b_3=28.125$

## Two-Factor Analysis of Variance Hypotheses Being Tested

Simultaneous analysis of two factors and measurement of mean response

Case. 1: equal replication

Terminology:

One *factor* termed A and one *factor* termed B

*b* is the number of levels in B

Researchers have sought to examine the effects of various types of music on agitation levels in patients in early and middle stages of Alzheimer's disease. 

Patients were selected based on their form of Alzheimer's disease. Three forms of music were tested: easy listening, Mozart, and piano interludes. The response variable agitation level was scored.

What is (are) the null hypothesis(ese) being tested?

|Group|Piano Interlude|Mozart|Easy Listening|
|---|---|---|---|
|Early Stage ALzheimer's |21   |9   |29   |
|                        |24   |12   |26   |
|                        |22   |10   |30   |
|                        |18   |5   |24   |
|                        |20   |9   |26   |
|Middle Stage Alzheimer's|22   |14   |15   |
|                        |20   |18   |18   |
|                        |25   |11   |20   |
|                        |18   |9   |13   |
|                        |20   |13   |19   |

Plot these data (means) on a single figure such that cell-level means can be evaluated.

|Group|Piano Interlude|Mozart|Easy Listening|
|---|---|---|---|
|Early Stage ALzheimer's |21   |9   |29   |
|                        |24   |12   |26   |
|                        |22   |10   |30   |
|                        |18   |5   |24   |
|                        |20   |9   |26   |
|Middle Stage Alzheimer's|22   |14   |15   |
|                        |20   |18   |18   |
|                        |25   |11   |20   |
|                        |18   |9   |13   |
|                        |20   |13   |19   |


## Three-Factor Analysis of Variance 

What hypotheses are being tested?

Evaluate respiratory rate of crabs (ml O2 hr-1) 

Factors: 

+ Sex
+ Species
+ Temperature

|Species 1|   |   |   |   |   |
|---|---|---|---|---|---|
|Low Temp   |   |Med Temp   |   |High Temp   |   |
|Male   |Female   |Male   |Female   |Male  |Female   |
|1.9   |1.8   |2.3   |2.4   |2.9   |3.0   |
|1.8   |1.7   |2.1   |2.7   |2.8   |3.1   |
|1.6   |1.4   |2.0   |2.4   |3.4   |3.0   |
|1.4   |1.5   |2.6   |2.6   |3.2   |2.7   |


|Species 2|   |   |   |   |   |
|---|---|---|---|---|---|
|Low Temp   |   |Med Temp   |   |High Temp   |   |
|Male   |Female   |Male   |Female   |Male  |Female   |
|2.1   |2.3   |2.4   |2.0   |3.6   |3.1   |
|2.0   |2.0   |2.6   |2.3   |3.1   |3.0   |
|1.8   |1.9   |2.7   |2.1   |3.4   |2.8   |
|2.2   |1.7   |2.3   |2.4   |3.2   |3.2   |


|Species 3|   |   |   |   |   |
|---|---|---|---|---|---|
|Low Temp   |   |Med Temp   |   |High Temp   |   |
|Male   |Female   |Male   |Female   |Male  |Female   |
|1.1   |1.4   |2.0   |2.4   |2.9   |3.2   |
|1.2   |1.0   |2.1   |2.6   |2.8   |2.9   |
|1.0   |1.3   |1.9   |2.3   |3.0   |2.8   |
|1.4   |1.2   |2.2   |2.2   |3.1   |2.9   |


## Multiway Factorial ANOVA Hypotheses Being Tested

|Popcorn|Oil Amt.|Batch|Yeild|
|---|---|---|---|
|Plain  |Little   |Large   |8.2   |
|Gourmet|Little   |Large   |8.6   |
|Plain  |Lots   |Large   |10.4   |
|Gourmet|Lots   |Large   |9.2   |
|Plain  |Little   |Small   |9.9   |
|Gourmet|Little   |Small   |12.1   |
|Plain  |Lots   |Small   |10.6   |
|Gourmet|Lots   |Small   |18.0   |
|Plain  |Little   |Large   |8.8   |
|Gourmet|Little   |Large   |8.2   |
|Plain  |Lots   |Large   |8.8   |
|Gourmet|Lots   |Large   |9.8   |
|Plain  |Little   |Small   |10.1   |
|Gourmet|Little   |Small   |15.9   |
|Plain  |Lots   |Small   |7.4   |
|Gourmet|Lots   |Small   |16.0   |


## Hypotheses Being Tested

Three factor ANOVA:

+ H~O~: Yield is the same in all three Batch sizes
+ H~O~ : Yield is the same in all three Oil amounts
+ H~O~ : Yield is the same in all three Popcorn types
+ H~O~ : The mean yield is the same for all levels of batch, independent of oil amount (Batch X Oil)
+ H~O~ : The mean yield is the same for all levels of Oil amount, independent of popcorn type (Oil X Type)
+ H~O~ : The mean yield is the same for all levels of batch, independent of popcorn type (Batch X Type)
+ H~O~ : Differences in mean Yield among the batch, oil amount, and popcorn type are independent of the other factors
      + (Batch X Type X Oil)
  

|   |Df   |Sum Sq   |Mean Sq   |F value   |Pr(>F)   |
|---|---|---|---|---|---| 
|popcorn type                      |1   |3.062   |3.062   |0.1731   |0.6883   |
|oil amount                        |1   |0.062   |0.062   |0.0035   |0.9541   |
|batch size                        |1   |52.562   |52.562   |2.9717   |0.1230   |
|popcorn type:oil amount           |1   |27.562   |27.562   |1.5583   |0.2472   |
|popcorn type:batch size           |1   |14.062   |14.062   |0.7951   |0.3986   |
|oil amount:batch size             |1   |0.063   |0.063    |0.0035   |0.9541   |
|popcorn type:oil amount:batch size|1   |1.563   |1.563    |0.0883   |0.7739   |
|Residuals                         |8   |141.500   |17.687   |   |   |

## Interaction Effects

Experiment: we are interested in oxygen consumption of two species of limpets in different concentration of seawater.

+ Factor A is the species of limpet (levels, a)
+ Factor B is the concentration of SW as a function of maximum salinity - 100, 75, and 50 % (levels, b)

|Completed anova   |   |   |   |
|---|---|---|---|
|Source of variation|df|SS|MS|
|Species   |1  |16.6380   |16.638 ns   |
|salinities|2  |10.3566   |5.178 ns   |
|Sp X Sal  |2  |194.8907  |97.445 **   |
|Error     |42 |401.5213  |9.560   |
|Total     |47 |623.4066  |   |

When the two factors are identified as A and B, the interaction is identified as the A X B interaction.

Variability not accounted for by A and B alone.

Interaction: The effect of one factor in the presence of a particular level of another factor.

There is an interaction between two factors if the effect of one factor depends on the levels of the second factor. 

|   |Species   |Species   |   |
|---|---|---|---|
|Seawater Concentration|A. scabra|A. digitalis|Mean|
|100%   |10.56   |7.43   |9.00   |
|75%   |7.89   |7.34   |10.11   |
|50%   |12.17   |12.33   |9.76   |
|Mean   |10.21   |9.03   |9.62   |

```{r, results='hide',message=FALSE, echo=FALSE}
scabra <- c(12.17,7.89,10.56)
digitalis <- c(12.33,7.34,7.43)

plot(x = c(50,75,100), y = scabra, xlim = c(0,100), type = "l", xlab = "% Seawater", ylim = c(0,12.5), ylab = expression(paste(mu,"l", O[2],"/mg dry body wt/min at ", 22^o, "C" )))
lines(x = c(50,75,100), y = digitalis, lty = 2)
legend("topleft", legend = c("A. scabra", "A. digitalis"), lty = c(1,2), bty ="n")
```

+ The response to salinity differs between the two species

+ At 75% salinity A. scabra consumes the least oxygen and A. digitalis consumes the most. 

+ Therefore a simple statement about the species response to salinity is not clear; all we can really say is:

+ The pattern of response to changes in salinity differed in the two species.

+ The difference among levels of one factor is not constant at all levels of the second factor

+ "It is generally not useful to speak of an individual factor effect - even if its *F* is significant - if there is a significant interaction effect" - Zar


## Post-hoc Tests

Tukey test - balanced, orthogonal designs

+ Step one: is to arrange and number all five sample means in order of increasing magnitude
+ Calculate the pairwise difference in sample means.

We use a *t*-test "analog" to calculate a q-statistic

Scheffe's test

Examine multiple contrasts:

+ ideas is to compare combinations of samples to each other instead of the comparison among individual k levels.

Compare the mean outflow volume of four different rivers: 5 vs 1,2,3,4

$H_0:\mu_2/3+\mu_4/3+\mu_3/3-\mu_5=0$

$H_0:(\mu_2+\mu_4+\mu_3)/3=\mu_5$

$c_2=\frac{1}{3}, \: c_4=\frac{1}{3}, \: c_3=\frac{1}{3}, \: and \: c_5=-1$

<Br>

Alternatives multiple contrasts:

$H_0:(\mu_1+\mu_5)/2-(\mu_2+\mu_4+\mu_3)/3=0$

$H_0:\mu_1-(\mu_2+\mu_4+\mu_3)/3$

Test Statistic:

$s=\frac{|\sum c_i\bar{Y}_i|}{SE}$

+ Where

$SE=\sqrt{s^2(\sum \frac{c^2_i}{n_i})}$

+ and the critical value of the test is

$S_{\alpha}=\sqrt{(k-1)F_{\alpha(1),k-1,N-k}}$

## Non-Parametric Tests

Violations of the assumptions

We assume equality of variance - ANOVA is a robust test.

Robust to unbalanced design.

How to deal with outliers:  

+ use in analysis if they are valid data. 

Test of normality: Shapiro Wilks

Test of equality of variance: Bartletts test.

Nonparametric analysis of variance.

If k > 2

Kruskal-Wallis test - analysis of variance by rank

Power increases with sample size.

If k = 2 the Kruskal-Wallis is equivalent to the Mann-Whitney test.

$H=\frac{12}{N(N+1)}\sum ^k_{i=1}\frac{R^2_i}{n_i}-3(N+1)$

If there are tied ranks

+ H needs to be corrected using a correction factor C.

$C=1-\frac{\sum t}{N^3-N}$

$H_c=\frac{H}{C}$

$\sum t=\sum (t^3_i-t_i)$

+ t~i~  is the number of tied ranks.

A limnologist obtained eight containers of water from each of four ponds. The pH of each water sample was measured. The data are arranged in ascending order within each pond. (One of the containers from pond 3 was lost, so n~3~ = 7, instead of 8; but the test procedure does not require equal numbers of data in each group.) The rank of each datum is shown parenthetically. 

+ H~0~: pH is the same in all four ponds. 
+ H~A~: pH is not the same in all four ponds.

<Br>

|Pond 1|Pond 2|Pond 3|Pond 4|
|---|---|---|---|
|7.68 (1)   |7.71 (6*)    |7.74 (13.5*)|7.71 (6*)  |
|7.69 (2)   |7.73 (10)    |7.75 (16)   |7.71 (6*)  |
|7.70 (3.5*)|7.74 (13.5*) |7.77 (18)   |7.74 (13.5)|
|7.70 (3.5*)|7.74 (13.5*) |7.78 (20*)  |7.79 (22)  |
|7.72 (8)   |7.78 (20*)   |7.80 (23.5*)|7.81 (26*)   |
|7.73 (10*) |7.78 (20*)   |7.81 (26*)  |7.85 (29)   |
|7.73 (10*) |7.80(23.5*)  |7.84 (28)   |7.87 (30)   |
|7.76 (17)  |7.81 (26*)   |            |7.91 (31)   |

|n~1~ =8    |n~2~ =8   |n~3~ =7   |n~4~ =8   |
|---|---|---|---|
|R~1~ =55   |R~2~ =132.5   |R~3~ =145   |R~4~ =163.5   |

*tied ranks


$H=\frac{12}{N(N+1)}\sum ^k_{i=1}\frac{R^2_i}{n_i}-3(N+1)$

$=\frac{12}{32(32)}[\frac{55^2}{8}+\frac{132.5^2}{8}+\frac{145^2}{7}+\frac{163.5^2}{8}]-3(32)$

$=11.876$


|Pond 1|Pond 2|Pond 3|Pond 4|
|---|---|---|---|
|7.68 (1)   |7.71 (6*)    |7.74 (13.5*)|7.71 (6*)  |
|7.69 (2)   |7.73 (10)    |7.75 (16)   |7.71 (6*)  |
|7.70 (3.5*)|7.74 (13.5*) |7.77 (18)   |7.74 (13.5)|
|7.70 (3.5*)|7.74 (13.5*) |7.78 (20*)  |7.79 (22)  |
|7.72 (8)   |7.78 (20*)   |7.80 (23.5*)|7.81 (26*)   |
|7.73 (10*) |7.78 (20*)   |7.81 (26*)  |7.85 (29)   |
|7.73 (10*) |7.80(23.5*)  |7.84 (28)   |7.87 (30)   |
|7.76 (17)  |7.81 (26*)   |            |7.91 (31)   |

|n~1~ =8    |n~2~ =8   |n~3~ =7   |n~4~ =8   |
|---|---|---|---|
|R~1~ =55   |R~2~ =132.5   |R~3~ =145   |R~4~ =163.5   |

*tied ranks


$\sum t=\sum (t^3_i-t_i)$

$\sum t=(2^3-2)+(3^3-3)+(3^3-3)+(4^3-4)+(3^3-3)+(2^3-2)+(3^3-)$

$\sum t=168$

|Pond 1|Pond 2|Pond 3|Pond 4|
|---|---|---|---|
|7.68 (1)   |7.71 (6*)    |7.74 (13.5*)|7.71 (6*)  |
|7.69 (2)   |7.73 (10)    |7.75 (16)   |7.71 (6*)  |
|7.70 (3.5*)|7.74 (13.5*) |7.77 (18)   |7.74 (13.5)|
|7.70 (3.5*)|7.74 (13.5*) |7.78 (20*)  |7.79 (22)  |
|7.72 (8)   |7.78 (20*)   |7.80 (23.5*)|7.81 (26*)   |
|7.73 (10*) |7.78 (20*)   |7.81 (26*)  |7.85 (29)   |
|7.73 (10*) |7.80(23.5*)  |7.84 (28)   |7.87 (30)   |
|7.76 (17)  |7.81 (26*)   |            |7.91 (31)   |

|n~1~ =8    |n~2~ =8   |n~3~ =7   |n~4~ =8   |
|---|---|---|---|
|R~1~ =55   |R~2~ =132.5   |R~3~ =145   |R~4~ =163.5   |

*tied ranks

$\sum t=\sum (t^3_i-t_i)$

$\sum t=(2^3-2)+(3^3-3)+(3^3-3)+(4^3-4)+(3^3-3)+(2^3-2)+(3^3-)$

$\sum t=168$

$C=1-\frac{\sum t}{N^3-N}=1-\frac{168}{31^3-31}=1-\frac{168}{29760}=0.9944$

$H_c=\frac{H}{C}=\frac{11.876}{0.9944}=11.943$

$\nu=k-1=3$


$F=\frac{(N-k)H_c}{(k-1)(N-1-H_c)}=\frac{(31-4)(11.943)}{(4-1)(31-1-11.943)}=5.95$

$F_{0.05(1),3,26}=2.98$

Reject H~0~

# Logistic Regression

When and why do we use logistic regression?

+ Binary
+ Multinomial

Theory behind logistic regression

Interpreting logistic regression

## When and Why?

To predict an outcome variable that is categorical from one or more categorical or continuous predictor variables.

Used because having a categorical  outcome variable violates the assumption of linearity in normal regression.

## We will focus on regression with one predictor

$P(Y)=\frac{1}{1+e^{-(b_0+b_1X_1+\varepsilon_i)}}$

Outcome

+ We predict the probability of the outcome occurring

b~0~ and b~1~

+ Can be thought of in much the same way as multiple regression
+ Note the normal regression equation forms part of the logistic regression equation

## Assessing the Model

log - likelihood $=\sum^N_{i=1}[\Upsilon_iln(P(\Upsilon_i))+(1-\Upsilon_1)ln(1-P(\Upsilon_i))]$

The log-likelihood statistic

+ Analogous to the residual sum of squares in multiple regression
+ It is an indicator of how much unexplained information there is after the model has been fitted.
+ Large values indicate poorly fitting statistical models.

## Assessing Changes in Models

It's possible to calculate a log-likelihood for different models and to compare these models by looking at the difference between their log-likelihoods.

$\chi^2=2[LL(new)-LL(baseline)]$

$(df=k_{new}-k_{baseline})$

## Assessing Predictors: The Odds Ratio

$\mbox{odss ratio}=\frac{\mbox{odds after a unit change in the predictor}}{\mbox{odds before a unit change in the predictor}}$

Indicates the change in odds resulting from a unit change in the predictor.

## Things That Can Go Wrong

Assumptions from linear regression:

+ Linearity
+ Independence of errors
+ Multicollinearity - we will spend time on this in multiple linear regression.

## Complete Separation

When the outcome variable can be perfectly predicted.

+ E.g. predicting whether someone is a burglar, your teenage son or your cat based on weight.
+ Weight is a perfect predictor of cat/burglar unless you have a very fat cat indeed!

```{r echo=FALSE, warning=FALSE}
par(mfrow = c(1,2))

weight <- c(rnorm(25, 50,10), rnorm(25,70,10))
p <- c(rep(0,25),rep(1,25))

g=glm(p~weight,family=binomial) # run a logistic regression model (in this case, generalized linear model with logit link). see ?glm
plot(weight, p, pch = 24, ylab = "Probability of Outcome", xlab = "Weight (kg)")
curve(predict(g,data.frame(weight=x),type="resp"),add=TRUE) # draws a curve based on prediction from logistic regression model

points(weight,fitted(g),pch=20) 


weight <- c(rnorm(25, 10,2), rnorm(25,50,5))
w2 <- c(rnorm(25, 5, 2), rnorm(25, 50, 15))
p <- c(rep(0,25),rep(1,25))

g=glm(p~weight,family=binomial)
g2=glm(p~w2,family=binomial)


plot(weight, p, pch = 24, xlab = "Weight (kg)", ylab = "Probability of Outcome")
points(w2, p, pch = 16)
curve(predict(g,data.frame(weight=x),type="resp"),add=TRUE) 
curve(predict(g2,data.frame(w2=x),type="resp"),add=TRUE, lty = 2) 


```

<Br>

## Over Dispersion

Over dispersion is where the variance is larger than expected from the model.

This can be caused by violating the assumption of independence.

This problem makes the standard errors too small! 

## An Example

Predictors of a treatment intervention.

Participants

+ 113 adults with a medical problem

Outcome:

+ Cured (1) or not cured (0).

Predictors:

+ Intervention: intervention or no treatment.

<Br>
