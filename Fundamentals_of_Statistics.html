<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Fundamentals_of_Statistics.utf8.md</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Biometry COA 606</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="syllabus.html">Course Syllabus</a>
</li>
<li>
  <a href="Fundamentals_of_Statistics.html">Fundamentals_of_Statistics</a>
</li>
<li>
  <a href="Linear_Models.html">Linear Models</a>
</li>
<li>
  <a href="Probability.html">Probability</a>
</li>
<li>
  <a href="Maximum_Likelihood.html">Maximum Liklihood</a>
</li>
<li>
  <a href="AIC.html">AIC</a>
</li>
<li>
  <a href="Multilevel_Models.html">Multilevel Models</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="introduction-to-statistical-analysis" class="section level1">
<h1>1 Introduction to statistical analysis</h1>
<div id="statistical-analysis" class="section level3">
<h3>1.1 Statistical Analysis</h3>
<p>Kachigan (1986) defines statistical analysis as the:</p>
<ul>
<li><p>Collection,</p></li>
<li><p>Organization, and</p></li>
<li><p>Interpretation of data according to well-defined procedures</p></li>
</ul>
<p>In this course we will use Kachigan’s definition as a framework and focus on aspects of collection (experimental design), organization (using descriptive and inferential approaches), and interpretation.</p>
<p>One of the primary facets of quantitative analysis is the need to be creative in your approaches - the methods outlined in this class provide the foundation for analysis but the practitioner is encouraged to explore the methods and practices in their discipline to best address the needs for interpretation.</p>
</div>
<div id="the-research-process" class="section level3">
<h3>1.2 The Research Process</h3>
<p>A.J. Underwood described the “components of design in ecological field experiments” (2009) and this stereotyped approach can be used in the natural sciences and other disciplines.</p>
<p>Scientific investigation is most efficient and leads to strong inference when the</p>
</div>
<div id="generating-and-testing-theories" class="section level3">
<h3>1.3 Generating and Testing Theories</h3>
<p>Field et al. (2012)</p>
<p>Theory</p>
<ul>
<li>A hypothesized general principle or set of principles that explains known findings about a topic and from which new hypotheses can be generated.</li>
</ul>
<p>Hypothesis</p>
<ul>
<li>A prediction from a theory.</li>
<li>E.g. the number of people turning up for a Big Brother audition that have narcissistic personality disorder will be higher than the general level (1%) in the population.</li>
</ul>
<p>Falsification</p>
<ul>
<li>The act of disproving a theory or hypothesis.</li>
</ul>
</div>
<div id="data-collection" class="section level3">
<h3>1.4 Data Collection</h3>
<p>Krebs: “Not everything that can be measured should be measured.”</p>
<p>Need to define variables.</p>
<ul>
<li>Anything that can be measured and exhibits variation among entities, space, and or time.</li>
</ul>
</div>
<div id="the-research-process-1" class="section level3">
<h3>1.5 The Research Process</h3>
<p><Br></p>
<!--
![Figure 1.5.1](C:/Users/w954394/Pictures/Biometry_FTP/Figures/Fund_Stats_Fig/Figure 1.1.png)
-->
<p><Br></p>
</div>
<div id="levels-of-measurement" class="section level3">
<h3>1.6 Levels of Measurement</h3>
<div id="categorical-entities-are-divided-into-distinct-categories" class="section level4">
<h4>Categorical (entities are divided into distinct categories):</h4>
<p>Binary variable: There are only two categories.</p>
<ul>
<li>e.g. dead or alive.</li>
</ul>
<p>Nominal variable: There are more than two categories.</p>
<ul>
<li>e.g. whether someone is an omnivore, vegetarian, vegan, or fruitarian.</li>
</ul>
<p>Ordinal variable: The same as a nominal variable but the categories have a logical order.</p>
<ul>
<li>e.g. whether people got a fail, a pass, a merit or a distinction in their exam.</li>
</ul>
</div>
<div id="continuous-entities-get-a-distinct-score" class="section level4">
<h4>Continuous (entities get a distinct score):</h4>
<p>Interval variable: Equal intervals on the variable represent equal differences in the property being measured.</p>
<ul>
<li>e.g. the difference between 6 and 8 is equivalent to the difference between 13 and 15.</li>
</ul>
<p>Ratio variable: The same as an interval variable, but the ratios of scores on the scale must also make sense.</p>
<ul>
<li>e.g. a score of 16 on an anxiety scale means that the person is, in reality, twice as anxious as someone scoring 8.</li>
</ul>
</div>
</div>
<div id="data-collection-2-how-to-measure" class="section level3">
<h3>1.7 Data Collection 2: How to Measure</h3>
<div id="correlational-research-measurative" class="section level4">
<h4>Correlational research (measurative):</h4>
<p>Observing what naturally goes on in the world without directly interfering with it.</p>
</div>
<div id="experimental-research-manipulative" class="section level4">
<h4>Experimental research (manipulative):</h4>
<p>One or more variable is systematically manipulated to see their effect (alone or in combination) on an outcome variable.</p>
<p>Statements can be made about cause and effect.</p>
</div>
</div>
<div id="experimental-research-methods" class="section level3">
<h3>1.8 Experimental Research Methods</h3>
<p>Cause and Effect (Hume, 1748)</p>
<p>1.Cause and effect must occur close together in time (contiguity).</p>
<p>2.The cause must occur before an effect does.</p>
<p>3.The effect should never occur without the presence of the cause.</p>
<p><Br></p>
</div>
</div>
<div id="frequency-distributions" class="section level1">
<h1>2 Frequency Distributions</h1>
<div id="frequency-distributions-and-variation" class="section level3">
<h3>2.1 Frequency Distributions and Variation</h3>
<p>-Frequency and cumulative frequency</p>
<p>-Theoretical frequency distributions</p>
<ul>
<li>Normal Distribution</li>
</ul>
<p>-Measures of central tendency</p>
<p>-Variation and associated measures</p>
</div>
<div id="frequency-distributions-1" class="section level3">
<h3>2.2 Frequency Distributions</h3>
<p>Univariate.</p>
<p>A tally of how frequently occurring a value is among a set of measured objects.</p>
<p>What does a qualitative evaluation of the frequency distribution allow?</p>
<p>Data reduction technique with a tradeoff.</p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p><em>Figure(s) 2.2.1 and 2.2.2</em></p>
<p><Br></p>
<center>
<p>Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data</p>
<table>
<thead>
<tr class="header">
<th>Phosphorous concentration</th>
<th>Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8.1 to 8.2</td>
<td>2</td>
</tr>
<tr class="even">
<td>8.2 to 8.3</td>
<td>6</td>
</tr>
<tr class="odd">
<td>8.3 to 8.4</td>
<td>8</td>
</tr>
<tr class="even">
<td>8.4 to 8.5</td>
<td>11</td>
</tr>
<tr class="odd">
<td>8.5 to 8.6</td>
<td>17</td>
</tr>
<tr class="even">
<td>8.6 to 8.7</td>
<td>17</td>
</tr>
<tr class="odd">
<td>8.7 to 8.8</td>
<td>24</td>
</tr>
<tr class="even">
<td>8.8 to 8.9</td>
<td>18</td>
</tr>
<tr class="odd">
<td>8.9 to 9.0</td>
<td>13</td>
</tr>
<tr class="even">
<td>9.0 to 9.1</td>
<td>10</td>
</tr>
<tr class="odd">
<td>9.1 to 9.2</td>
<td>4</td>
</tr>
</tbody>
</table>
Total frequency = 130 = n
</center>
<p><em>Table 2.2.1:  A Frequency Table of Continuous Data</em></p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p><em>Figure 2.2.3</em></p>
<p><Br></p>
</div>
<div id="cumulative-frequency-distributions" class="section level3">
<h3>2.3 Cumulative Frequency Distributions</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p><em>Figure 2.3.1</em></p>
<p><Br></p>
<p>Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Cummulative Frequency</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Phosphorous concentration</td>
<td>Frequency</td>
<td>Starting with Low Values</td>
<td>Starting with High Values</td>
</tr>
<tr class="even">
<td>8.15 to 8.25</td>
<td>2</td>
<td>2</td>
<td>130</td>
</tr>
<tr class="odd">
<td>8.25 to 8.35</td>
<td>6</td>
<td>8</td>
<td>128</td>
</tr>
<tr class="even">
<td>8.35 to 8.45</td>
<td>8</td>
<td>16</td>
<td>122</td>
</tr>
<tr class="odd">
<td>8.45 to 8.55</td>
<td>11</td>
<td>27</td>
<td>114</td>
</tr>
<tr class="even">
<td>8.55 to 8.65</td>
<td>17</td>
<td>44</td>
<td>130</td>
</tr>
<tr class="odd">
<td>8.65 to 8.75</td>
<td>17</td>
<td>61</td>
<td>86</td>
</tr>
<tr class="even">
<td>8.75 to 8.85</td>
<td>24</td>
<td>85</td>
<td>69</td>
</tr>
<tr class="odd">
<td>8.85 to 8.95</td>
<td>18</td>
<td>103</td>
<td>45</td>
</tr>
<tr class="even">
<td>8.95 to 9.05</td>
<td>13</td>
<td>116</td>
<td>27</td>
</tr>
<tr class="odd">
<td>9.05 to 9.15</td>
<td>10</td>
<td>126</td>
<td>14</td>
</tr>
<tr class="even">
<td>9.15 to 9.25</td>
<td>4</td>
<td>130</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Total frequency = 130 = n</p>
<p><em>Table 2.3.1:  A Frequency Table of Continuous Data</em></p>
<p><Br></p>
</div>
<div id="what-interval-to-choose" class="section level3">
<h3>2.4 What Interval to Choose?</h3>
<p>Domain knowledge</p>
</div>
<div id="frequency-distributions-2" class="section level3">
<h3>2.5 Frequency Distributions</h3>
<div id="empirical-vs.-theoretical" class="section level4">
<h4>Empirical vs. Theoretical</h4>
<p>Empirical: observed finite</p>
<p>Theoretical: infinite number of observations</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p><em>Figure 2.5.1</em></p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><em>Figure 2.5.2</em></p>
<p><Br></p>
</div>
<div id="focus-on-theoretical-distribution" class="section level4">
<h4>Focus on Theoretical Distribution:</h4>
<p>Move from determining the frequency of a value (empirical) to the relative frequency of an interval of values.</p>
<ul>
<li>Relabel y-axis, why?</li>
<li>Every value of x has a non-zero probability, cannot know how often a single value occurs.</li>
<li>Sum of the probability of the obsevations is 100%.</li>
</ul>
<p>So, we can only determine the relative frequency of an interval of values.</p>
<p>Use calculus to get the integral.</p>
<p>There are many types of theoretical distributions - we will focus on the primary ones used in frequentist statistics.</p>
</div>
</div>
<div id="everything-you-ever-wanted-to-know-about-statistics-aims-and-objectives" class="section level3">
<h3>2.6 Everything You Ever Wanted to Know about Statistics Aims and Objectives</h3>
<p>Know what a statistical model is and why we use them.</p>
<ul>
<li>The mean</li>
</ul>
<p>Know what the ‘fit’ of a model is and why it is important.</p>
<ul>
<li>The standard deviation</li>
</ul>
<p>Distinguish models for samples and populations.</p>
</div>
<div id="pie-charts" class="section level3">
<h3>2.7 Pie Charts</h3>
<p>Try to think of other ways to display your data.</p>
</div>
<div id="a-simple-statistical-model" class="section level3">
<h3>2.8 A Simple Statistical Model</h3>
<p>In statistics we fit models to our data (i.e. we use a statistical model to represent what is happening in the real world).</p>
<p>The mean is a hypothetical value (i.e. it doesn’t have to be a value that actually exists in the data set).</p>
<p>The mean is simple statistical model.</p>
</div>
<div id="measures-of-central-tendency" class="section level3">
<h3>2.9 Measures of Central Tendency</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p><em>Figure 2.9.1</em></p>
<p><Br></p>
</div>
<div id="the-mean" class="section level3">
<h3>2.10 The Mean</h3>
<p>The mean is the sum of all scores divided by the number of scores.</p>
<p>The mean is also the value from which the (squared) scores deviate least (it has the least error).</p>
</div>
<div id="the-mean-example" class="section level3">
<h3>2.11 The Mean: Example</h3>
<p>Collect some data - teaching scores from students for a lecturer:</p>
<ul>
<li>1, 3, 4, 3, 2</li>
</ul>
<p>Add them up</p>
<p>Divide by the number of scores, n:</p>
</div>
<div id="the-mean-as-a-model" class="section level3">
<h3>2.12 The Mean as a Model</h3>
<p><span class="math inline">\(outcome_i = (model) + error_i\)</span></p>
<!-- \begin{aligned} -->
<!-- outcome_{lecturer1}&=(\bar{X}) + error_{lecturer1}\\ -->
<!-- &=2.6+error_{lecturer1} -->
<!-- \end{aligned} -->
</div>
<div id="variation-measuring-the-fit-of-the-model" class="section level3">
<h3>2.13 Variation: Measuring the ‘Fit’ of the Model</h3>
<p>The mean is a model of what happens in the real world: the typical score.</p>
<p>It is not a perfect representation of the data.</p>
<p>How can we assess how well the mean represents reality?</p>
</div>
<div id="the-dispersion-range" class="section level3">
<h3>2.14 The Dispersion: Range</h3>
<p>The Range</p>
<ul>
<li>The smallest score subtracted from the largest</li>
</ul>
<p>Example</p>
<ul>
<li>Number of friends of 11 Facebook users.</li>
<li>22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252</li>
<li>Range = 252 - 22 = 230</li>
<li>Very biased by outliers, why?</li>
</ul>
</div>
<div id="the-dispersion-the-interquartile-range" class="section level3">
<h3>2.15 The Dispersion: The Interquartile Range</h3>
<p>Quartiles</p>
<ul>
<li>The three values that split the sorted data into four equal parts.</li>
<li>Second quartile = median.</li>
<li>Lower quartile = median of lower half of the data.</li>
<li>Upper quartile = median of upper half of the data.</li>
</ul>
</div>
<div id="a-perfect-fit" class="section level3">
<h3>2.16 A Perfect Fit</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><em>Figure 2.16.1</em></p>
<p><Br></p>
</div>
<div id="calculating-error" class="section level3">
<h3>2.17 Calculating ‘Error’</h3>
<p>A deviation is the difference between the mean and an actual data point.</p>
<p>Deviations can be calculated by taking each score and subtracting the mean from it:</p>
<center>
<span class="math inline">\(deviation = x_i - \bar{x}\)</span>
</center>
<p><Br> <Br> <Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><em>Figure 2.17.1</em></p>
<p><Br></p>
</div>
<div id="use-the-total-error" class="section level3">
<h3>2.18 Use the Total Error?</h3>
<p>We could just take the error between the mean and the data and add them.</p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Score</th>
<th>Mean</th>
<th>Deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.6</td>
<td>-1.6</td>
</tr>
<tr class="even">
<td>2</td>
<td>2.6</td>
<td>-0.6</td>
</tr>
<tr class="odd">
<td>3</td>
<td>2.6</td>
<td>0.4</td>
</tr>
<tr class="even">
<td>3</td>
<td>2.6</td>
<td>0.4</td>
</tr>
<tr class="odd">
<td>4</td>
<td>2.6</td>
<td>1.4</td>
</tr>
<tr class="even">
<td></td>
<td>Total =</td>
<td>0</td>
</tr>
</tbody>
</table>
<p><em>Table 2.18.1:  Total error</em></p>
<p><Br></p>
<center>
<span class="math inline">\(\Sigma(X - \bar{X}) = 0\)</span>
</center>
<p><Br> <Br> <Br></p>
</div>
<div id="mean-absolute-deviation-mad" class="section level3">
<h3>2.19 Mean Absolute Deviation (MAD)</h3>
<p>Also, called MAE (mean absolute error).</p>
<center>
<span class="math inline">\(MAE = \frac{sum|x_i - \bar{x}|}{n}\)</span>
</center>
<p><Br></p>
</div>
<div id="sum-of-squared-errors" class="section level3">
<h3>2.20 Sum of Squared Errors</h3>
<p>We could add the deviations to find out the total error.</p>
<p>Deviations cancel out because some are positive and others negative.</p>
<p>Therefore, we square each deviation.</p>
<p>If we add these squared deviations we get the sum of squared errors (SS).</p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Score</th>
<th>Mean</th>
<th>Deviation</th>
<th>Sqaured Deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.6</td>
<td>-1.6</td>
<td>2.56</td>
</tr>
<tr class="even">
<td>2</td>
<td>2.6</td>
<td>-0.6</td>
<td>0.36</td>
</tr>
<tr class="odd">
<td>3</td>
<td>2.6</td>
<td>0.4</td>
<td>0.16</td>
</tr>
<tr class="even">
<td>3</td>
<td>2.6</td>
<td>0.4</td>
<td>0.16</td>
</tr>
<tr class="odd">
<td>4</td>
<td>2.6</td>
<td>1.4</td>
<td>1.96</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>Total</td>
<td>5.20</td>
</tr>
</tbody>
</table>
<p><em>Table 2.20.1:  Sqaured Deviation</em></p>
<p><Br></p>
<center>
<span class="math inline">\(SS = \Sigma(X - \bar{X})^2 = 5.20\)</span>
</center>
</div>
<div id="variance" class="section level3">
<h3>2.21 Variance</h3>
<p>The sum of squares is a good measure of overall variability, but is dependent on the number of scores.</p>
<p>We calculate the average variability by dividing by the number of scores (n).</p>
<p>This value is called the variance (s<sup>2</sup>).</p>
<center>
<span class="math inline">\(varience (s^2) = \frac{SS}{N-1} = \frac{\Sigma(x_i-\bar{x})^2}{N-1} = \frac{5.20}{4} = 1.3\)</span>
</center>
<p><Br></p>
</div>
<div id="standard-deviation" class="section level3">
<h3>2.22 Standard Deviation</h3>
<p>The variance has one problem: it is measured in units squared.</p>
<p>This isn’t a very meaningful metric so we take the square root value.</p>
<p>This is the standard deviation(s).</p>
<p><span class="math inline">\(s = \sqrt\frac{\Sigma^n_{i=1}(x_i-\bar{x})^2}{n} = \sqrt\frac{5.20}{5} = 1.02\)</span></p>
<p><Br></p>
</div>
<div id="things-to-remember" class="section level3">
<h3>2.23 Things to Remember</h3>
<p>The sum of squares, variance, and standard deviation represent the same thing:</p>
<ul>
<li>The fit of the mean to the data</li>
<li>The variability in the data</li>
<li>How well the mean represents the observed data</li>
<li>Error</li>
</ul>
</div>
<div id="same-mean-different-standard-deviation" class="section level3">
<h3>2.24 Same Mean, Different Standard Deviation</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p><em>Figures 2.24.1 and 2.24.2</em></p>
<p><Br></p>
</div>
<div id="the-standard-deviation-and-the-shape-of-a-distribution" class="section level3">
<h3>2.25 The Standard Deviation and the Shape of a Distribution</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p><em>Figures 2.25.1 and 2.25.2</em></p>
<p><Br></p>
</div>
<div id="areas-under-the-normal-curve-for-various-standard-deviation" class="section level3">
<h3>2.26 Areas Under the Normal Curve for Various Standard Deviation</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p><em>Figure 2.26.1</em></p>
<p><Br></p>
</div>
<div id="going-beyond-the-data-z-scores" class="section level3">
<h3>2.27 Going Beyond the Data: z-scores</h3>
<p>z-scores</p>
<ul>
<li>Standardising a score with respect to the other scores in the group.</li>
<li>Expresses a score in terms of how many standard deviations it is away from the mean.</li>
<li>The distribution of z-scores has a mean of 0 and SD = 1.</li>
</ul>
<center>
<span class="math inline">\(z = \frac{X-\bar{X}}{S}\)</span>
</center>
<p><Br></p>
</div>
<div id="properties-of-z-scores" class="section level3">
<h3>2.28 Properties of z-scores</h3>
<p>1.96 cuts off the top 2.5% of the distribution.</p>
<p>-1.96 cuts off the bottom 2.5% of the distribution.</p>
<p>As such, 95% of z-scores lie between -1.96 and 1.96.</p>
<p>99% of z-scores lie between -2.58 and 2.58.</p>
<p>99.9% of them lie between -3.29 and 3.29.</p>
<p><Br></p>
</div>
</div>
<div id="probability-sampling-and-parameters" class="section level1">
<h1>3 Probability, Sampling, and Parameters</h1>
<div id="probability-sampling-and-parameters-1" class="section level3">
<h3>3.1 Probability, Sampling, and Parameters</h3>
<ul>
<li>Probability</li>
<li>Random variables</li>
<li>Sampling from a normal distribution</li>
<li>Sampling from a bionomial distribution</li>
<li>Parameter Estimation</li>
<li>Student’s t-distribution</li>
</ul>
</div>
<div id="probability" class="section level3">
<h3>3.2 Probability</h3>
<p>SETS: A collection of items.</p>
<p>Element: on item of a set.</p>
<p>Subsets have multiple elements and are themselves sets.</p>
<p>Outcome set.</p>
<ul>
<li>In an experiment (or other phenomenon that yields results to observe), there is a set (usually very large) of possible outcomes. Let us refer to this as the outcome set.</li>
</ul>
<p>Intersect - the common elements in two sets.</p>
<p>Mutually exclusive - Sets with no elements in common , null intersect.</p>
<p>Union - the combination of elements in two sets - what element is in either set or both?</p>
<p>Complement - the remainder of outcomes in a set that are not in a subset</p>
<div id="venn-diagram" class="section level4">
<h4>Venn Diagram</h4>
<!--
![Figure 3.2.1](C:/Users/w954394/Pictures/Biometry_FTP/Figures/Fund_Stats_Fig/Figure 17.png) 
-->
<p><Br></p>
</div>
</div>
<div id="sampling-distributions" class="section level3">
<h3>3.3 Sampling Distributions</h3>
<p>Random samples:</p>
<ul>
<li>Every possible member of the population has an equal probability of being included in the sample.</li>
<li>Ex: scientific exit polling vs. twitter polls</li>
<li>Think of a normally distributed frequency distribution</li>
<li>Only statistically valid data that can be used for analysis - the first assumption of parametric statistics.</li>
</ul>
</div>
<div id="the-normal-distribution" class="section level3">
<h3>3.4 The Normal Distribution</h3>
<center>
<span class="math inline">\(Y_i = \frac{1}{\sigma\sqrt2\pi}e^-(X_i-\mu)^2/2\sigma^2\)</span>
</center>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p><em>Figure 3.4.1</em></p>
<p><Br></p>
<center>
<span class="math inline">\(Y_i = \frac{1}{\sigma\sqrt2\pi}e^-(X_i-\mu)^2/2\sigma^2\)</span>
</center>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p><em>Figures 3.4.2 and 3.4.3</em></p>
<p><Br></p>
</div>
<div id="sample-from-a-normal-distribution" class="section level3">
<h3>3.5 Sample from a Normal Distribution</h3>
<p>Normal distribution sampling theorem:</p>
<ul>
<li>Sampling distribution is normal when the population distribution is normal.</li>
<li>Sample mean = population mean</li>
<li>Sample sd = population s</li>
</ul>
</div>
<div id="sampling-from-a-non-normal-distribution" class="section level3">
<h3>3.6 Sampling from a Non-Normal Distribution</h3>
<p>Central limit theorem:</p>
<ul>
<li>Drawing samples from a non-normal population will result in a sampling distribution that will approximate a normal distribution when the sample size is large.</li>
<li>How can we test this - what approach would you use?</li>
</ul>
<div id="we-will-explore-the-sampling-of-distributions-in-the-in-class-assignment." class="section level4">
<h4>We will explore the sampling of distributions in the in-class assignment.</h4>
</div>
</div>
<div id="parameter-estimation" class="section level3">
<h3>3.7 Parameter Estimation</h3>
<div id="point-estimation" class="section level4">
<h4>Point estimation</h4>
<p>Single value based on sampling</p>
<p>Often mean and sd</p>
<p>Goal:</p>
<ol style="list-style-type: decimal">
<li>Achieve and unbiased estimate - Long-run (infinite sampling)</li>
<li>Derive an efficient estimator - Fewest number samples to obtain accurate value</li>
</ol>
</div>
</div>
<div id="interval-estimation" class="section level3">
<h3>3.8 Interval Estimation</h3>
<p>How close is the true parameter to the estimate?</p>
<p>Confidence intervals and limits.</p>
</div>
</div>
<div id="hypothesis-testing-and-power" class="section level1">
<h1>4 Hypothesis Testing and Power</h1>
<p>Hypothesis Testing</p>
<p>Power</p>
<p>Assumptions of parametric statistics</p>
<div id="statistical-hypothesis-testing" class="section level3">
<h3>4.1 Statistical Hypothesis Testing</h3>
<p>State:</p>
<ul>
<li>H<sub>O</sub></li>
<li>H<sub>A</sub></li>
</ul>
<p>Declare</p>
<ul>
<li>Alpha level</li>
</ul>
<p>Collect Data</p>
<p>Compare the test statistic to the critical value (determined by alpha)</p>
<p>State the resulting probability</p>
<p>State testable hypothesis</p>
<ul>
<li>These are a set of mutually exclusive and exhaustive outcomes.</li>
<li>Test statistic will support one or the other.</li>
<li>H<sub>O</sub> <sub>Null</sub><br />
</li>
<li>H<sub>A</sub> <sub>Alternative</sub></li>
</ul>
<p><Br></p>
<p><span class="math inline">\(H_0: \mu = 0, H_A:\mu \ne 0\)</span></p>
<p><span class="math inline">\(H_0: \mu = 3.5 cm, H_A:\mu \ne 3.5 cm\)</span></p>
<p><span class="math inline">\(H_0: \mu = 10.5 kg, H_A:\mu \ne 10.5 kg\)</span></p>
<p><Br></p>
</div>
<div id="example-use-z-score-to-test-mean" class="section level3">
<h3>4.2 Example: Use z-score to Test Mean</h3>
<p>Is the mean fuel consumption of a population of busses equal to 20 mpg?</p>
<p>What is the null hypothesis?</p>
<p>We need information about the population</p>
<ul>
<li>Mean</li>
<li>Population standard deviation</li>
<li>Calculate z-score</li>
<li>What is the probability that the mean is 20 mpg given: + Sigma = 0.3, Mean = 19.1</li>
</ul>
</div>
<div id="evaluate-z-score" class="section level3">
<h3>4.3 Evaluate z-score</h3>
<p>What is the probability that we would get this z score?</p>
<p>Hypothetical z-scores</p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><em>Figure 4.3.1</em></p>
<p><Br> <Br></p>
<table>
<thead>
<tr class="header">
<th>z</th>
<th>.00</th>
<th>.01</th>
<th>.02</th>
<th>.03</th>
<th>.04</th>
<th>.05</th>
<th>.06</th>
<th>.07</th>
<th>.08</th>
<th>.09</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.0</td>
<td>.5000</td>
<td>.5040</td>
<td>.5080</td>
<td>.5120</td>
<td>.5160</td>
<td>.5199</td>
<td>.5239</td>
<td>.5279</td>
<td>.5319</td>
<td>.5359</td>
</tr>
<tr class="even">
<td>0.1</td>
<td>.5398</td>
<td>.5438</td>
<td>.5478</td>
<td>.5517</td>
<td>.5557</td>
<td>.5596</td>
<td>.5636</td>
<td>.5675</td>
<td>.5714</td>
<td>.5753</td>
</tr>
<tr class="odd">
<td>0.2</td>
<td>.5793</td>
<td>.5832</td>
<td>.5871</td>
<td>.5910</td>
<td>.5948</td>
<td>.5987</td>
<td>.6026</td>
<td>.6064</td>
<td>.6103</td>
<td>.6141</td>
</tr>
<tr class="even">
<td>0.3</td>
<td>.6179</td>
<td>.6217</td>
<td>.6255</td>
<td>.6293</td>
<td>.6331</td>
<td>.6368</td>
<td>.6406</td>
<td>.6443</td>
<td>.6480</td>
<td>.6517</td>
</tr>
<tr class="odd">
<td>0.4</td>
<td>.6554</td>
<td>.6591</td>
<td>.6628</td>
<td>.6664</td>
<td>.6700</td>
<td>.6736</td>
<td>.6772</td>
<td>.6808</td>
<td>.6844</td>
<td>.6879</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>.6915</td>
<td>.6950</td>
<td>.6985</td>
<td>.7019</td>
<td>.7054</td>
<td>.7088</td>
<td>.7123</td>
<td>.7157</td>
<td>.7190</td>
<td>.7224</td>
</tr>
<tr class="odd">
<td>0.6</td>
<td>.7257</td>
<td>.7291</td>
<td>.7324</td>
<td>.7357</td>
<td>.7389</td>
<td>.7422</td>
<td>.7454</td>
<td>.7486</td>
<td>.7517</td>
<td>.7549</td>
</tr>
<tr class="even">
<td>0.7</td>
<td>.7580</td>
<td>.7611</td>
<td>.7642</td>
<td>.7673</td>
<td>.7704</td>
<td>.7734</td>
<td>.7764</td>
<td>.7794</td>
<td>.7823</td>
<td>.7852</td>
</tr>
<tr class="odd">
<td>0.8</td>
<td>.7881</td>
<td>.7910</td>
<td>.7939</td>
<td>.7967</td>
<td>.7995</td>
<td>.8023</td>
<td>.8051</td>
<td>.8078</td>
<td>.8106</td>
<td>.8133</td>
</tr>
</tbody>
</table>
<p><em>Table 4.3.1:  Standard Normal Posibilities</em></p>
<p><Br></p>
</div>
<div id="is-it-meaningful-significance-level" class="section level3">
<h3>4.4 Is it meaningful? Significance Level</h3>
<p>Declare</p>
<ul>
<li>Alpha level</li>
<li>p vs. alpha</li>
<li>Define prior to test</li>
<li>Two tail and one tail test</li>
</ul>
</div>
<div id="alpha" class="section level3">
<h3>4.5 Alpha</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p><em>Figure 4.5.1</em></p>
<p><Br></p>
</div>
<div id="statistical-hypothesis-testing-1" class="section level3">
<h3>4.6 Statistical Hypothesis Testing</h3>
<p>Ex: Look to see if the population mean is not different from some specified value.</p>
<p>H<sub>O</sub>: u = 0</p>
<p>H<sub>A</sub>: u is not equal 0</p>
<p>Introduce the idea of a critical value</p>
<ul>
<li>Alpha level of 0.05</li>
</ul>
<p>We have data taken from the weight change in horses given some medical treatment.</p>
<p>We are interested to know if the mean change in weight that we found +1.29 kg is significantly different from 0 kg.</p>
<ul>
<li>We calculate the z-score and find that Z = 1.45</li>
</ul>
<p><span class="math inline">\(P(mean \ge 1.29) = P(Z \ge 1.45) = ?\)</span></p>
<p><span class="math inline">\(P(mean \le 1.29) = P(Z \le 1.45) = ?\)</span></p>
<p>Z = 1.96 is the rejection region at 2.5%</p>
<ul>
<li>This is the ‘region of rejection’</li>
</ul>
<p>Now we have a way to objectively reject or accept the null hypothesis.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p><em>Figure 4.6.1</em></p>
<p><Br></p>
</div>
<div id="one--and-two-tailed-tests" class="section level3">
<h3>4.7 One- and Two-Tailed Tests</h3>
<p>Alternative to testing ‘is the value different.’</p>
<p>In some cases we care about the direction of the difference.</p>
<p>Use one-tailed test</p>
<ul>
<li>In general, one-tailed hypotheses about a mean are: + <span class="math inline">\(H_0:\mu\ge\mu_0\)</span> and <span class="math inline">\(H_A:\mu&lt;\mu_0\)</span></li>
<li>In which case, H<sub>0</sub> is rejected if the test statistic is in the left-hand tail of the distribution or: + <span class="math inline">\(H_0:\mu\le\mu_0\)</span> and <span class="math inline">\(H_A:\mu&gt;\mu_0\)</span></li>
</ul>
<p>Contrast the region of rejection for these.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p><em>Figures 4.7.1 and 4.7.2</em></p>
<p><Br></p>
</div>
<div id="type-1-and-type-2-errors" class="section level3">
<h3>4.8 Type 1 and Type 2 Errors</h3>
<p>Sometimes we:</p>
<ul>
<li>Reject the null hypothesis when it is true.</li>
<li>Accept the alternative hypothesis when it is false.</li>
</ul>
<p>Type 1 error or alpha error - frequency of rejecting H<sub>0</sub> when it is true.</p>
<p>Type 1 error rate is equal to alpha.</p>
<p>Type 1 error: “rejecting the null hypothesis when it is true.”</p>
<p>Type 1 error or ‘<span class="math inline">\(\alpha\)</span> error’ is equal to <span class="math inline">\(\alpha\)</span></p>
<p>Now we have some criteria to choose alpha.</p>
<p>So if your <span class="math inline">\(\alpha\)</span>, or critical value is 0.10</p>
<ul>
<li>We have a 10% probability of rejecting the null hypothesis when we should have, in fact, accepted it.</li>
</ul>
<div id="type-1-alpha-error" class="section level4">
<h4>Type 1 (alpha) Error</h4>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p><em>Figure 4.8.1</em></p>
<p><Br></p>
<p>Type 2 error: “accepting the null hypothesis when it is false.”</p>
<p>Type 2 error or ‘<span class="math inline">\(\beta\)</span> error’ is equal to <span class="math inline">\(\beta\)</span>.</p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>If H<sub>0</sub> is true</th>
<th>If H<sub>0</sub> is false</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>If H<sub>0</sub> is rejected</td>
<td>Type I error</td>
<td>No error</td>
</tr>
<tr class="even">
<td>If H<sub>0</sub> is not rejected</td>
<td>No error</td>
<td>Type II error</td>
</tr>
</tbody>
</table>
<p><em>Table 4.8.1:  Two Types of Errors in Hypothesis Testing</em></p>
<p><Br></p>
<p>Thought experiments:</p>
<ul>
<li>Ex. Endangered species conservation</li>
<li>Ex. Pharmaceutical testing</li>
</ul>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>If H<sub>0</sub> is true</th>
<th>If H<sub>0</sub> is false</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>If H<sub>0</sub> is rejected</td>
<td><span class="math inline">\(\alpha\)</span></td>
<td><span class="math inline">\(1-\beta\)</span> (“power”) No error</td>
</tr>
<tr class="even">
<td>If H<sub>0</sub> is not rejected</td>
<td>No error <span class="math inline">\(1-\alpha\)</span></td>
<td><span class="math inline">\(\beta\)</span></td>
</tr>
</tbody>
</table>
<p><em>Table 4.8.2:  Long-term Probabilities of Outcomes in Hypothesis Testing</em></p>
<p><Br></p>
</div>
</div>
<div id="power" class="section level3">
<h3>4.9 Power</h3>
<p>Power: the probability that a statistical test will reject a null hypothesis when it is false (proper rejection).</p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p><em>Figure 4.9.1</em></p>
<p><Br> <Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p><em>Figure 4.9.2</em></p>
<p><Br></p>
<div id="leafs-power-simulation-in-r" class="section level4">
<h4>Leaf’s power simulation in R</h4>
<p><Br></p>
</div>
</div>
<div id="what-influences-statistical-power" class="section level3">
<h3>4.10 What Influences Statistical Power?</h3>
<p>Picture 2 (Copied and pasted from book?)</p>
</div>
<div id="assumptions" class="section level3">
<h3>4.11 Assumptions</h3>
<p>Assumptions - When broken then we are not able to make inference or accurate descriptions about reality.</p>
<p>Thus our models are flawed descriptions and inferences will be compromised.</p>
<ul>
<li>Assumptions of parametric tests based on the normal distribution.</li>
<li>Understand the assumption of normality.</li>
<li>Understand homogeneity of variance.</li>
<li>Know how to correct problems (with respect to the assumptions of normality) in the data.</li>
</ul>
<p>Parametric tests based on the normal distribution assume:</p>
<ul>
<li><p>Normally distributed + Distribution of samples + Model distribution (residuals)</p></li>
<li>Homogeneity of variance</li>
<li><p>Interval or ratio level data + Some data are intrinsically not normally distributed.</p></li>
<li><p>Independence of observation</p></li>
</ul>
</div>
<div id="the-normal-distribution-review" class="section level3">
<h3>4.12 The Normal Distribution Review</h3>
<p>Commonly the distribution of measurements (frequency of data collected from interval data) have a bell shaped distribution</p>
<p>Parameters of the model determine its shape.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p><em>Figure 4.12.1</em></p>
<p><Br></p>
<p>Two-parameter distribution</p>
<p><span class="math inline">\(f(x,\mu,\sigma)=\frac{1}{\sigma\sqrt2\pi}e^-(x-\mu)^2/2\sigma^2\)</span></p>
<p>Symmetric around mu</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p><em>Figure 4.12.2</em></p>
<p><Br></p>
</div>
<div id="distribution-of-samples-central-limit-theorem" class="section level3">
<h3>4.13 Distribution of Samples Central Limit Theorem</h3>
<p>How well do samples represent the population?</p>
<p>A key foundation of frequentist statistics - samples are random variables: When we take a sample from the population we are taking one of many possible samples.</p>
<p>Thought experiment - take many, many samples from a population.</p>
<p>Do we expect all sample to have the same mean value (the same sample mean)?</p>
<ul>
<li>No, there is variation in the samples - ‘Sampling variation.’</li>
</ul>
<p>The frequency histogram of samples is the sampling distribution.</p>
<p>Analog to standard deviation</p>
<ul>
<li>SD how well does model fit the data</li>
</ul>
<p>We can take the standard deviation of the sample mean.</p>
<ul>
<li>Termed ‘Standard Error of the Sampling mean’</li>
<li>Or ‘Standard Error’</li>
</ul>
<p>If the sample is large then sampling error can be approximated:</p>
<center>
<span class="math inline">\(SE=\frac{s}{\sqrt n}\)</span>
</center>
<p><Br></p>
</div>
<div id="assumption-1-sample-observations-and-associated-deviations-are-normally-distributed." class="section level3">
<h3>4.14 Assumption #1: Sample observations and associated deviations are normally distributed.</h3>
<p>We will review how to check these assumptions in this lecture and the following lab.</p>
</div>
<div id="assumption-2-homogeneity-of-variance." class="section level3">
<h3>4.15 Assumption #2: Homogeneity of Variance.</h3>
<p>Data taken from groups must have homogenous variance.</p>
<p>Homogenous does not mean ‘equal’ but equal in the probabilistic sense.</p>
<p>We will review how to check these assumptions in this lecture and the following lab.</p>
</div>
<div id="assumption-3-interval-and-ratio-scale" class="section level3">
<h3>4.16 Assumption #3: Interval and Ratio Scale</h3>
<p>Continuous variables</p>
<ul>
<li>Interval scale (equal intervals between measurements).</li>
<li>Ratio scale - Conversion of interval data such that ratio of measurements was meaningful.</li>
</ul>
<p>Ordinal data - rankings -</p>
<ul>
<li>Darker, faster, shorter and might label these 1,2,3,4,5 to reflect increases in magnitude.</li>
<li>Really convey less information - data condensation.</li>
</ul>
<p>Nominal or Categorical data</p>
<ul>
<li>Example are public surveys</li>
<li>Willingness to vote for a candidate? Economic class, Taxonomic categories.</li>
</ul>
</div>
<div id="assumption-4" class="section level3">
<h3>4.17 Assumption #4</h3>
<p>Observations are independent</p>
<p>The measurement of one sample does not influence the measurement of another sample.</p>
<ul>
<li>Measurements taken in space and time are examples - experimenter needs to determine when there is zero correlation between the samples.</li>
<li>Behavioral Example - the opinion of one person influences the behavior of another person and hence the measurements are correlated.</li>
</ul>
</div>
<div id="assessing-normality" class="section level3">
<h3>4.18 Assessing Normality</h3>
<p>We don’t have access to the population distribution so we usually test the observed data</p>
<p>Graphical displays</p>
<ul>
<li>Q-Q plot (or P-P plot)</li>
<li>Histogram</li>
</ul>
<p>Kolmogorov-Smirnov</p>
<p>Shapiro-Wilk</p>
</div>
<div id="assessing-homogeneity-of-variance" class="section level3">
<h3>4.19 Assessing Homogeneity of Variance</h3>
<p>Figures</p>
<p>Levene’s test</p>
<ul>
<li>Tests if variances in different groups are the same.</li>
<li>Significant = variances not ‘equal’</li>
<li>Non-significant = variances are ‘equal’</li>
</ul>
<p>Variance ratio</p>
<ul>
<li>With 2 or more groups</li>
<li>VR = largest variance/smallest variance</li>
<li>If VR &lt; 2, homogeneity can be assumed</li>
</ul>
</div>
<div id="correcting-data-problems" class="section level3">
<h3>4.20 Correcting Data ‘Problems’</h3>
<p>Log transformation log(X<sub>i</sub>) or log(X<sub>i</sub> +1)</p>
<ul>
<li>Reduce positive skew.</li>
</ul>
<p>Square root transformation:</p>
<ul>
<li>Also reduces positive skew. Can also be useful for stabilizing variance.</li>
</ul>
<p>Reciprocal transformation (1/ X<sub>i</sub>):</p>
<ul>
<li>Dividing 1 by each score also reduces the impact of large scores.</li>
<li>This transformation reverses the scores</li>
<li>You can avoid this by reversing the scores before the transformation, 1/(X<sub>Highest</sub> - X<sub>i</sub>).</li>
</ul>
</div>
<div id="to-transform-or-not" class="section level3">
<h3>4.21 To Transform Or Not</h3>
<p>Transforming the data helps as often as it hinders the accuracy of F</p>
<p>The central limit theorem: sampling distribution will be normal in samples &gt; 40 anyway.</p>
<ul>
<li><p>Transforming the data changes the hypothesis being tested. + E.g. when using a log transformation and comparing means, you change from comparing arithmetic means to comparing geometric means.</p></li>
<li>In small samples it is tricky to determine normality one way or another. The consequences for the statistical model of applying the ‘wrong’ transformation could be worse than the consequences of analysing the untransformed scores.</li>
<li><p>Alternative - use non-parametric statistics or Bayesian approaches.</p></li>
</ul>
</div>
</div>
<div id="parameteric-and-non-parameteric-correlation" class="section level1">
<h1>5 Parameteric and Non-Parameteric Correlation</h1>
<div id="correlation" class="section level3">
<h3>5.1 Correlation</h3>
<p>Linear dependence of variables</p>
<ul>
<li>Scatterplots</li>
<li>Covariance</li>
<li>Pearson’s correlation coefficient</li>
</ul>
<p>Nonparametric measures</p>
<ul>
<li>Spearman’s rho</li>
<li>Kendall’s tau</li>
</ul>
<p>Interpreting correlations</p>
<ul>
<li>Causality</li>
</ul>
<p>Partial correlations</p>
</div>
<div id="what-is-a-correlation" class="section level3">
<h3>5.2 What is a Correlation?</h3>
<p>It is a way of measuring the extent to which two variables are related.</p>
<p>It measures the pattern of responses across variables.</p>
<p><Br></p>
<div id="small-relationship" class="section level4">
<h4>Small Relationship</h4>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p><em>Figure 5.2.1</em></p>
<p><Br></p>
</div>
<div id="positive-relationship" class="section level4">
<h4>Positive Relationship</h4>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p><em>Figure 5.2.2</em></p>
<p><Br></p>
</div>
<div id="negative-relationship" class="section level4">
<h4>Negative Relationship</h4>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p><em>Figure 5.2.3</em></p>
<p><Br></p>
</div>
</div>
<div id="measuring-relationships" class="section level3">
<h3>5.3 Measuring Relationships</h3>
<p>We need to see whether as one variable increases, the other increases, decreases or stays the same.</p>
<p>This can be done by calculating the covariance.</p>
<ul>
<li>We look at how much each score deviates from the mean.</li>
<li>If both variables deviate from the mean by the same amount, they are likely to be related.</li>
</ul>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Participant</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adverts Watched</td>
<td>5</td>
<td>4</td>
<td>4</td>
<td>6</td>
<td>8</td>
<td>5.4</td>
<td>1.67</td>
</tr>
<tr class="even">
<td>Packets Bought</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>13</td>
<td>15</td>
<td>11</td>
<td>2.92</td>
</tr>
</tbody>
</table>
<p><em>Table 5.3.1:  Measuring Relationships</em></p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Participant</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adverts Watched</td>
<td>5</td>
<td>4</td>
<td>4</td>
<td>6</td>
<td>8</td>
<td>5.4</td>
<td>1.67</td>
</tr>
<tr class="even">
<td>Packets Bought</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>13</td>
<td>15</td>
<td>11</td>
<td>2.92</td>
</tr>
<tr class="odd">
<td>Advertiser Residual</td>
<td>-0.04</td>
<td>-1.4</td>
<td>-1.4</td>
<td>0.6</td>
<td>2.6</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Packets residual</td>
<td>-3</td>
<td>-2</td>
<td>-1</td>
<td>2</td>
<td>4</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><em>Table 5.3.2:  Measuring Relationships</em></p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p><em>Figure 5.3.1</em></p>
<p><Br></p>
</div>
<div id="re-examination-of-variance" class="section level3">
<h3>5.4 Re-examination of Variance</h3>
<p>The variance tells us by how much scores deviate from the mean for a single variable.</p>
<p>It is closely linked to the sum of squares.</p>
<p>Covariance is similar - it tells is by how much scores on two variables differ from their respective means.</p>
<p><span class="math inline">\(variance=\frac{\Sigma(x_i - \bar{x})}{N-1}\)</span></p>
<p><span class="math inline">\(variance=\frac{\Sigma(x_i - \bar{x})(x_i - \bar{x})}{N-1}\)</span></p>
<p><Br></p>
</div>
<div id="covariance" class="section level3">
<h3>5.5 Covariance</h3>
<p>Calculate the error between the mean and each subject’s score for the first variable (x).</p>
<p>Calculate the error between the mean and their score for the second variable (y).</p>
<p>Multiply these error values.</p>
<p>Add these values and you get the cross product deviations.</p>
<p>The covariance is the average cross-product deviations:</p>
<p>Table 1000 =</p>
<p><Br></p>
<p><span class="math inline">\(N_1(a_1,_1) + N_2(a_2,_1)\)</span></p>
<p><span class="math inline">\(cov(x,y)=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{N-1}\)</span></p>
<p><span class="math inline">\(=\frac{(-0.4)(-3)+(-1.4)(-2)+(-1.4)(-1)+(0.6)(2)+(2.6)(4)}{4}\)</span></p>
<p><span class="math inline">\(=\frac{1.2+2.8+1.4+1.2+10.4}{4}\)</span></p>
<p><span class="math inline">\(=\frac{17}{4}\)</span></p>
<p><span class="math inline">\(=4.25\)</span></p>
<p><Br></p>
</div>
<div id="problems-with-covariance" class="section level3">
<h3>5.6 Problems with Covariance</h3>
<p>Dependent on the units of measurement.</p>
<ul>
<li>E.g. the covariance of two variables measured in miles might be 4.25, but if the same scores are converted to kilometres, the covariance is 11.</li>
</ul>
<p>One solution: standardize it</p>
<ul>
<li>Divide by the standard deviations of both variables.</li>
</ul>
<p>The standardized version of covariance is known as the correlation coefficient.</p>
<ul>
<li>It is relatively unaffected by units of measurement.</li>
</ul>
</div>
<div id="the-correlation-coefficient" class="section level3">
<h3>5.7 The Correlation Coefficient</h3>
<p><span class="math inline">\(r=\frac{cov_xy}{s_xs_y}\)</span></p>
<p><span class="math inline">\(=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{(N-1)s_xs_y}\)</span></p>
<p><Br> <Br></p>
<p><span class="math inline">\(r=\frac{cov_xy}{s_xs_y}\)</span></p>
<p><span class="math inline">\(=\frac{4.25}{1.67 * 2.92}\)</span></p>
<p><span class="math inline">\(=0.87\)</span></p>
<p><Br></p>
<p>Termed Pearson-product moment correlation coefficient</p>
<p>It is a testable hypothesis</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p><em>Figure 5.7.1</em></p>
<p><Br></p>
<p>It is a testable hypothesis</p>
<p>Testing <span class="math inline">\(H_0: \rho=0\)</span> versus <span class="math inline">\(H_A: \rho\ne0\)</span></p>
<p>The standard error of the correlation coefficient is calculated as:</p>
<p><span class="math inline">\(S_r=\sqrt\frac{1-r^2}{n-2}\)</span></p>
<p>It is a testable hypothesis</p>
<p>r = 0.870</p>
<p>n = 12 (new data set, with more samples)</p>
<p>The critical value is:</p>
<p><Br></p>
<p><span class="math inline">\(t=\frac{r}{S_r}= \frac{0.870}{0.156}= 5.58\)</span></p>
<p>t<sub>0.05(2),10</sub> =2.228</p>
<p>Testing <span class="math inline">\(H_0: \rho=0\)</span> versus <span class="math inline">\(H_A: \rho\ne0\)</span></p>
<p><Br></p>
<p>It varies between -1 and +1</p>
<ul>
<li>0 = no relationship</li>
</ul>
<p>It is an effect size</p>
<ul>
<li>?.1 = small effect</li>
<li>?.3 = medium effect</li>
<li>?.5 = large effect</li>
</ul>
<p>Coefficient of determination, r2</p>
<ul>
<li>By squaring the value of r you get the proportion of variance in one variable shared by the other.</li>
</ul>
</div>
<div id="correlation-and-causality" class="section level3">
<h3>5.8 Correlation and Causality</h3>
<p>The third-variable problem:</p>
<ul>
<li>In any correlation, causality between two variables cannot be assumed because there may be other measured or unmeasured variables affecting the results.</li>
<li>Could be many latent variables..</li>
</ul>
<p>Direction of causality:</p>
<ul>
<li>Correlation coefficients say nothing about which variable causes the other to change.</li>
</ul>
</div>
<div id="non-parametric-correlation" class="section level3">
<h3>5.9 Non-parametric Correlation</h3>
<p>Spearman’s rho</p>
<ul>
<li>Pearson’s correlation on the ranked data</li>
</ul>
<p>Kendall’s tau</p>
<ul>
<li>“Better” than Spearman’s for small samples</li>
</ul>
</div>
<div id="spearman-rank-correlation-coefficient" class="section level3">
<h3>5.10 Spearman Rank Correlation Coefficient</h3>
<p>d is the difference between two numbers in each pair of ranks</p>
<p>n = number of pairs of data</p>
<p><span class="math inline">\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)</span></p>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
</tr>
</tbody>
</table>
<p><em>Table 5.10.1:  Spearman Rank Correlation Coefficient</em></p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
<th>Rank 1</th>
<th>Rank 2</th>
<th>d</th>
<th>d<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
<td>1</td>
<td>3</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><em>Table 5.10.2:  Spearman Rank Correlation Coefficient</em></p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
<th>Rank 1</th>
<th>Rank 2</th>
<th>d</th>
<th>d<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><em>Table 5.10.3:  Spearman Rank Correlation Coefficient</em></p>
<p><Br></p>
<p><span class="math inline">\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)</span></p>
<p><span class="math inline">\(=1-(\frac{6*6}{3(3^2 - 1)})\)</span></p>
<p><Br></p>
<p>We can use this value as the calculated r value</p>
<p>The critical value is a two tailed value with n</p>
</div>
<div id="partial-and-semi-partial-correlations" class="section level3">
<h3>5.11 Partial and Semi-partial Correlations</h3>
<p>Partial correlation:</p>
<ul>
<li>Measures the relationship between two variables, controlling for the effect that a third variable has on them both.</li>
</ul>
<p>Semi-partial correlation:</p>
<ul>
<li>Measures the relationship between two variables controlling for the effect that a third variable has on only one of the others.</li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
