---
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---
# 1 AIC 

### 1.1 Model selection via AIC

Model selection:  how do you choose between (and compare) alternate models?

Multimodel inference:  how do you combine information from more than 1 model?

"Strategic science" is dependent on multiple working hypotheses.

This principal encourages hard thinking to identify candidate alternative hypotheses

Steps:  

1. Observations  
2. Model  
3. Hypothesis  
4. Null Hypothesis  
5. Test or Experiment  
    a. Retain Null Hypothesis (start back at 1)  
    b. Reject Null Hypothesis  
        - Refine the model or hypothesis

AICc is suggesting that a particular model is best in the sense of trading-off bias versus variance of the fitted model parameters, for a given sample size (i.e., "best" in the K-L information sense).

So there is a set of a priori hypotheses

+ How does this contrast with our standard approach to hypothesis testing?

Traditional methods have been based on test statistics and their associated p-values. 

From the p-value comes the arbitrary judgement concerning statistical significance and the dichotomous ruling about rejection or failure to reject the null hypothesis.

### 1.2 P-Value

The meaning of a P value. The definition of a P value might seem strained. One starts with experimental data and then computes a test statistic that has a known distribution by design (e.g., t or F or z or x^2^). A P value is then the probability that a test statistic would be as large as, or larger than, the actual computed test statistic, given the null. It is a "tail probability" and for this reason (there are others) P values are not evidence (Royall 1997). People often want to "redefine" such P values to be the probability of the null, given the data - this is seriously wrong (see e.g. Selke et al. 2001). 

### 1.3 The Information-Theoretic Approach

So, this approach is more efficient

The idea is to obtain quantitative measures of the strength of evidence for each alternative hypothesis help advance science.

### 1.4 Recall the Likelihood Principle.

"Within the framework of a statistical model, a set of data supports one statistical hypothesis better than the other if the likelihood of the first hypothesis, on the data, exceeds the likelihood of the second hypothesis". (Edwards 1972)

### 1.5 AIC 

1970's Akaike established the information criteria (AIC)

AIC=$-2log(\mathcal{L})+2K$

Equivalent to:

AIC=2k+n Log(RRS/n)

### 1.6 We Seek to Compare Multiple Candidate Hypotheses

AICmin is the candidate model with the lowest AIC value

$\Delta_i=AICc_i-AICc_{min}$, \ for \ $i=1,2,\ldots,R.$

The delta AIC  values are on a continuous scale on information and are interpretable as:

+ Delta AIC above 20 have no support
+ Delta AIC above 9 to 11 have little support

### 1.7 In Practice 

So, first we compute AIC for each model

Then calculate the delta AIC value for each

+ Each model is an alternative hypothesis about reality

Select the one or ones with the smallest information loss or smallest distance from "full reality"

### 1.8 AIC 

We select the model with smallest value of AIC (i.e. closest to "truth").

AIC will identify the best model in the set, even if all the models are poor!

It is the researcher's (your) responsibility that the set of candidate models includes well founded, realistic models.

### 1.9 Bias in Model Fitting

Process error

Measurement error

Model selection error - many alternative models may fit similarly.

### 1.10 Bias and Uncertainty in Model Selection

Model Selection Bias: Chance inclusion of meaningless variables in a model will produce a biased underestimate of the variance, and a corresponding exaggeration of the precision of the model (the problem with "fishing expeditions").

Model Selection Uncertainty: The fact that we are using data (with uncertainty) to both estimate parameters and to select the best model necessarily introduces uncertainty into the model selection process.

* See discussion on pages 43-47 of Burnham and Anderson 

### 1.11 Model Weighting and Averaging

How can we understand the effects of multiple models: Model averaging:

$\omega-i=(\mbox{exp}(-0.2\Delta_i))/(\sum^5_{k=1}\mbox{exp}(-0.5\Delta_k))$

Let Y~hat~ be the predicted value from the i^th^ model, model averages are the weighted mean prediction:

$\hat{\bar{Y}}=\sum^R_{i=1}\mathcal{W}_i\hat{Y}$

<Br>

### 1.12 Multimodel Evaluation

|Model description|Model natation|
|---|---|
|Male body size ('body')        |$\beta_0+\beta_{2i}X_{2i}$   |
|Food availability ('food')     |$\beta_0+\beta_4X_4$   |
|Male dominance ('status')      |$\beta_0+\beta_5X_5$   |
|Territory quality ('territory')|$\beta_0+\beta_7X_7$   |
|Body + Food                    |$\beta_0+\beta_{2i}X_{2i}+\beta_4X_4$   |
|Body + Status                  |$\beta_0+\beta_{2i}X_{2i}+\beta_5X_5$   |
|Body + Territory               |$\beta_0+\beta_{2i}X_{2i}+\beta_7X_7$   |
|Food + status                  |$\beta_0+\beta_4X_4+\beta_5X_5$   |
|Food + territory               |$\beta_0+\beta_4X_4+\beta_7X_7$   |
|Body + Food + Status           |$\beta_0+\beta_{2i}X_{2i}+\beta_4X_4+\beta_5X_5$   |
|Body + Food + Territory        |$\beta_0+\beta_{2i}X_{2i}+\beta_4X_4+\beta_7X_7$    |
|Body x Status                  |$\beta_0+\beta_{2i}X_{2i}+\beta_5X_5+\beta_{2i,5}(X_{2i}*X_5)$    |
|Body x Territory               |$\beta_0+\beta_{2i}X_{2i}+\beta_7X_7+\beta_{2i,7}(X_{2i}*X_7)$   |
|Food x Territory               |$\beta_0+\beta_4X_4+\beta_7X_7+\beta_{4,7}(X_4*X_7)$   |
|Intercept only                 |$\beta_0$   |

*Table 1.12.1: \ A model set for the example examining ecologocal factors and extra-pair paternity in a hypothetical bird species* 

<Br>

AIC can be used to compare i >= 2 models

Models can be nested or not.

Why not just always use a "global model" the highest parameterized model?

+ Each data set only has a finite amount of information. Each time a parameter estimate is made, the amount of "left over" information is reduced. 
+ The probability of finding spurious effects are increased.
+ Principal of parsimony

### 1.13 Parsimony

Suppose there exist multiple alternative  explanations for an occurrence. 

A more complex model (more parameters) is expected to have higher likelihood, so we need some way to penalize models with higher numbers of parameters..

+ In this case the simpler one is usually better. 
+ Another way of saying it is that the more assumptions you have to make, the more unlikely an explanation is.












































































































































































[LINK to another html file](about.html)
[LINK to another html file](cv.html)