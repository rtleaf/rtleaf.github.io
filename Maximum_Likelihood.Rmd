---
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---
# 1 Maximum Likelihood 

### 1.1 Maximum Likelihood

Find the parameters of a model that best fit the data.

Forms the foundation of Bayesian inference

### 1.2 Distributions of Discrete Variables

Random variables (the observed data)

+ Discrete
+ Are integer values

Example:

+ Binomial
+ Multinomial
+ Poisson
+ Negative binomial

### 1.3 Distributions of Continuous Variables

Random variables are continuous

Example:

+ Gaussian (normal)
+ Log normal
+ Gamma
+ Beta

### 1.4 PMF of Poisson

Probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value

P(Yi=k| rate \: parameter=r)=$\frac{e^{-r}r^k}{k!}$

In one unit of time we predict that Y~i~ = k

P(Yi=k| rate \: parameter=r)=$\frac{e^{-r}r^k}{k!}$

```{r echo = F}
plot(dpois(0:20,1), type = "o", xlab = "k", ylab = "P(X=k)")
lines(dpois(0:20,4), type = "o", pch = 16)
lines(dpois(0:20,10), type = "o", pch = 16, col = "grey")
legend("topright", pch = c(1,16,16), col = c("black", "black", "grey"), legend = c("lambda = 1", "lambda = 4", "lambda = 10"), bty = "n")

```



*Figure 1.4.1*

<Br>

### 1.5 The Poisson Distribution 

Description 

+ Density, distrubution function, quantile function and random generation for the Poisson distribution with parameter lambda.

Usage

+ dpois (x, lambda, log = FALSE)
+ ppois (q, lambda, lower.tail = TRUE, log.p = FALSE)
+ qpois (p, lambda, lower.tail = TRUE, log.p = FALSE)
+ rpois (n, lambda)

Arguments 

+ x - Vector of (non-negative integer) quantiles
+ q - Vector of quantiles 
+ p - vector of probabilities 
+ n - number of random values to return
+ lambda - vector of (non-negative) means
+ log, log.p - logical; if TRUE, probabilities p are given as log(p)
+ lower.tail - logical; if TRUE, (default), probabilities are P[X $\leq$ x], otherwise, P[X > x] 

### 1.6 Likelihood

P(Y~i~ | p)

Probability distribution of observing data Yi, given a particular parameter value, p

Subscript on Y indicates that there are many possible outcomes but only one possible parameter.

P(Yi=k| rate \: parameter=r)=$\frac{e^{-r}r^k}{k!}$

This expression is the probability of "data" given the hypothesis.

+ Data are k events in one unit time
+ Hypothesis is that the rate parameter is r

After collection of the data, the data are known.

Alternative hypotheses are different values of r.

Given the data, how likely are the possible hypotheses?

Introduce symbol: "L" likelihood

L(data | hypothesis), L(Y | p~m~)

Shift in thinking - m alternative parameters.

One set of data

Difference in likelihood and probability:

+ Probability: the hypothesis is known, data are unknown
+ Likelihood: data are known, hypothesis is not known

### 1.7 Likelihood in Practice

Generate data

```{r echo= F}
set.seed(5432)
hist(rnorm(500, 20,10), xlab = "Random Draws", main = "")
```


*Figure 1.7.1*

<Br>

Determine range of parameter values that are alternative hypotheses

+ best.guess.mu <- seq(15,25,by = 0.1)
+	best.guess.sig <- 5

Determine the probability that the data came from a distribution with a given parameter value

```{r echo = F}
set.seed(5432)
best.guess.mu <- seq(15,25,by = 0.1)
best.guess.sig <- 5
y <- rnorm(101,20,10)
mean <- mean(best.guess.mu)
n <- 22
like <- c()
like <- .5*n*log(2*pi) -.5*n*log(best.guess.sig) - (1/(2*best.guess.sig))*(best.guess.mu-mean)**2
plot(best.guess.mu, like)


```

*Figure 1.7.2*


