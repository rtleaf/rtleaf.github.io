\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{}
    \pretitle{\vspace{\droptitle}}
  \posttitle{}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  

\begin{document}

{
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{introduction-to-statistical-analysis}{%
\section{1 Introduction to statistical
analysis}\label{introduction-to-statistical-analysis}}

\hypertarget{statistical-analysis}{%
\subsubsection{1.1 Statistical Analysis}\label{statistical-analysis}}

Kachigan (1986) defines statistical analysis as the:

\begin{itemize}
\item
  Collection,
\item
  Organization, and
\item
  Interpretation of data according to well-defined procedures
\end{itemize}

In this course we will use Kachigan's definition as a framework and
focus on aspects of collection (experimental design), organization
(using descriptive and inferential approaches), and interpretation.

One of the primary facets of quantitative analysis is the need to be
creative in your approaches - the methods outlined in this class provide
the foundation for analysis but the practitioner is encouraged to
explore the methods and practices in their discipline to best address
the needs for interpretation.

\hypertarget{the-research-process}{%
\subsubsection{1.2 The Research Process}\label{the-research-process}}

A.J. Underwood described the ``components of design in ecological field
experiments'' (2009) and this stereotyped approach can be used in the
natural sciences and other disciplines.

Scientific investigation is most efficient and leads to strong inference
when the

\hypertarget{generating-and-testing-theories}{%
\subsubsection{1.3 Generating and Testing
Theories}\label{generating-and-testing-theories}}

Field et al.~(2012)

Theory

\begin{itemize}
\tightlist
\item
  A hypothesized general principle or set of principles that explains
  known findings about a topic and from which new hypotheses can be
  generated.
\end{itemize}

Hypothesis

\begin{itemize}
\tightlist
\item
  A prediction from a theory.
\item
  E.g. the number of people turning up for a Big Brother audition that
  have narcissistic personality disorder will be higher than the general
  level (1\%) in the population.
\end{itemize}

Falsification

\begin{itemize}
\tightlist
\item
  The act of disproving a theory or hypothesis.
\end{itemize}

\hypertarget{data-collection}{%
\subsubsection{1.4 Data Collection}\label{data-collection}}

Krebs: ``Not everything that can be measured should be measured.''

Need to define variables.

\begin{itemize}
\tightlist
\item
  Anything that can be measured and exhibits variation among entities,
  space, and or time.
\end{itemize}

\hypertarget{the-research-process-1}{%
\subsubsection{1.5 The Research Process}\label{the-research-process-1}}

\hypertarget{levels-of-measurement}{%
\subsubsection{1.6 Levels of Measurement}\label{levels-of-measurement}}

\hypertarget{categorical-entities-are-divided-into-distinct-categories}{%
\paragraph{Categorical (entities are divided into distinct
categories):}\label{categorical-entities-are-divided-into-distinct-categories}}

Binary variable: There are only two categories.

\begin{itemize}
\tightlist
\item
  e.g.~dead or alive.
\end{itemize}

Nominal variable: There are more than two categories.

\begin{itemize}
\tightlist
\item
  e.g.~whether someone is an omnivore, vegetarian, vegan, or fruitarian.
\end{itemize}

Ordinal variable: The same as a nominal variable but the categories have
a logical order.

\begin{itemize}
\tightlist
\item
  e.g.~whether people got a fail, a pass, a merit or a distinction in
  their exam.
\end{itemize}

\hypertarget{continuous-entities-get-a-distinct-score}{%
\paragraph{Continuous (entities get a distinct
score):}\label{continuous-entities-get-a-distinct-score}}

Interval variable: Equal intervals on the variable represent equal
differences in the property being measured.

\begin{itemize}
\tightlist
\item
  e.g.~the difference between 6 and 8 is equivalent to the difference
  between 13 and 15.
\end{itemize}

Ratio variable: The same as an interval variable, but the ratios of
scores on the scale must also make sense.

\begin{itemize}
\tightlist
\item
  e.g.~a score of 16 on an anxiety scale means that the person is, in
  reality, twice as anxious as someone scoring 8.
\end{itemize}

\hypertarget{data-collection-2-how-to-measure}{%
\subsubsection{1.7 Data Collection 2: How to
Measure}\label{data-collection-2-how-to-measure}}

\hypertarget{correlational-research-measurative}{%
\paragraph{Correlational research
(measurative):}\label{correlational-research-measurative}}

Observing what naturally goes on in the world without directly
interfering with it.

\hypertarget{experimental-research-manipulative}{%
\paragraph{Experimental research
(manipulative):}\label{experimental-research-manipulative}}

One or more variable is systematically manipulated to see their effect
(alone or in combination) on an outcome variable.

Statements can be made about cause and effect.

\hypertarget{experimental-research-methods}{%
\subsubsection{1.8 Experimental Research
Methods}\label{experimental-research-methods}}

Cause and Effect (Hume, 1748)

1.Cause and effect must occur close together in time (contiguity).

2.The cause must occur before an effect does.

3.The effect should never occur without the presence of the cause.

\hypertarget{frequency-distributions}{%
\section{2 Frequency Distributions}\label{frequency-distributions}}

\hypertarget{frequency-distributions-and-variation}{%
\subsubsection{2.1 Frequency Distributions and
Variation}\label{frequency-distributions-and-variation}}

-Frequency and cumulative frequency

-Theoretical frequency distributions

\begin{itemize}
\tightlist
\item
  Normal Distribution
\end{itemize}

-Measures of central tendency

-Variation and associated measures

\hypertarget{frequency-distributions-1}{%
\subsubsection{2.2 Frequency
Distributions}\label{frequency-distributions-1}}

Univariate.

A tally of how frequently occurring a value is among a set of measured
objects.

What does a qualitative evaluation of the frequency distribution allow?

Data reduction technique with a tradeoff.

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-1-1.pdf}

\emph{Figure(s) 2.2.1 and 2.2.2}

Determination of the Amount of Phosphorous in Leaves: A Frequency Table
of Continuous Data

\begin{longtable}[]{@{}ll@{}}
\toprule
Phosphorous concentration & Frequency\tabularnewline
\midrule
\endhead
8.1 to 8.2 & 2\tabularnewline
8.2 to 8.3 & 6\tabularnewline
8.3 to 8.4 & 8\tabularnewline
8.4 to 8.5 & 11\tabularnewline
8.5 to 8.6 & 17\tabularnewline
8.6 to 8.7 & 17\tabularnewline
8.7 to 8.8 & 24\tabularnewline
8.8 to 8.9 & 18\tabularnewline
8.9 to 9.0 & 13\tabularnewline
9.0 to 9.1 & 10\tabularnewline
9.1 to 9.2 & 4\tabularnewline
\bottomrule
\end{longtable}

Total frequency = 130 = n

\emph{Table 2.2.1: ~A Frequency Table of Continuous Data}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-2-1.pdf}

\emph{Figure 2.2.3}

\hypertarget{cumulative-frequency-distributions}{%
\subsubsection{2.3 Cumulative Frequency
Distributions}\label{cumulative-frequency-distributions}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-3-1.pdf}

\emph{Figure 2.3.1}

Determination of the Amount of Phosphorous in Leaves: A Frequency Table
of Continuous Data

\begin{longtable}[]{@{}llll@{}}
\toprule
\begin{minipage}[b]{0.22\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
Cummulative Frequency\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.22\columnwidth}\raggedright
Phosphorous concentration\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
Frequency\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
Starting with Low Values\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
Starting with High Values\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.15 to 8.25\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
2\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
130\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.25 to 8.35\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
6\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
8\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
128\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.35 to 8.45\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
8\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
16\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
122\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.45 to 8.55\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
11\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
27\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
114\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.55 to 8.65\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
17\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
44\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
130\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.65 to 8.75\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
17\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
61\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
86\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.75 to 8.85\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
24\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
85\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
69\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.85 to 8.95\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
18\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
103\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
45\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
8.95 to 9.05\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
13\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
116\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
27\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
9.05 to 9.15\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
10\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
126\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
14\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.22\columnwidth}\raggedright
9.15 to 9.25\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
4\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
130\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
4\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Total frequency = 130 = n

\emph{Table 2.3.1: ~A Frequency Table of Continuous Data}

\hypertarget{what-interval-to-choose}{%
\subsubsection{2.4 What Interval to
Choose?}\label{what-interval-to-choose}}

Domain knowledge

\hypertarget{frequency-distributions-2}{%
\subsubsection{2.5 Frequency
Distributions}\label{frequency-distributions-2}}

\hypertarget{empirical-vs.-theoretical}{%
\paragraph{Empirical vs.~Theoretical}\label{empirical-vs.-theoretical}}

Empirical: observed finite

Theoretical: infinite number of observations

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-4-1.pdf}

\emph{Figure 2.5.1}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-5-1.pdf}

\emph{Figure 2.5.2}

\hypertarget{focus-on-theoretical-distribution}{%
\paragraph{Focus on Theoretical
Distribution:}\label{focus-on-theoretical-distribution}}

Move from determining the frequency of a value (empirical) to the
relative frequency of an interval of values.

\begin{itemize}
\tightlist
\item
  Relabel y-axis, why?
\item
  Every value of x has a non-zero probability, cannot know how often a
  single value occurs.
\item
  Sum of the probability of the obsevations is 100\%.
\end{itemize}

So, we can only determine the relative frequency of an interval of
values.

Use calculus to get the integral.

There are many types of theoretical distributions - we will focus on the
primary ones used in frequentist statistics.

\hypertarget{everything-you-ever-wanted-to-know-about-statistics-aims-and-objectives}{%
\subsubsection{2.6 Everything You Ever Wanted to Know about Statistics
Aims and
Objectives}\label{everything-you-ever-wanted-to-know-about-statistics-aims-and-objectives}}

Know what a statistical model is and why we use them.

\begin{itemize}
\tightlist
\item
  The mean
\end{itemize}

Know what the `fit' of a model is and why it is important.

\begin{itemize}
\tightlist
\item
  The standard deviation
\end{itemize}

Distinguish models for samples and populations.

\hypertarget{pie-charts}{%
\subsubsection{2.7 Pie Charts}\label{pie-charts}}

Don't ever, under any circumstance, use them.

\hypertarget{a-simple-statistical-model}{%
\subsubsection{2.8 A Simple Statistical
Model}\label{a-simple-statistical-model}}

In statistics we fit models to our data (i.e.~we use a statistical model
to represent what is happening in the real world).

The mean is a hypothetical value (i.e.~it doesn't have to be a value
that actually exists in the data set).

The mean is simple statistical model.

\hypertarget{measures-of-central-tendency}{%
\subsubsection{2.9 Measures of Central
Tendency}\label{measures-of-central-tendency}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-6-1.pdf}

\emph{Figure 2.9.1}

\hypertarget{the-mean}{%
\subsubsection{2.10 The Mean}\label{the-mean}}

The mean is the sum of all scores divided by the number of scores.

The mean is also the value from which the (squared) scores deviate least
(it has the least error).

\hypertarget{the-mean-example}{%
\subsubsection{2.11 The Mean: Example}\label{the-mean-example}}

Collect some data - teaching scores from students for a lecturer:

\begin{itemize}
\tightlist
\item
  1, 3, 4, 3, 2
\end{itemize}

Add them up

Divide by the number of scores, n:

\hypertarget{the-mean-as-a-model}{%
\subsubsection{2.12 The Mean as a Model}\label{the-mean-as-a-model}}

\(outcome_i = (model) + error_i\)

\textbackslash{}begin\{aligned\} outcome\_\{lecturer1\}\&=(\bar\{X\}) +
error\_\{lecturer1\}\textbackslash{}

\&=2.6+error\_\{lecturer1\} \textbackslash{}end\{aligned\}

\hypertarget{variation-measuring-the-fit-of-the-model}{%
\subsubsection{2.13 Variation: Measuring the `Fit' of the
Model}\label{variation-measuring-the-fit-of-the-model}}

The mean is a model of what happens in the real world: the typical
score.

It is not a perfect representation of the data.

How can we assess how well the mean represents reality?

\hypertarget{the-dispersion-range}{%
\subsubsection{2.14 The Dispersion: Range}\label{the-dispersion-range}}

The Range

\begin{itemize}
\tightlist
\item
  The smallest score subtracted from the largest
\end{itemize}

Example

\begin{itemize}
\tightlist
\item
  Number of friends of 11 Facebook users.
\item
  22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252
\item
  Range = 252 - 22 = 230
\item
  Very biased by outliers, why?
\end{itemize}

\hypertarget{the-dispersion-the-interquartile-range}{%
\subsubsection{2.15 The Dispersion: The Interquartile
Range}\label{the-dispersion-the-interquartile-range}}

Quartiles

\begin{itemize}
\tightlist
\item
  The three values that split the sorted data into four equal parts.
\item
  Second quartile = median.
\item
  Lower quartile = median of lower half of the data.
\item
  Upper quartile = median of upper half of the data.
\end{itemize}

\hypertarget{a-perfect-fit}{%
\subsubsection{2.16 A Perfect Fit}\label{a-perfect-fit}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-7-1.pdf}

\emph{Figure 2.16.1}

\hypertarget{calculating-error}{%
\subsubsection{2.17 Calculating `Error'}\label{calculating-error}}

A deviation is the difference between the mean and an actual data point.

Deviations can be calculated by taking each score and subtracting the
mean from it:

\(deviation = x_i - \bar{x}\)

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-8-1.pdf}

\emph{Figure 2.17.1}

\hypertarget{use-the-total-error}{%
\subsubsection{2.18 Use the Total Error?}\label{use-the-total-error}}

We could just take the error between the mean and the data and add them.

\begin{longtable}[]{@{}lll@{}}
\toprule
Score & Mean & Deviation\tabularnewline
\midrule
\endhead
1 & 2.6 & -1.6\tabularnewline
2 & 2.6 & -0.6\tabularnewline
3 & 2.6 & 0.4\tabularnewline
3 & 2.6 & 0.4\tabularnewline
4 & 2.6 & 1.4\tabularnewline
& Total = & 0\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 2.18.1: ~Total error}

\(\Sigma(X - \bar{X}) = 0\)

\hypertarget{mean-absolute-deviation-mad}{%
\subsubsection{2.19 Mean Absolute Deviation
(MAD)}\label{mean-absolute-deviation-mad}}

Also, called MAE (mean absolute error).

\(MAE = \frac{sum|x_i - \bar{x}|}{n}\)

\hypertarget{sum-of-squared-errors}{%
\subsubsection{2.20 Sum of Squared Errors}\label{sum-of-squared-errors}}

We could add the deviations to find out the total error.

Deviations cancel out because some are positive and others negative.

Therefore, we square each deviation.

If we add these squared deviations we get the sum of squared errors
(SS).

\begin{longtable}[]{@{}llll@{}}
\toprule
Score & Mean & Deviation & Sqaured Deviation\tabularnewline
\midrule
\endhead
1 & 2.6 & -1.6 & 2.56\tabularnewline
2 & 2.6 & -0.6 & 0.36\tabularnewline
3 & 2.6 & 0.4 & 0.16\tabularnewline
3 & 2.6 & 0.4 & 0.16\tabularnewline
4 & 2.6 & 1.4 & 1.96\tabularnewline
& & Total & 5.20\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 2.20.1: ~Sqaured Deviation}

\(SS = \Sigma(X - \bar{X})^2 = 5.20\)

\hypertarget{variance}{%
\subsubsection{2.21 Variance}\label{variance}}

The sum of squares is a good measure of overall variability, but is
dependent on the number of scores.

We calculate the average variability by dividing by the number of scores
(n).

This value is called the variance (s\textsuperscript{2}).

\(varience (s^2) = \frac{SS}{N-1} = \frac{\Sigma(x_i-\bar{x})^2}{N-1} = \frac{5.20}{4} = 1.3\)

\hypertarget{standard-deviation}{%
\subsubsection{2.22 Standard Deviation}\label{standard-deviation}}

The variance has one problem: it is measured in units squared.

This isn't a very meaningful metric so we take the square root value.

This is the standard deviation(s).

\(s = \sqrt\frac{\Sigma^n_{i=1}(x_i-\bar{x})^2}{n} = \sqrt\frac{5.20}{5} = 1.02\)

\hypertarget{things-to-remember}{%
\subsubsection{2.23 Things to Remember}\label{things-to-remember}}

The sum of squares, variance, and standard deviation represent the same
thing:

\begin{itemize}
\tightlist
\item
  The fit of the mean to the data
\item
  The variability in the data
\item
  How well the mean represents the observed data
\item
  Error
\end{itemize}

\hypertarget{same-mean-different-standard-deviation}{%
\subsubsection{2.24 Same Mean, Different Standard
Deviation}\label{same-mean-different-standard-deviation}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-9-1.pdf}

\emph{Figures 2.24.1 and 2.24.2}

\hypertarget{the-standard-deviation-and-the-shape-of-a-distribution}{%
\subsubsection{2.25 The Standard Deviation and the Shape of a
Distribution}\label{the-standard-deviation-and-the-shape-of-a-distribution}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-10-1.pdf}

\emph{Figures 2.25.1 and 2.25.2}

\hypertarget{areas-under-the-normal-curve-for-various-standard-deviation}{%
\subsubsection{2.26 Areas Under the Normal Curve for Various Standard
Deviation}\label{areas-under-the-normal-curve-for-various-standard-deviation}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-11-1.pdf}

\emph{Figure 2.26.1}

\hypertarget{going-beyond-the-data-z-scores}{%
\subsubsection{2.27 Going Beyond the Data:
z-scores}\label{going-beyond-the-data-z-scores}}

z-scores

\begin{itemize}
\tightlist
\item
  Standardising a score with respect to the other scores in the group.
\item
  Expresses a score in terms of how many standard deviations it is away
  from the mean.
\item
  The distribution of z-scores has a mean of 0 and SD = 1.
\end{itemize}

\(z = \frac{X-\bar{X}}{S}\)

\hypertarget{properties-of-z-scores}{%
\subsubsection{2.28 Properties of
z-scores}\label{properties-of-z-scores}}

1.96 cuts off the top 2.5\% of the distribution.

-1.96 cuts off the bottom 2.5\% of the distribution.

As such, 95\% of z-scores lie between -1.96 and 1.96.

99\% of z-scores lie between -2.58 and 2.58.

99.9\% of them lie between -3.29 and 3.29.

\hypertarget{probability-sampling-and-parameters}{%
\section{3 Probability, Sampling, and
Parameters}\label{probability-sampling-and-parameters}}

\hypertarget{probability-sampling-and-parameters-1}{%
\subsubsection{3.1 Probability, Sampling, and
Parameters}\label{probability-sampling-and-parameters-1}}

\begin{itemize}
\tightlist
\item
  Probability
\item
  Random variables
\item
  Sampling from a normal distribution
\item
  Sampling from a bionomial distribution
\item
  Parameter Estimation
\item
  Student's t-distribution
\end{itemize}

\hypertarget{probability}{%
\subsubsection{3.2 Probability}\label{probability}}

SETS: A collection of items.

Element: on item of a set.

Subsets have multiple elements and are themselves sets.

Outcome set.

\begin{itemize}
\tightlist
\item
  In an experiment (or other phenomenon that yields results to observe),
  there is a set (usually very large) of possible outcomes. Let us refer
  to this as the outcome set.
\end{itemize}

Intersect - the common elements in two sets.

Mutually exclusive - Sets with no elements in common , null intersect.

Union - the combination of elements in two sets - what element is in
either set or both?

Complement - the remainder of outcomes in a set that are not in a subset

\hypertarget{venn-diagram}{%
\paragraph{Venn Diagram}\label{venn-diagram}}

\hypertarget{sampling-distributions}{%
\subsubsection{3.3 Sampling
Distributions}\label{sampling-distributions}}

Random samples:

\begin{itemize}
\tightlist
\item
  Every possible member of the population has an equal probability of
  being included in the sample.
\item
  Ex: scientific exit polling vs.~twitter polls
\item
  Think of a normally distributed frequency distribution
\item
  Only statistically valid data that can be used for analysis - the
  first assumption of parametric statistics.
\end{itemize}

\hypertarget{the-normal-distribution}{%
\subsubsection{3.4 The Normal
Distribution}\label{the-normal-distribution}}

\(Y_i = \frac{1}{\sigma\sqrt2\pi}e^-(X_i-\mu)^2/2\sigma^2\)

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-12-1.pdf}

\emph{Figure 3.4.1}

\(Y_i = \frac{1}{\sigma\sqrt2\pi}e^-(X_i-\mu)^2/2\sigma^2\)

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-13-1.pdf}

\emph{Figures 3.4.2 and 3.4.3}

\hypertarget{sample-from-a-normal-distribution}{%
\subsubsection{3.5 Sample from a Normal
Distribution}\label{sample-from-a-normal-distribution}}

Normal distribution sampling theorem:

\begin{itemize}
\tightlist
\item
  Sampling distribution is normal when the population distribution is
  normal.
\item
  Sample mean = population mean
\item
  Sample sd = population s
\end{itemize}

\hypertarget{sampling-from-a-non-normal-distribution}{%
\subsubsection{3.6 Sampling from a Non-Normal
Distribution}\label{sampling-from-a-non-normal-distribution}}

Central limit theorem:

\begin{itemize}
\tightlist
\item
  Drawing samples from a non-normal population will result in a sampling
  distribution that will approximate a normal distribution when the
  sample size is large.
\item
  How can we test this - what approach would you use?
\end{itemize}

\hypertarget{we-will-explore-the-sampling-of-distributions-in-the-in-class-assignment.}{%
\paragraph{We will explore the sampling of distributions in the in-class
assignment.}\label{we-will-explore-the-sampling-of-distributions-in-the-in-class-assignment.}}

\hypertarget{parameter-estimation}{%
\subsubsection{3.7 Parameter Estimation}\label{parameter-estimation}}

\hypertarget{point-estimation}{%
\paragraph{Point estimation}\label{point-estimation}}

Single value based on sampling

Often mean and sd

Goal:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Achieve and unbiased estimate - Long-run (infinite sampling)
\item
  Derive an efficient estimator - Fewest number samples to obtain
  accurate value
\end{enumerate}

\hypertarget{interval-estimation}{%
\subsubsection{3.8 Interval Estimation}\label{interval-estimation}}

How close is the true parameter to the estimate?

Confidence intervals and limits.

\hypertarget{hypothesis-testing-and-power}{%
\section{4 Hypothesis Testing and
Power}\label{hypothesis-testing-and-power}}

Hypothesis Testing

Power

Assumptions of parametric statistics

\hypertarget{statistical-hypothesis-testing}{%
\subsubsection{4.1 Statistical Hypothesis
Testing}\label{statistical-hypothesis-testing}}

State:

\begin{itemize}
\tightlist
\item
  H\textsubscript{O}
\item
  H\textsubscript{A}
\end{itemize}

Declare

\begin{itemize}
\tightlist
\item
  Alpha level
\end{itemize}

Collect Data

Compare the test statistic to the critical value (determined by alpha)

State the resulting probability

State testable hypothesis

\begin{itemize}
\tightlist
\item
  These are a set of mutually exclusive and exhaustive outcomes.
\item
  Test statistic will support one or the other.
\item
  H\textsubscript{O} \textsubscript{Null}\\
\item
  H\textsubscript{A} \textsubscript{Alternative}
\end{itemize}

\(H_0: \mu = 0, H_A:\mu \ne 0\)

\(H_0: \mu = 3.5 cm, H_A:\mu \ne 3.5 cm\)

\(H_0: \mu = 10.5 kg, H_A:\mu \ne 10.5 kg\)

\hypertarget{example-use-z-score-to-test-mean}{%
\subsubsection{4.2 Example: Use z-score to Test
Mean}\label{example-use-z-score-to-test-mean}}

Is the mean fuel consumption of a population of busses equal to 20 mpg?

What is the null hypothesis?

We need information about the population

\begin{itemize}
\tightlist
\item
  Mean
\item
  Population standard deviation
\item
  Calculate z-score
\item
  What is the probability that the mean is 20 mpg given: + Sigma = 0.3,
  Mean = 19.1
\end{itemize}

\hypertarget{evaluate-z-score}{%
\subsubsection{4.3 Evaluate z-score}\label{evaluate-z-score}}

What is the probability that we would get this z score?

Hypothetical z-scores

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-14-1.pdf}

\emph{Figure 4.3.1}

\begin{longtable}[]{@{}lllllllllll@{}}
\toprule
z & .00 & .01 & .02 & .03 & .04 & .05 & .06 & .07 & .08 &
.09\tabularnewline
\midrule
\endhead
0.0 & .5000 & .5040 & .5080 & .5120 & .5160 & .5199 & .5239 & .5279 &
.5319 & .5359\tabularnewline
0.1 & .5398 & .5438 & .5478 & .5517 & .5557 & .5596 & .5636 & .5675 &
.5714 & .5753\tabularnewline
0.2 & .5793 & .5832 & .5871 & .5910 & .5948 & .5987 & .6026 & .6064 &
.6103 & .6141\tabularnewline
0.3 & .6179 & .6217 & .6255 & .6293 & .6331 & .6368 & .6406 & .6443 &
.6480 & .6517\tabularnewline
0.4 & .6554 & .6591 & .6628 & .6664 & .6700 & .6736 & .6772 & .6808 &
.6844 & .6879\tabularnewline
0.5 & .6915 & .6950 & .6985 & .7019 & .7054 & .7088 & .7123 & .7157 &
.7190 & .7224\tabularnewline
0.6 & .7257 & .7291 & .7324 & .7357 & .7389 & .7422 & .7454 & .7486 &
.7517 & .7549\tabularnewline
0.7 & .7580 & .7611 & .7642 & .7673 & .7704 & .7734 & .7764 & .7794 &
.7823 & .7852\tabularnewline
0.8 & .7881 & .7910 & .7939 & .7967 & .7995 & .8023 & .8051 & .8078 &
.8106 & .8133\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 4.3.1: ~Standard Normal Posibilities}

\hypertarget{is-it-meaningful-significance-level}{%
\subsubsection{4.4 Is it meaningful? Significance
Level}\label{is-it-meaningful-significance-level}}

Declare

\begin{itemize}
\tightlist
\item
  Alpha level
\item
  p vs.~alpha
\item
  Define prior to test
\item
  Two tail and one tail test
\end{itemize}

\hypertarget{alpha}{%
\subsubsection{4.5 Alpha}\label{alpha}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-15-1.pdf}

\emph{Figure 4.5.1}

\hypertarget{statistical-hypothesis-testing-1}{%
\subsubsection{4.6 Statistical Hypothesis
Testing}\label{statistical-hypothesis-testing-1}}

Ex: Look to see if the population mean is not different from some
specified value.

H\textsubscript{O}: u = 0

H\textsubscript{A}: u is not equal 0

Introduce the idea of a critical value

\begin{itemize}
\tightlist
\item
  Alpha level of 0.05
\end{itemize}

We have data taken from the weight change in horses given some medical
treatment.

We are interested to know if the mean change in weight that we found
+1.29 kg is significantly different from 0 kg.

\begin{itemize}
\tightlist
\item
  We calculate the z-score and find that Z = 1.45
\end{itemize}

\(P(mean \ge 1.29) = P(Z \ge 1.45) = ?\)

\(P(mean \le 1.29) = P(Z \le 1.45) = ?\)

Z = 1.96 is the rejection region at 2.5\%

\begin{itemize}
\tightlist
\item
  This is the `region of rejection'
\end{itemize}

Now we have a way to objectively reject or accept the null hypothesis.

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-16-1.pdf}

\emph{Figure 4.6.1}

\hypertarget{one--and-two-tailed-tests}{%
\subsubsection{4.7 One- and Two-Tailed
Tests}\label{one--and-two-tailed-tests}}

Alternative to testing `is the value different.'

In some cases we care about the direction of the difference.

Use one-tailed test

\begin{itemize}
\tightlist
\item
  In general, one-tailed hypotheses about a mean are: +
  \(H_0:\mu\ge\mu_0\) and \(H_A:\mu<\mu_0\)
\item
  In which case, H\textsubscript{0} is rejected if the test statistic is
  in the left-hand tail of the distribution or: + \(H_0:\mu\le\mu_0\)
  and \(H_A:\mu>\mu_0\)
\end{itemize}

Contrast the region of rejection for these.

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-17-1.pdf}

\emph{Figures 4.7.1 and 4.7.2}

\hypertarget{type-1-and-type-2-errors}{%
\subsubsection{4.8 Type 1 and Type 2
Errors}\label{type-1-and-type-2-errors}}

Sometimes we:

\begin{itemize}
\tightlist
\item
  Reject the null hypothesis when it is true.
\item
  Accept the alternative hypothesis when it is false.
\end{itemize}

Type 1 error or alpha error - frequency of rejecting H\textsubscript{0}
when it is true.

Type 1 error rate is equal to alpha.

Type 1 error: ``rejecting the null hypothesis when it is true.''

Type 1 error or `\(\alpha\) error' is equal to \(\alpha\)

Now we have some criteria to choose alpha.

So if your \(\alpha\), or critical value is 0.10

\begin{itemize}
\tightlist
\item
  We have a 10\% probability of rejecting the null hypothesis when we
  should have, in fact, accepted it.
\end{itemize}

\hypertarget{type-1-alpha-error}{%
\paragraph{Type 1 (alpha) Error}\label{type-1-alpha-error}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-18-1.pdf}

\emph{Figure 4.8.1}

Type 2 error: ``accepting the null hypothesis when it is false.''

Type 2 error or `\(\beta\) error' is equal to \(\beta\).

\begin{longtable}[]{@{}lll@{}}
\toprule
& If H\textsubscript{0} is true & If H\textsubscript{0} is
false\tabularnewline
\midrule
\endhead
If H\textsubscript{0} is rejected & Type I error & No
error\tabularnewline
If H\textsubscript{0} is not rejected & No error & Type II
error\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 4.8.1: ~Two Types of Errors in Hypothesis Testing}

Thought experiments:

\begin{itemize}
\tightlist
\item
  Ex. Endangered species conservation
\item
  Ex. Pharmaceutical testing
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\toprule
& If H\textsubscript{0} is true & If H\textsubscript{0} is
false\tabularnewline
\midrule
\endhead
If H\textsubscript{0} is rejected & \(\alpha\) & \(1-\beta\) (``power'')
No error\tabularnewline
If H\textsubscript{0} is not rejected & No error \(1-\alpha\) &
\(\beta\)\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 4.8.2: ~Long-term Probabilities of Outcomes in Hypothesis
Testing}

\hypertarget{power}{%
\subsubsection{4.9 Power}\label{power}}

Power: the probability that a statistical test will reject a null
hypothesis when it is false (proper rejection).

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-19-1.pdf}

\emph{Figure 4.9.1}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-20-1.pdf}

\emph{Figure 4.9.2}

\hypertarget{leafs-power-simulation-in-r}{%
\paragraph{Leaf's power simulation in
R}\label{leafs-power-simulation-in-r}}

\hypertarget{what-influences-statistical-power}{%
\subsubsection{4.10 What Influences Statistical
Power?}\label{what-influences-statistical-power}}

Picture 2 (Copied and pasted from book?)

\hypertarget{assumptions}{%
\subsubsection{4.11 Assumptions}\label{assumptions}}

Assumptions - When broken then we are not able to make inference or
accurate descriptions about reality.

Thus our models are flawed descriptions and inferences will be
compromised.

\begin{itemize}
\tightlist
\item
  Assumptions of parametric tests based on the normal distribution.
\item
  Understand the assumption of normality.
\item
  Understand homogeneity of variance.
\item
  Know how to correct problems (with respect to the assumptions of
  normality) in the data.
\end{itemize}

Parametric tests based on the normal distribution assume:

\begin{itemize}
\item
  Normally distributed + Distribution of samples + Model distribution
  (residuals)
\item
  Homogeneity of variance
\item
  Interval or ratio level data + Some data are intrinsically not
  normally distributed.
\item
  Independence of observation
\end{itemize}

\hypertarget{the-normal-distribution-review}{%
\subsubsection{4.12 The Normal Distribution
Review}\label{the-normal-distribution-review}}

Commonly the distribution of measurements (frequency of data collected
from interval data) have a bell shaped distribution

Parameters of the model determine its shape.

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-21-1.pdf}

\emph{Figure 4.12.1}

Two-parameter distribution

\(f(x,\mu,\sigma)=\frac{1}{\sigma\sqrt2\pi}e^-(x-\mu)^2/2\sigma^2\)

Symmetric around mu

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-22-1.pdf}

\emph{Figure 4.12.2}

\hypertarget{distribution-of-samples-central-limit-theorem}{%
\subsubsection{4.13 Distribution of Samples Central Limit
Theorem}\label{distribution-of-samples-central-limit-theorem}}

How well do samples represent the population?

A key foundation of frequentist statistics - samples are random
variables: When we take a sample from the population we are taking one
of many possible samples.

Thought experiment - take many, many samples from a population.

Do we expect all sample to have the same mean value (the same sample
mean)?

\begin{itemize}
\tightlist
\item
  No, there is variation in the samples - `Sampling variation.'
\end{itemize}

The frequency histogram of samples is the sampling distribution.

Analog to standard deviation

\begin{itemize}
\tightlist
\item
  SD how well does model fit the data
\end{itemize}

We can take the standard deviation of the sample mean.

\begin{itemize}
\tightlist
\item
  Termed `Standard Error of the Sampling mean'
\item
  Or `Standard Error'
\end{itemize}

If the sample is large then sampling error can be approximated:

\(SE=\frac{s}{\sqrt n}\)

\hypertarget{assumption-1-sample-observations-and-associated-deviations-are-normally-distributed.}{%
\subsubsection{4.14 Assumption \#1: Sample observations and associated
deviations are normally
distributed.}\label{assumption-1-sample-observations-and-associated-deviations-are-normally-distributed.}}

We will review how to check these assumptions in this lecture and the
following lab.

\hypertarget{assumption-2-homogeneity-of-variance.}{%
\subsubsection{4.15 Assumption \#2: Homogeneity of
Variance.}\label{assumption-2-homogeneity-of-variance.}}

Data taken from groups must have homogenous variance.

Homogenous does not mean `equal' but equal in the probabilistic sense.

We will review how to check these assumptions in this lecture and the
following lab.

\hypertarget{assumption-3-interval-and-ratio-scale}{%
\subsubsection{4.16 Assumption \#3: Interval and Ratio
Scale}\label{assumption-3-interval-and-ratio-scale}}

Continuous variables

\begin{itemize}
\tightlist
\item
  Interval scale (equal intervals between measurements).
\item
  Ratio scale - Conversion of interval data such that ratio of
  measurements was meaningful.
\end{itemize}

Ordinal data - rankings -

\begin{itemize}
\tightlist
\item
  Darker, faster, shorter and might label these 1,2,3,4,5 to reflect
  increases in magnitude.
\item
  Really convey less information - data condensation.
\end{itemize}

Nominal or Categorical data

\begin{itemize}
\tightlist
\item
  Example are public surveys
\item
  Willingness to vote for a candidate? Economic class, Taxonomic
  categories.
\end{itemize}

\hypertarget{assumption-4}{%
\subsubsection{4.17 Assumption \#4}\label{assumption-4}}

Observations are independent

The measurement of one sample does not influence the measurement of
another sample.

\begin{itemize}
\tightlist
\item
  Measurements taken in space and time are examples - experimenter needs
  to determine when there is zero correlation between the samples.
\item
  Behavioral Example - the opinion of one person influences the behavior
  of another person and hence the measurements are correlated.
\end{itemize}

\hypertarget{assessing-normality}{%
\subsubsection{4.18 Assessing Normality}\label{assessing-normality}}

We don't have access to the population distribution so we usually test
the observed data

Graphical displays

\begin{itemize}
\tightlist
\item
  Q-Q plot (or P-P plot)
\item
  Histogram
\end{itemize}

Kolmogorov-Smirnov

Shapiro-Wilk

\hypertarget{assessing-homogeneity-of-variance}{%
\subsubsection{4.19 Assessing Homogeneity of
Variance}\label{assessing-homogeneity-of-variance}}

Figures

Levene's test

\begin{itemize}
\tightlist
\item
  Tests if variances in different groups are the same.
\item
  Significant = variances not `equal'
\item
  Non-significant = variances are `equal'
\end{itemize}

Variance ratio

\begin{itemize}
\tightlist
\item
  With 2 or more groups
\item
  VR = largest variance/smallest variance
\item
  If VR \textless{} 2, homogeneity can be assumed
\end{itemize}

\hypertarget{correcting-data-problems}{%
\subsubsection{4.20 Correcting Data
`Problems'}\label{correcting-data-problems}}

Log transformation log(X\textsubscript{i}) or log(X\textsubscript{i} +1)

\begin{itemize}
\tightlist
\item
  Reduce positive skew.
\end{itemize}

Square root transformation:

\begin{itemize}
\tightlist
\item
  Also reduces positive skew. Can also be useful for stabilizing
  variance.
\end{itemize}

Reciprocal transformation (1/ X\textsubscript{i}):

\begin{itemize}
\tightlist
\item
  Dividing 1 by each score also reduces the impact of large scores.
\item
  This transformation reverses the scores
\item
  You can avoid this by reversing the scores before the transformation,
  1/(X\textsubscript{Highest} - X\textsubscript{i}).
\end{itemize}

\hypertarget{to-transform-or-not}{%
\subsubsection{4.21 To Transform Or Not}\label{to-transform-or-not}}

Transforming the data helps as often as it hinders the accuracy of F

The central limit theorem: sampling distribution will be normal in
samples \textgreater{} 40 anyway.

\begin{itemize}
\item
  Transforming the data changes the hypothesis being tested. + E.g. when
  using a log transformation and comparing means, you change from
  comparing arithmetic means to comparing geometric means.
\item
  In small samples it is tricky to determine normality one way or
  another. The consequences for the statistical model of applying the
  `wrong' transformation could be worse than the consequences of
  analysing the untransformed scores.
\item
  Alternative - use non-parametric statistics or Bayesian approaches.
\end{itemize}

\hypertarget{parameteric-and-non-parameteric-correlation}{%
\section{5 Parameteric and Non-Parameteric
Correlation}\label{parameteric-and-non-parameteric-correlation}}

\hypertarget{correlation}{%
\subsubsection{5.1 Correlation}\label{correlation}}

Linear dependence of variables

\begin{itemize}
\tightlist
\item
  Scatterplots
\item
  Covariance
\item
  Pearson's correlation coefficient
\end{itemize}

Nonparametric measures

\begin{itemize}
\tightlist
\item
  Spearman's rho
\item
  Kendall's tau
\end{itemize}

Interpreting correlations

\begin{itemize}
\tightlist
\item
  Causality
\end{itemize}

Partial correlations

\hypertarget{what-is-a-correlation}{%
\subsubsection{5.2 What is a Correlation?}\label{what-is-a-correlation}}

It is a way of measuring the extent to which two variables are related.

It measures the pattern of responses across variables.

\hypertarget{small-relationship}{%
\paragraph{Small Relationship}\label{small-relationship}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-23-1.pdf}

\emph{Figure 5.2.1}

\hypertarget{positive-relationship}{%
\paragraph{Positive Relationship}\label{positive-relationship}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-24-1.pdf}

\emph{Figure 5.2.2}

\hypertarget{negative-relationship}{%
\paragraph{Negative Relationship}\label{negative-relationship}}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-25-1.pdf}

\emph{Figure 5.2.3}

\hypertarget{measuring-relationships}{%
\subsubsection{5.3 Measuring
Relationships}\label{measuring-relationships}}

We need to see whether as one variable increases, the other increases,
decreases or stays the same.

This can be done by calculating the covariance.

\begin{itemize}
\tightlist
\item
  We look at how much each score deviates from the mean.
\item
  If both variables deviate from the mean by the same amount, they are
  likely to be related.
\end{itemize}

\begin{longtable}[]{@{}llllllll@{}}
\toprule
Participant & 1 & 2 & 3 & 4 & 5 & Mean & SD\tabularnewline
\midrule
\endhead
Adverts Watched & 5 & 4 & 4 & 6 & 8 & 5.4 & 1.67\tabularnewline
Packets Bought & 8 & 9 & 10 & 13 & 15 & 11 & 2.92\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 5.3.1: ~Measuring Relationships}

\begin{longtable}[]{@{}llllllll@{}}
\toprule
Participant & 1 & 2 & 3 & 4 & 5 & Mean & SD\tabularnewline
\midrule
\endhead
Adverts Watched & 5 & 4 & 4 & 6 & 8 & 5.4 & 1.67\tabularnewline
Packets Bought & 8 & 9 & 10 & 13 & 15 & 11 & 2.92\tabularnewline
Advertiser Residual & -0.04 & -1.4 & -1.4 & 0.6 & 2.6 & &\tabularnewline
Packets residual & -3 & -2 & -1 & 2 & 4 & &\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 5.3.2: ~Measuring Relationships}

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-26-1.pdf}

\emph{Figure 5.3.1}

\hypertarget{re-examination-of-variance}{%
\subsubsection{5.4 Re-examination of
Variance}\label{re-examination-of-variance}}

The variance tells us by how much scores deviate from the mean for a
single variable.

It is closely linked to the sum of squares.

Covariance is similar - it tells is by how much scores on two variables
differ from their respective means.

\(variance=\frac{\Sigma(x_i - \bar{x})}{N-1}\)

\(variance=\frac{\Sigma(x_i - \bar{x})(x_i - \bar{x})}{N-1}\)

\hypertarget{covariance}{%
\subsubsection{5.5 Covariance}\label{covariance}}

Calculate the error between the mean and each subject's score for the
first variable (x).

Calculate the error between the mean and their score for the second
variable (y).

Multiply these error values.

Add these values and you get the cross product deviations.

The covariance is the average cross-product deviations:

Table 1000 =

\(N_1(a_1,_1) + N_2(a_2,_1)\)

\(cov(x,y)=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{N-1}\)

\(=\frac{(-0.4)(-3)+(-1.4)(-2)+(-1.4)(-1)+(0.6)(2)+(2.6)(4)}{4}\)

\(=\frac{1.2+2.8+1.4+1.2+10.4}{4}\)

\(=\frac{17}{4}\)

\(=4.25\)

\hypertarget{problems-with-covariance}{%
\subsubsection{5.6 Problems with
Covariance}\label{problems-with-covariance}}

Dependent on the units of measurement.

\begin{itemize}
\tightlist
\item
  E.g. the covariance of two variables measured in miles might be 4.25,
  but if the same scores are converted to kilometres, the covariance is
  11.
\end{itemize}

One solution: standardize it

\begin{itemize}
\tightlist
\item
  Divide by the standard deviations of both variables.
\end{itemize}

The standardized version of covariance is known as the correlation
coefficient.

\begin{itemize}
\tightlist
\item
  It is relatively unaffected by units of measurement.
\end{itemize}

\hypertarget{the-correlation-coefficient}{%
\subsubsection{5.7 The Correlation
Coefficient}\label{the-correlation-coefficient}}

\(r=\frac{cov_xy}{s_xs_y}\)

\(=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{(N-1)s_xs_y}\)

\(r=\frac{cov_xy}{s_xs_y}\)

\(=\frac{4.25}{1.67 * 2.92}\)

\(=0.87\)

Termed Pearson-product moment correlation coefficient

It is a testable hypothesis

\includegraphics{Fundamentals_of_Statistics_files/figure-latex/unnamed-chunk-27-1.pdf}

\emph{Figure 5.7.1}

It is a testable hypothesis

Testing \(H_0: \rho=0\) versus \(H_A: \rho\ne0\)

The standard error of the correlation coefficient is calculated as:

\(S_r=\sqrt\frac{1-r^2}{n-2}\)

It is a testable hypothesis

r = 0.870

n = 12 (new data set, with more samples)

The critical value is:

\(t=\frac{r}{S_r}= \frac{0.870}{0.156}= 5.58\)

t\textsubscript{0.05(2),10} =2.228

Testing \(H_0: \rho=0\) versus \(H_A: \rho\ne0\)

It varies between -1 and +1

\begin{itemize}
\tightlist
\item
  0 = no relationship
\end{itemize}

It is an effect size

\begin{itemize}
\tightlist
\item
  ?.1 = small effect
\item
  ?.3 = medium effect
\item
  ?.5 = large effect
\end{itemize}

Coefficient of determination, r2

\begin{itemize}
\tightlist
\item
  By squaring the value of r you get the proportion of variance in one
  variable shared by the other.
\end{itemize}

\hypertarget{correlation-and-causality}{%
\subsubsection{5.8 Correlation and
Causality}\label{correlation-and-causality}}

The third-variable problem:

\begin{itemize}
\tightlist
\item
  In any correlation, causality between two variables cannot be assumed
  because there may be other measured or unmeasured variables affecting
  the results.
\item
  Could be many latent variables..
\end{itemize}

Direction of causality:

\begin{itemize}
\tightlist
\item
  Correlation coefficients say nothing about which variable causes the
  other to change.
\end{itemize}

\hypertarget{non-parametric-correlation}{%
\subsubsection{5.9 Non-parametric
Correlation}\label{non-parametric-correlation}}

Spearman's rho

\begin{itemize}
\tightlist
\item
  Pearson's correlation on the ranked data
\end{itemize}

Kendall's tau

\begin{itemize}
\tightlist
\item
  ``Better'' than Spearman's for small samples
\end{itemize}

\hypertarget{spearman-rank-correlation-coefficient}{%
\subsubsection{5.10 Spearman Rank Correlation
Coefficient}\label{spearman-rank-correlation-coefficient}}

d is the difference between two numbers in each pair of ranks

n = number of pairs of data

\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)

\begin{longtable}[]{@{}ll@{}}
\toprule
Data 1 & Data 2\tabularnewline
\midrule
\endhead
6 & 2\tabularnewline
4 & 9\tabularnewline
7 & 3\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 5.10.1: ~Spearman Rank Correlation Coefficient}

\begin{longtable}[]{@{}llllll@{}}
\toprule
Data 1 & Data 2 & Rank 1 & Rank 2 & d &
d\textsuperscript{2}\tabularnewline
\midrule
\endhead
6 & 2 & 2 & 1 & &\tabularnewline
4 & 9 & 1 & 3 & &\tabularnewline
7 & 3 & 3 & 2 & &\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 5.10.2: ~Spearman Rank Correlation Coefficient}

\begin{longtable}[]{@{}llllll@{}}
\toprule
Data 1 & Data 2 & Rank 1 & Rank 2 & d &
d\textsuperscript{2}\tabularnewline
\midrule
\endhead
6 & 2 & 2 & 1 & 1 & 1\tabularnewline
4 & 9 & 1 & 3 & 2 & 4\tabularnewline
7 & 3 & 3 & 2 & 1 & 1\tabularnewline
\bottomrule
\end{longtable}

\emph{Table 5.10.3: ~Spearman Rank Correlation Coefficient}

\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)

\(=1-(\frac{6*6}{3(3^2 - 1)})\)

We can use this value as the calculated r value

The critical value is a two tailed value with n

\hypertarget{partial-and-semi-partial-correlations}{%
\subsubsection{5.11 Partial and Semi-partial
Correlations}\label{partial-and-semi-partial-correlations}}

Partial correlation:

\begin{itemize}
\tightlist
\item
  Measures the relationship between two variables, controlling for the
  effect that a third variable has on them both.
\end{itemize}

Semi-partial correlation:

\begin{itemize}
\tightlist
\item
  Measures the relationship between two variables controlling for the
  effect that a third variable has on only one of the others.
\end{itemize}

\href{about.html}{LINK to another html file} \href{cv.html}{LINK to
another html file}


\end{document}
