---
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---
# 1 Introduction to statistical analysis
### 1.1 Statistical Analysis 
Kachigan (1986) defines statistical analysis as the:

+ Collection,

+ Organization, and

+ Interpretation of data according to well-defined procedures

In this course we will use Kachigan's definition as a framework and focus on aspects of collection (experimental design), organization (using descriptive and inferential approaches), and interpretation.

One of the primary facets of quantitative analysis is the need to be creative in your approaches - the methods outlined in this class provide the foundation for analysis but the practitioner is encouraged to explore the methods and practices in their discipline to best address the needs for interpretation.  

### 1.2 The Research Process

A.J. Underwood described the "components of design in ecological field experiments" (2009) and this stereotyped approach can be used in the natural sciences and other disciplines.

Scientific investigation is most efficient and leads to strong inference when the 


### 1.3 Generating and Testing Theories

Field et al. (2012)

Theory 

+ A hypothesized general principle or set of principles that explains known findings about a topic and from which new hypotheses can be generated.

Hypothesis

+ A prediction from a theory. 
+ E.g. the number of people turning up for a Big  Brother audition that have narcissistic personality disorder will be higher than the general level (1%) in the population.

Falsification 

+ The act of disproving a theory or hypothesis.

### 1.4 Data Collection 

Krebs: "Not everything that can be measured should be measured."

Need to define variables.

+ Anything that can be measured and exhibits variation among entities, space, and or time.


### 1.5 The Research Process

<Br>

<!--
![Figure 1.5.1](C:/Users/w954394/Pictures/Biometry_FTP/Figures/Fund_Stats_Fig/Figure 1.1.png)
-->

<Br>

### 1.6 Levels of Measurement 

#### Categorical (entities are divided into distinct categories):

Binary variable: There are only two categories.

+ e.g. dead or alive.

Nominal variable: There are more than two categories. 

+ e.g. whether someone is an omnivore, vegetarian, vegan, or fruitarian.

Ordinal variable: The same as a nominal variable but the categories have a logical order.

+ e.g. whether people got a fail, a pass, a merit or a distinction in their exam.

#### Continuous (entities get a distinct score):

Interval variable: Equal intervals on the variable represent equal differences in the property being measured.

+ e.g. the difference between 6 and 8 is equivalent to the difference between 13 and 15.

Ratio variable: The same as an interval variable, but the ratios of scores on the scale must also make sense.

+ e.g. a score of 16 on an anxiety scale means that the person is, in reality, twice as anxious as someone scoring 8.

### 1.7 Data Collection 2: How to Measure

#### Correlational research (measurative):

Observing what naturally goes on in the world without directly interfering with it.

#### Experimental research (manipulative):

One or more variable is systematically manipulated to see their effect (alone or in combination) on an outcome variable.

Statements can be made about cause and effect.

### 1.8 Experimental Research Methods 

Cause and Effect (Hume, 1748)

1.Cause and effect must occur close together in time (contiguity). 

2.The cause must occur before an effect does.

3.The effect should never occur without the presence of the cause.

<Br>

# 2 Frequency Distributions

### 2.1 Frequency Distributions and Variation

-Frequency and cumulative frequency

-Theoretical frequency distributions

+ Normal Distribution

-Measures of central tendency

-Variation and associated measures

### 2.2 Frequency Distributions

Univariate.

A tally of how frequently occurring a value is among a set of measured objects.

What does a qualitative evaluation of the frequency distribution allow?

Data reduction technique with a tradeoff.

<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 2.2.1 and 2.2.2
par(mfrow = c(1,2)) # This tells R to put 1 row, 2 columns
stuff.to.plot <- rnorm(n = 1000, mean = 50, sd = 15)
hist(stuff.to.plot, main = "", xlab = "Score", xlim = c(0,100))
box()
abline(h = 0)

stuff.to.plot <- rnorm(n = 1000, mean = 50, sd = 8)
hist(stuff.to.plot, main = "", xlab = "Score", xlim = c(0,100))
box()
abline(h = 0)

```

*Figure(s) 2.2.1 and 2.2.2*

<Br>

<center>
Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data

| Phosphorous concentration |  Frequency |
| ---           | ---|
| 8.1 to 8.2  | 2  |
| 8.2 to 8.3  | 6  |
| 8.3 to 8.4  | 8  |
| 8.4 to 8.5  | 11 |
| 8.5 to 8.6  | 17 |
| 8.6 to 8.7  | 17 |
| 8.7 to 8.8  | 24 |
| 8.8 to 8.9  | 18 |      
| 8.9 to 9.0  | 13 |
| 9.0 to 9.1  | 10 |       
| 9.1 to 9.2  | 4  | 
Total frequency = 130 = n
</center>

*Table 2.2.1: \ A Frequency Table of Continuous Data*


<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 2.2.3
par(mfrow = c(1,1)) # This tells R to put 1 row, 2 columns
stuff.to.plot <- c(rep(8.1,2), rep(8.3,6), rep(8.4,8), rep(8.5,11), rep(8.6,17), rep(8.7,17), rep(8.8,24), rep(8.9,18), rep(9.0,13), rep(9.1,10), rep(9.2,4))
hist(stuff.to.plot, main = "", xlab = "Phosphorous (mg/g of leaf)", xlim = c(8.1,9.3), freq = T)
box()


```

*Figure 2.2.3*

<Br>

### 2.3 Cumulative Frequency Distributions


```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 2.2.3
par(mfrow = c(1,1), mar = c(4,4,1,4)) # This tells R to put 1 row, 2 columns
x <- c(8.1, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9.0, 9.1, 9.2)
y <- c(2,8,16,27,44,61,85,103,116,126,130)
y2 <- y/130
plot(x,y, main = "", xlab = "Phosphorous (mg/g of leaf)", ylab = "Cumulative Frequency", xlim = c(8.0,9.3), type = 'b',pch=16)
par(new=T)
plot(x,y2,axes=F,xlab = NA, ylab=NA, pch = NA)
axis(side=4)
mtext(side = 4, line = 2, "Relative Cummulative Frequency")
box()

```

*Figure 2.3.1*

<Br>


Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data



|                           |           |                Cummulative Frequency                 |
|---                        |---        | ---                      |  ---                      |             
| Phosphorous concentration | Frequency | Starting with Low Values | Starting with High Values |
| 8.15 to 8.25  | 2  |   2 | 130 |
| 8.25 to 8.35  | 6  |   8 | 128 |
| 8.35 to 8.45  | 8  |  16 | 122 |   
| 8.45 to 8.55  | 11 |  27 | 114 |
| 8.55 to 8.65  | 17 |  44 | 130 |
| 8.65 to 8.75  | 17 |  61 | 86  |
| 8.75 to 8.85  | 24 |  85 | 69  |
| 8.85 to 8.95  | 18 | 103 | 45  |    
| 8.95 to 9.05  | 13 | 116 | 27  |
| 9.05 to 9.15  | 10 | 126 | 14  |     
| 9.15 to 9.25  | 4  | 130 | 4   |

Total frequency = 130 = n


*Table 2.3.1: \ A Frequency Table of Continuous Data*

<Br>

### 2.4 What Interval to Choose? 

Domain knowledge 

### 2.5 Frequency Distributions 

#### Empirical vs. Theoretical

Empirical: observed finite

Theoretical: infinite number of observations


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,1,4)) # This tells R to put 1 row, 2 columns
x <- c(8.1, 8.3, 8.4, 8.5, 8.6, 8.7, 8.8, 8.9, 9.0, 9.1, 9.2)
y <- c(2,6,8,11,17,17,24,18,13,10,4)
y2 <- y/130
plot(x,y, main = "", xlab = "Phosphorous (mg/g of leaf)", ylab = "Frequency", xlim = c(8.0,9.3), type = 'b',pch=16)
par(new=T)
plot(x,y2,axes=F,xlab = NA, ylab=NA, pch = NA)
axis(side=4)
mtext(side = 4, line = 2, "Relative Frequency")
box()

```

*Figure 2.5.1*

<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-6,6, length.out = 100), y = dnorm(seq(-6,6,length.out = 100)), xlab = "X", ylab = "Y", type = "l", yaxt = 'n')


```

*Figure 2.5.2*

<Br>

#### Focus on Theoretical Distribution:

Move from determining the frequency of a value (empirical) to the relative frequency of an interval of values.

+ Relabel y-axis, why?
+ Every value of x has a non-zero probability, cannot know how often a single value occurs.
+ Sum of the probability of the obsevations is 100%.

So, we can only determine the relative frequency of an interval of values.

Use calculus to get the integral.

There are many types of theoretical distributions - we will focus on the primary ones used in frequentist statistics.

### 2.6 Everything You Ever Wanted to Know about Statistics Aims and Objectives

Know what a statistical model is and why we use them.

+ The mean 

Know what the 'fit' of a model is and why it is important.

+ The standard deviation

Distinguish models for samples and populations.

### 2.7 Pie Charts 

Try to think of other ways to display your data.

### 2.8 A Simple Statistical Model

In statistics we fit models to our data (i.e. we use a statistical model to represent what is happening in the real world).

The mean is a hypothetical value (i.e. it doesn't have to be a value that actually exists in the data set).

The mean is simple statistical model.

### 2.9 Measures of Central Tendency 


```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 4.8.1
par(mfrow = c(2,2), mar = c(2,1,1,1),cex = .5) # This tells R to put 1 row, 1 columns

x1 <- seq(-4,4, length.out = 100); y1 <- dnorm(seq(-4,4,length.out = 100))
mu <- mean(y1); medi <- median(y1); mo <- mode(y1)
plot(x = x1, y = y1, xlab = "", ylab = "", type = "l", yaxt = 'n', xaxt = 'n')
axis(1, at= mu+.2,labels = "Mean/Median/Mode")

x2 <- seq(-5,20, length.out = 2000); y2 <- .5*dnorm(x2,2,3.5)+.5*dnorm(x2,12,3.5)

plot(x = x2, y = y2, type = "l", yaxt = 'n', xaxt = 'n', ylab = "", xlab = "")    
axis(1, at= c(3, 7, 13),labels = c("Mode", "Mean/Median", "Mode"))

x3 <- seq(0,1,length.out = 1000); y3 <- dbeta(seq(0,1,length.out = 1000),2,5)
plot(x = x3, y = y3, type = "l", yaxt = 'n', xaxt = 'n', ylab = "", xlab = "")
axis(1, at= c(.2, .35, .5),labels = c("Mode", "Median", "Mean"))

x4 <- seq(0,1,length.out = 1000); y4 <- dbeta(seq(0,1,length.out = 1000),5,2)
plot(x = x4, y = y4, type = "l", yaxt = 'n', xaxt = 'n', ylab = "", xlab = "")
axis(1, at= c(.5, .65, .8),labels = c("Mean", "Median", "Mode"))

```

*Figure 2.9.1*

<Br>

### 2.10 The Mean 

The mean is the sum of all scores divided by the number of scores.

The mean is also the value from which the (squared) scores deviate least (it has the least error).

### 2.11 The Mean: Example 

Collect some data - teaching scores from students for a lecturer:

+ 1, 3, 4, 3, 2

Add them up

Divide by the number of scores, n:

### 2.12 The Mean as a Model  
  
$outcome_i = (model) + error_i$  
  
  
<!-- \begin{aligned} -->
<!-- outcome_{lecturer1}&=(\bar{X}) + error_{lecturer1}\\ -->

<!-- &=2.6+error_{lecturer1} -->
<!-- \end{aligned} -->


### 2.13 Variation: Measuring the 'Fit' of the Model

The mean is a model of what happens in the real world: the typical score.

It is not a perfect representation of the data.

How can we assess how well the mean represents reality?

### 2.14 The Dispersion: Range

The Range

+ The smallest score subtracted from the largest

Example

+ Number of friends of 11 Facebook users.
+ 22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252
+ Range = 252 - 22 = 230
+ Very biased by outliers, why?

### 2.15 The Dispersion: The Interquartile Range

Quartiles

+ The three values that split the sorted data into four equal parts.
+ Second quartile = median.
+ Lower quartile  = median of lower half of the data.
+ Upper quartile = median of upper half of the data.


### 2.16 A Perfect Fit 


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,1,4))  
x <- c(1,2,3,4,5)
y <- c(5,5,5,5,5)
plot(x,y, main = "", xlab = "Rater", ylab = "Rating (out of 5)", xlim = c(0,6), ylim = c(0,6), type = 'b',pch=16)
par(new=T)
box()

```

*Figure 2.16.1*

<Br>

### 2.17 Calculating 'Error'

A deviation is the difference between the mean and an actual data point.

Deviations can be calculated by taking each score and subtracting the mean from it:

<center>
$deviation = x_i - \bar{x}$
</center>

<Br>
<Br>
<Br>


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,1,4))  
x <- c(1,2,3,4,5)
y <- c(1.6,2,3,3,4)
plot(x,y, main = "", xlab = "Lecturer", ylab = "Number of Friends", xlim = c(0,5), ylim = c(0,4), pch=16)
par(new=T)
box()
abline(h = 2.6)
```

*Figure 2.17.1*

<Br>

### 2.18 Use the Total Error?

We could just take the error between the mean and the data and add them.

<Br>

|Score|Mean|Deviation|
|---|---|---|
| 1 | 2.6  |-1.6 |
| 2 | 2.6  |-0.6 |
| 3 | 2.6  |0.4  |
| 3 | 2.6  |0.4  |
| 4 | 2.6  |1.4  |
|   | Total = |0 |

*Table 2.18.1: \ Total error*

<Br>

<center>
$\Sigma(X - \bar{X}) = 0$
</center>

<Br>
<Br>
<Br>

### 2.19 Mean Absolute Deviation (MAD)

Also, called MAE (mean absolute error). 

<center>
$MAE = \frac{sum|x_i - \bar{x}|}{n}$
</center>

<Br>

### 2.20 Sum of Squared Errors

We could add the deviations to find out the total error.

Deviations cancel out because some are positive and others negative.

Therefore, we square each deviation.

If we add these squared deviations we get the sum of squared errors (SS).

<Br>

|Score|Mean|Deviation|Sqaured Deviation|
|---|---  |---    |---   |
| 1 | 2.6 | -1.6  | 2.56 |
| 2 | 2.6 | -0.6  | 0.36 |
| 3 | 2.6 | 0.4   | 0.16 |
| 3 | 2.6 | 0.4   | 0.16 |
| 4 | 2.6 | 1.4   | 1.96 |
|   |     | Total | 5.20 |

*Table 2.20.1: \ Sqaured Deviation*

<Br>

<center>
$SS = \Sigma(X - \bar{X})^2 = 5.20$
</center>

### 2.21 Variance

The sum of squares is a good measure of overall variability, but is dependent on the number of scores.

We calculate the average variability by dividing by the number of scores (n).

This value is called the variance (s^2^).

<center>
$varience (s^2) = \frac{SS}{N-1} = \frac{\Sigma(x_i-\bar{x})^2}{N-1} = \frac{5.20}{4} = 1.3$
</center>

<Br>

### 2.22 Standard Deviation 

The variance has one problem: it is measured in units squared.

This isn't a very meaningful metric so we take the square root value.

This is the standard deviation(s). 

$s = \sqrt\frac{\Sigma^n_{i=1}(x_i-\bar{x})^2}{n} = \sqrt\frac{5.20}{5} = 1.02$

<Br>

### 2.23 Things to Remember 

The sum of squares, variance, and standard deviation represent the same thing:

+ The fit of the mean to the data
+ The variability in the data
+ How well the mean represents the observed data
+ Error

### 2.24 Same Mean, Different Standard Deviation 


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,2), mar = c(4,4,1,0))  
x <- c(1,2,3,4,5)
y <- c(3,2,3,3,2)
plot(x,y, main = "Standard Deviation = 0.55", xlab = "Lecturer", ylab = "Overall Rating of Lecturer", xlim = c(0,5), ylim = c(0,6), pch=16)
box()
abline(h = 2.6)

par(mar = c(4,2.5,1,1))
x <- c(1,2,3,4,5)
y <- c(4,1,5,1,2)
plot(x,y, main = "Standard Deviation = 1.82", xlab = "Lecturer", ylab = NA, xlim = c(0,5), ylim = c(0,6), pch=16)
box()
abline(h = 2.6)


labels <- c(expression(paste(mu,"-3",sigma)), expression(paste(mu,"-2",sigma)))

```

*Figures 2.24.1 and 2.24.2*

<Br>

### 2.25 The Standard Deviation and the Shape of a Distribution 


```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 2.2.1 and 2.2.2
par(mfrow = c(1,2)) # This tells R to put 1 row, 2 columns
stuff.to.plot <- rnorm(n = 1000, mean = 50, sd = 15)
hist(stuff.to.plot, main = "Large Standard Deviation", xlab = "Score", xlim = c(0,100))
box()
abline(h = 0)

stuff.to.plot <- rnorm(n = 1000, mean = 50, sd = 8)
hist(stuff.to.plot, main = "Small Standard Deviation", xlab = "Score", xlim = c(0,100))
box()
abline(h = 0)

```

*Figures 2.25.1 and 2.25.2*

<Br>

### 2.26 Areas Under the Normal Curve for Various Standard Deviation 


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-8,8, length.out = 100), y = dnorm(seq(-8,8,length.out = 100)), xlab = "", ylab = "", type = "l", yaxt = 'n')
abline(v = -6)
abline(v = -4)
abline(v = -2)
abline(v = 0)
abline(v = 2)
abline(v = 4)
abline(v = 6)
#x axis labels and percentages need to be added 
```

*Figure 2.26.1*

<Br>

### 2.27 Going Beyond the Data: z-scores

z-scores

+ Standardising a score with respect to the other scores in the group.
+ Expresses a score in terms of how many standard deviations it is away from the mean.
+ The distribution of z-scores has a mean of 0 and SD = 1.

<center>
$z = \frac{X-\bar{X}}{S}$
</center>

<Br>

### 2.28 Properties of z-scores

1.96 cuts off the top 2.5% of the distribution.

-1.96 cuts off the bottom 2.5% of the distribution.

As such, 95% of z-scores lie between -1.96 and 1.96.

99% of z-scores lie between -2.58 and 2.58.

99.9% of them lie between -3.29 and 3.29. 

<Br>

# 3 Probability, Sampling, and Parameters

### 3.1 Probability, Sampling, and Parameters

+ Probability
+ Random variables
+ Sampling from a normal distribution
+ Sampling from a bionomial distribution
+ Parameter Estimation
+ Student's t-distribution

### 3.2 Probability

SETS: A collection of items. 

Element: on item of a set.

Subsets have multiple elements and are themselves sets.

Outcome set.

+ In an experiment (or other phenomenon that yields results to observe), there is a set (usually very large) of possible outcomes. Let us refer to this as the outcome set. 

Intersect - the common elements in two sets.

Mutually exclusive - Sets with no elements in common , null intersect.

Union - the combination of elements in two sets - what element is in either set or both?

Complement - the remainder of outcomes in a set that are not in a subset

#### Venn Diagram 

<!--
![Figure 3.2.1](C:/Users/w954394/Pictures/Biometry_FTP/Figures/Fund_Stats_Fig/Figure 17.png) 
-->

<Br>

### 3.3 Sampling Distributions

Random samples:

+ Every possible member of the population has an equal probability of being included in the sample.
+ Ex: scientific exit polling vs. twitter polls
+ Think of a normally distributed frequency distribution
+ Only statistically valid data that can be used for analysis - the first assumption of parametric statistics.

### 3.4 The Normal Distribution 

<center>
$Y_i = \frac{1}{\sigma\sqrt2\pi}e^-(X_i-\mu)^2/2\sigma^2$
</center>

<Br>


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-6,6, length.out = 100), y = dnorm(seq(-6,6,length.out = 100)), xlab = "X", ylab = "Y", type = "l", yaxt = 'n')

#Needs correct x axis labels on tick marks 
```

*Figure 3.4.1*

<Br>

<center>
$Y_i = \frac{1}{\sigma\sqrt2\pi}e^-(X_i-\mu)^2/2\sigma^2$
</center>

<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
# Figures 30 and 31 (went out of order)
par(mfrow = c(2,1)) # This tells R to put 1 row, 1 columns
xlim. <- c(-3.5,5.5)
plot(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)), 
     xlab = "Z", ylab = "Y", type = "l", yaxt = 'n', xlim = xlim.)
lines(x = seq(xlim.[1],xlim.[2], length.out = 100) + 1, y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)))
lines(x = seq(xlim.[1],xlim.[2], length.out = 100) + 2, y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)))


xlim. <- c(-5.5,5.5)
plot(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)), 
     xlab = "Z", ylab = "Y", type = "l", yaxt = 'n', xlim = xlim.)
lines(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100), sd = 1.5))
lines(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100), sd = 2))

#needs arrows and sigmas 
```

*Figures 3.4.2 and 3.4.3*

<Br>

### 3.5 Sample from a Normal Distribution

Normal distribution sampling theorem:

+ Sampling distribution is normal when the population distribution is normal.
+ Sample mean = population mean
+ Sample sd = population s

### 3.6 Sampling from a Non-Normal Distribution 

Central limit theorem:

+ Drawing samples from a non-normal population will result in a sampling distribution that will approximate a normal distribution when the sample size is large.
+ How can we test this - what approach would you use?

#### We will explore the sampling of distributions in the in-class assignment.

### 3.7 Parameter Estimation

#### Point estimation

Single value based on sampling

Often mean and sd

Goal: 

 1. Achieve and unbiased estimate - Long-run (infinite sampling) 
 2. Derive an efficient estimator - Fewest number samples to obtain accurate value

### 3.8 Interval Estimation

How close is the true parameter to the estimate?

Confidence intervals and limits.

# 4 Hypothesis Testing and Power

Hypothesis Testing

Power

Assumptions of parametric statistics

### 4.1 Statistical Hypothesis Testing

State:

+ H~O~
+ H~A~

Declare

+ Alpha level

Collect Data

Compare the test statistic to the critical value (determined by alpha)

State the resulting probability

State testable hypothesis

+ These are a set of mutually exclusive and exhaustive outcomes. 
+ Test statistic will support one or the other.	
+ H~O~ ~Null~  
+ H~A~ ~Alternative~

<Br>

$H_0: \mu = 0,  H_A:\mu \ne 0$

$H_0: \mu = 3.5 cm,  H_A:\mu \ne 3.5 cm$

$H_0: \mu = 10.5 kg,  H_A:\mu \ne 10.5 kg$

<Br>

### 4.2 Example: Use z-score to Test Mean

Is the mean fuel consumption of a population of busses equal to 20 mpg?

What is the null hypothesis?

We need information about the population

+ Mean
+ Population standard deviation
+ Calculate z-score
+ What is the probability that the mean is 20 mpg given:
      + Sigma = 0.3, Mean = 19.1
        
### 4.3 Evaluate z-score

What is the probability that we would get this z score?

Hypothetical z-scores

<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = NA, ylab = NA, type = "l", yaxt = 'n', xaxt = 'n')
abline(v = 1.96)

#Needs "z" on x axis 
```

*Figure 4.3.1*

<Br>
<Br>

| z | .00 | .01   | .02   | .03   | .04   | .05   | .06   | .07   | .08   | .09   |
|---|---|---|---|---|---|---|---|---|---|---|
| 0.0 | .5000 | .5040 | .5080 | .5120 | .5160 | .5199 | .5239 | .5279 | .5319 | .5359 |
| 0.1 | .5398 | .5438 | .5478 | .5517 | .5557 | .5596 | .5636 | .5675 | .5714 | .5753 |
| 0.2 | .5793 | .5832 | .5871 | .5910 | .5948 | .5987 | .6026 | .6064 | .6103 | .6141 |
| 0.3 | .6179 | .6217 | .6255 | .6293 | .6331 | .6368 | .6406 | .6443 | .6480 | .6517 |
| 0.4 | .6554 | .6591 | .6628 | .6664 | .6700 | .6736 | .6772 | .6808 | .6844 | .6879 |
| 0.5 | .6915 | .6950 | .6985 | .7019 | .7054 | .7088 | .7123 | .7157 | .7190 | .7224 |
| 0.6 | .7257 | .7291 | .7324 | .7357 | .7389 | .7422 | .7454 | .7486 | .7517 | .7549 |
| 0.7 | .7580 | .7611 | .7642 | .7673 | .7704 | .7734 | .7764 | .7794 | .7823 | .7852 |
| 0.8 | .7881 | .7910 | .7939 | .7967 | .7995 | .8023 | .8051 | .8078 | .8106 | .8133 |

*Table 4.3.1: \ Standard Normal Posibilities*

<Br>

### 4.4 Is it meaningful? Significance Level

Declare

+ Alpha level
+ p vs. alpha
+ Define prior to test
+ Two tail and one tail test

### 4.5 Alpha


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "Z", ylab = "Y", type = "l", yaxt = 'n')
abline(v = -1.96)
abline(v = 1.96)
#Needs "-1.96", "1.45", and "1.96" arrows on x axis 
```

*Figure 4.5.1*

<Br>

### 4.6 Statistical Hypothesis Testing

Ex: Look to see if the population mean is not different from some specified value.

H~O~: u = 0

H~A~: u is not equal 0

Introduce the idea of a critical value

+ Alpha level of 0.05

We have data taken from the weight change in horses given some medical treatment.

We are interested to know if the mean change in weight that we found +1.29 kg is significantly different from 0 kg.

+ We calculate the z-score and find that Z = 1.45

$P(mean \ge 1.29) = P(Z \ge 1.45) = ?$

$P(mean \le 1.29) = P(Z \le 1.45) = ?$

Z = 1.96 is the rejection region at 2.5%

+ This is the 'region of rejection'

Now we have a way to objectively reject or accept the null hypothesis.


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "Z", ylab = "Y", type = "l", yaxt = 'n')
abline(v = -1.96)
abline(v = 1.96)
#Needs "-1.96", "1.45", and "1.96" arrows on x axis 
```

*Figure 4.6.1*

<Br>

### 4.7 One- and Two-Tailed Tests

Alternative to testing 'is the value different.'

In some cases we care about the direction of the difference.

Use one-tailed test

+ In general, one-tailed hypotheses about a mean are: 
      + $H_0:\mu\ge\mu_0$ and $H_A:\mu<\mu_0$
+ In which case, H~0~ is rejected if the test statistic is in the left-hand tail of the distribution or:
      + $H_0:\mu\le\mu_0$ and $H_A:\mu>\mu_0$

Contrast the region of rejection for these.


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(2,1), mar=c(4,4,1,1)) 
plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "Z (a)", ylab = "Y", type = "l", yaxt = 'n')
abline(v = -1.96)
abline(v = 1.96)
#Needs "-1.96", "1.45", and "1.96" arrows on x axis 


plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "Z (b)", ylab = "Y", type = "l", yaxt = 'n')
abline(v = -1.645)

#Needs "-1.645" 
```

*Figures 4.7.1 and 4.7.2*

<Br>

### 4.8 Type 1 and Type 2 Errors


Sometimes we:

+ Reject the null hypothesis when it is true. 
+ Accept the alternative hypothesis when it is false.

Type 1 error or alpha error - frequency of rejecting H~0~ when it is true.

Type 1 error rate is equal to alpha.

Type 1 error: "rejecting the null hypothesis when it is true."

Type 1 error or '$\alpha$ error' is equal to $\alpha$ 

Now we have some criteria to choose alpha.

So if your $\alpha$, or critical value is 0.10

+ We have a 10% probability of rejecting the null hypothesis when we should have, in fact, accepted it.

#### Type 1 (alpha) Error

<Br>


```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 4.8.1
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns

plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "Z", ylab = "Y", type = "l", yaxt = 'n')
abline(v = c(1.96,-1.96))
#Needs "-1.96", "1.45", and "1.96" arrows on x axis 
```

*Figure 4.8.1*

<Br>

Type 2 error: "accepting the null hypothesis when it is false."

Type 2 error or '$\beta$ error' is equal to $\beta$.


<Br>


|   | If H~0~ is true | If H~0~ is false |
|---|---|---|
| If H~0~ is rejected  | Type I error   | No error   |
| If H~0~ is not rejected  | No error   | Type II error  |

*Table 4.8.1: \ Two Types of Errors in Hypothesis Testing*

<Br>

Thought experiments:

+ Ex. Endangered species conservation
+ Ex. Pharmaceutical testing 


<Br>

|   | If H~0~ is true | If H~0~ is false |
|---|---|---|
| If H~0~ is rejected  | $\alpha$   | $1-\beta$ ("power") No error   |
| If H~0~ is not rejected  | No error $1-\alpha$   | $\beta$ |

*Table 4.8.2: \ Long-term Probabilities of Outcomes in Hypothesis Testing* 

<Br>

### 4.9 Power 

Power: the probability that a statistical test will reject a null hypothesis when it is false (proper rejection).

<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 4.8.1
par(mfrow = c(2,1), mar=c(4,4,1,1)) # This tells R to put 1 row, 1 columns

plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "(a)", ylab = "Y", type = "l", yaxt = 'n', xlim = c(-4,6))
abline(v = c(1.96,-1.96))
#Needs arrows on x axis

plot(x = seq(-3,5, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "(c)", ylab = "Y", type = "l", yaxt = 'n', xlim = c(-4,6))
abline(v = c(1.96,-1.96))

```

*Figure 4.9.1*

<Br>
<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 4.8.1
par(mfrow = c(2,1), mar=c(4,4,1,1)) # This tells R to put 1 row, 1 columns

plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "(a)", ylab = "Y", type = "l", yaxt = 'n', xlim = c(-4,6))
abline(v = c(1.96,-1.96))
#Needs arrows on x axis

plot(x = seq(-2,6, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "(c)", ylab = "Y", type = "l", yaxt = 'n', xlim = c(-4,6))
abline(v = c(1.96,-1.96))

```

*Figure 4.9.2*

<Br>

#### Leaf's power simulation in R

<Br>

### 4.10 What Influences Statistical Power?

Picture 2 (Copied and pasted from book?)

### 4.11 Assumptions 

Assumptions - When broken then we are not able to make inference or accurate descriptions about reality. 

Thus our models are flawed descriptions and inferences will be compromised.

+ Assumptions of parametric tests based on the normal distribution.
+ Understand the assumption of normality.
+ Understand homogeneity of variance.
+ Know how to correct problems (with respect to the assumptions of normality) in the data.

Parametric tests based on the normal distribution assume:

+ Normally distributed
      + Distribution of samples
      + Model distribution (residuals)
        
+ Homogeneity of variance
+ Interval or ratio level data
      + Some data are intrinsically not normally distributed.
        
+ Independence of observation

### 4.12 The Normal Distribution Review

Commonly the distribution of measurements (frequency of data collected from interval data) have a bell shaped distribution

Parameters of the model determine its shape.

```{r, results='hide',message=FALSE, echo=FALSE}
# Figures 30 and 31 (went out of order)
par(mfrow = c(2,1)) # This tells R to put 1 row, 1 columns
xlim. <- c(-3.5,5.5)
plot(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)), 
     xlab = "Z", ylab = "Y", type = "l", yaxt = 'n', xlim = xlim.)
lines(x = seq(xlim.[1],xlim.[2], length.out = 100) + 1, y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)))


xlim. <- c(-5.5,5.5)
plot(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)), 
     xlab = "Z", ylab = "Y", type = "l", yaxt = 'n', xlim = xlim.)
lines(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100), sd = 1.5))
lines(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100), sd = 2))

#Needs arrows
```

*Figure 4.12.1*

<Br>

Two-parameter distribution

$f(x,\mu,\sigma)=\frac{1}{\sigma\sqrt2\pi}e^-(x-\mu)^2/2\sigma^2$

Symmetric around mu


```{r, results='hide',message=FALSE, echo=FALSE}
# Histogram for fig 4.8.1
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns

plot(x = seq(-4,4, length.out = 100), y = dnorm(seq(-4,4,length.out = 100)), xlab = "Z", ylab = "Y", type = "l", yaxt = 'n')
#Needs correct x axis
```

*Figure 4.12.2*

<Br>


### 4.13 Distribution of Samples Central Limit Theorem

How well do samples represent the population?

A key foundation of frequentist statistics - samples are random variables: When we take a sample from the population we are taking one of many possible samples.

Thought experiment - take many, many samples from a population.

Do we expect all sample to have the same mean value (the same sample mean)?

+ No, there is variation in the samples - 'Sampling variation.'

The frequency histogram of samples is the sampling distribution.

Analog to standard deviation  

+ SD how well does model fit the data

We can take the standard deviation of the sample mean.

+ Termed 'Standard Error of the Sampling mean'
+ Or 'Standard Error'

If the sample is large then sampling error can be approximated:

<center>
$SE=\frac{s}{\sqrt n}$
</center>

<Br>

### 4.14 Assumption #1: Sample observations and associated deviations are normally distributed.

We will review how to check these assumptions in this lecture and the following lab.

### 4.15 Assumption #2: Homogeneity of Variance.

Data taken from groups must have homogenous variance.

Homogenous does not mean 'equal' but equal in the probabilistic sense.

We will review how to check these assumptions in this lecture and the following lab.

### 4.16 Assumption #3: Interval and Ratio Scale

Continuous variables

+ Interval scale (equal intervals between measurements).
+ Ratio scale - Conversion of interval data such that ratio of measurements was meaningful.

Ordinal data - rankings - 

+ Darker, faster, shorter and might label these 1,2,3,4,5 to reflect increases in magnitude. 
+ Really convey less information - data condensation.

Nominal or Categorical data 

+ Example are public surveys
+ Willingness to vote for a candidate? Economic class, Taxonomic categories.

### 4.17 Assumption #4

Observations are independent

The measurement of one sample does not influence the measurement of another sample.

+ Measurements taken in space and time are examples - experimenter needs to determine when there is zero correlation between the samples.
+ Behavioral Example - the opinion of one person influences the behavior of another person and hence the measurements are correlated.

### 4.18 Assessing Normality

We don't have access to the population distribution so we usually test the observed data

Graphical displays

+ Q-Q plot (or P-P plot)
+ Histogram

Kolmogorov-Smirnov

Shapiro-Wilk

### 4.19 Assessing Homogeneity of Variance

Figures

Levene's test

+ Tests if variances in different groups are the same.
+ Significant = variances not 'equal'
+ Non-significant = variances are 'equal'

Variance ratio

+ With 2 or more groups
+ VR = largest variance/smallest variance
+ If VR < 2, homogeneity can be assumed

### 4.20 Correcting Data 'Problems'

Log transformation log(X~i~) or log(X~i~ +1)

+ Reduce positive skew.

Square root transformation:

+ Also reduces positive skew. Can also be useful for stabilizing variance.

Reciprocal transformation (1/ X~i~):

+ Dividing 1 by each score also reduces the impact of large scores. 
+ This transformation reverses the scores
+ You can avoid this by reversing the scores before the transformation, 1/(X~Highest~ - X~i~).

### 4.21 To Transform Or Not

Transforming the data helps as often as it hinders the accuracy of F 

The central limit theorem: sampling distribution will be normal in samples > 40 anyway.

+ Transforming the data changes the hypothesis being tested.
      + E.g. when using a log transformation and comparing means, you change from comparing arithmetic means to comparing geometric means.

+ In small samples it is tricky to determine normality one way or another.
The consequences for the statistical model of applying the 'wrong' transformation could be worse than the consequences of analysing the untransformed scores.
+ Alternative - use non-parametric statistics or Bayesian approaches.


# 5 Parameteric and Non-Parameteric Correlation

### 5.1 Correlation 

Linear dependence of variables

+ Scatterplots
+ Covariance
+ Pearson's correlation coefficient

Nonparametric measures

+ Spearman's rho
+ Kendall's tau

Interpreting correlations

+ Causality

Partial correlations

### 5.2 What is a Correlation?

It is a way of measuring the extent to which two variables are related.

It measures the pattern of responses across variables.

<Br>

#### Small Relationship 


```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1))  
x <- rnorm(100, 50, 35)
y <- rnorm(100, 0, 55)
plot(x,y, xlim = c(0,100), pch = 16, xlab = "x.val", ylab = "y.val")
line <- lm(y~x)
sum <- summary(line)
slope <- sum$coefficients[2,1]
b <- sum$coefficients[1,1]
curve(slope*x+b, from = min(x), to = max(x), add = T)
box()

```

*Figure 5.2.1*

<Br>

#### Positive Relationship

```{r, echo=FALSE}
library(MASS)
out <- as.data.frame(mvrnorm(100, mu = c(0,0), 
                     Sigma = matrix(c(1,0.56,0.56,1), ncol = 2), 
                    empirical = TRUE))

out$V1.s <- (out$V1 - min(out$V1))*10+10 #x
out$V2.s <- (out$V2 - min(out$V2))*20+30 #y

plot(out$V1.s,out$V2.s, pch = 16, xlab = "x.val", ylab = "y.val", xlim = c(10,60), main = "Positive Relationship")
line <- lm(out$V2.s~out$V1.s)
sum <- summary(line)
slope <- sum$coefficients[2,1]
b <- sum$coefficients[1,1]
curve(slope*x+b, from = 10, to = 60, add = T, lwd = 2)


```

*Figure 5.2.2*

<Br>

#### Negative Relationship

```{r, echo=FALSE}
library(MASS)
out <- as.data.frame(mvrnorm(100, mu = c(0,0), 
                     Sigma = matrix(c(1,-0.56,-0.56,1), ncol = 2), 
                    empirical = TRUE))

out$V1.s <- (out$V1 - min(out$V1))*10+10 #x
out$V2.s <- (out$V2 - min(out$V2))*20+30 #y

plot(out$V1.s,out$V2.s, pch = 16, xlab = "x.val", ylab = "y.val", xlim = c(10,60), main = "Negative Relationship")
line <- lm(out$V2.s~out$V1.s)
sum <- summary(line)
slope <- sum$coefficients[2,1]
b <- sum$coefficients[1,1]
curve(slope*x+b, from = 10, to = 60, add = T, lwd = 2)


```

*Figure 5.2.3*

<Br>

### 5.3 Measuring Relationships

We need to see whether as one variable increases, the other increases, decreases or stays the same.

This can be done by calculating the covariance.

+ We look at how much each score deviates from the mean.
+ If both variables deviate from the mean by the same amount, they are likely to be related.

<Br>

| Participant | 1 | 2 | 3 | 4 | 5 | Mean | SD |
|---|---|---|---|---|---|---|---|
| Adverts Watched | 5 | 4 | 4 | 6 | 8 | 5.4 | 1.67 |
| Packets Bought  | 8 | 9 | 10| 13| 15| 11  | 2.92 |

*Table 5.3.1: \ Measuring Relationships*

<Br>

| Participant | 1 | 2 | 3 | 4 | 5 | Mean | SD |
|---|---|---|---|---|---|---|---|
| Adverts Watched | 5 | 4 | 4 | 6 | 8 | 5.4 | 1.67 |
| Packets Bought  | 8 | 9 | 10| 13| 15| 11  | 2.92 |
| Advertiser Residual | -0.04 | -1.4 | -1.4 | 0.6 | 2.6 |
| Packets residual | -3 |-2 | -1 | 2 | 4 |

*Table 5.3.2: \ Measuring Relationships* 

<Br>

```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1), mar = c(4,4,1,4)) # This tells R to put 1 row, 2 columns
x <- c(1,2,3,4,5,1,2,3,4,5)
y <- c(5,3,3,6,8,8,9,10,13,14)
plot(x,y, main = "", xlab = "seq(1,5)", ylab = NA, xlim = c(0,6), ylim = c(0,20), pch=16)
par(new=T)
box()
abline(h = c(5.5, 11))

```

*Figure 5.3.1*

<Br>

### 5.4 Re-examination of Variance

The variance tells us by how much scores deviate from the mean for a single variable.

It is closely linked to the sum of squares.

Covariance is similar - it tells is by how much scores on two variables differ from their respective means.


$variance=\frac{\Sigma(x_i - \bar{x})}{N-1}$

$variance=\frac{\Sigma(x_i - \bar{x})(x_i - \bar{x})}{N-1}$


<Br>

### 5.5 Covariance

Calculate the error between the mean and each subject's score for the first variable (x).

Calculate the error between the mean and their score for the second variable (y).

Multiply these error values.

Add these values and you get the cross product deviations.

The covariance is the average cross-product deviations:

Table 1000 =

<Br>

$N_1(a_1,_1) + N_2(a_2,_1)$

$cov(x,y)=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{N-1}$

$=\frac{(-0.4)(-3)+(-1.4)(-2)+(-1.4)(-1)+(0.6)(2)+(2.6)(4)}{4}$

$=\frac{1.2+2.8+1.4+1.2+10.4}{4}$

$=\frac{17}{4}$

$=4.25$


<Br>

### 5.6 Problems with Covariance

Dependent on the units of measurement.

+ E.g. the covariance of two variables measured in miles might be 4.25, but if the same scores are converted to kilometres, the covariance is 11.

One solution: standardize it

+ Divide by the standard deviations of both variables.

The standardized version of covariance is known as the correlation coefficient.

+ It is relatively unaffected by units of measurement.

### 5.7 The Correlation Coefficient

$r=\frac{cov_xy}{s_xs_y}$

$=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{(N-1)s_xs_y}$


<Br>
<Br>


$r=\frac{cov_xy}{s_xs_y}$

$=\frac{4.25}{1.67 * 2.92}$

$=0.87$


<Br>

Termed Pearson-product moment correlation coefficient

It is a testable hypothesis

```{r, results='hide',message=FALSE, echo=FALSE}
x <- rep(seq(1,5), each = 2)
y <- c(5,7,4,9,4,10,7,13,7,15)
plot(x, y, pch = 16, ylim = c(0,20))
abline(h = 6)
abline(h = 12)

```

*Figure 5.7.1*

<Br>

It is a testable hypothesis

Testing $H_0: \rho=0$ versus $H_A: \rho\ne0$

The standard error of the correlation coefficient is calculated as:

$S_r=\sqrt\frac{1-r^2}{n-2}$

It is a testable hypothesis

r  = 0.870

n  = 12 (new data set, with more samples)

The critical value is:

<Br>

$t=\frac{r}{S_r}= \frac{0.870}{0.156}= 5.58$

t~0.05(2),10~ =2.228

Testing $H_0: \rho=0$ versus $H_A: \rho\ne0$

<Br>

It varies between -1 and +1

+ 0 = no relationship

It is an effect size

+ ?.1 = small effect
+ ?.3 = medium effect
+ ?.5 = large effect

Coefficient of determination, r2

+ By squaring the value of r you get the proportion of variance in one variable shared by the other.

### 5.8 Correlation and Causality

The third-variable problem:

+ In any correlation, causality between two variables cannot be assumed because there may be other measured or unmeasured variables affecting the results.
+ Could be many latent variables..

Direction of causality:

+ Correlation coefficients say nothing about which variable causes the other to change.

### 5.9 Non-parametric Correlation

Spearman's rho

+ Pearson's correlation on the ranked data

Kendall's tau

+ "Better" than Spearman's for small samples

### 5.10 Spearman Rank Correlation Coefficient

d is the difference between two numbers in each pair of ranks

n = number of pairs of data


$r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})$

|Data 1|Data 2|
|---|---|
|6|2|
|4|9|
|7|3|

*Table 5.10.1: \ Spearman Rank Correlation Coefficient*

<Br>

|Data 1|Data 2|Rank 1|Rank 2|d|d^2^|
|---|---|---|---|---|---|
|6|2|2|1|
|4|9|1|3|
|7|3|3|2|

*Table 5.10.2: \ Spearman Rank Correlation Coefficient*

<Br>

|Data 1|Data 2|Rank 1|Rank 2|d|d^2^|
|---|---|---|---|---|---|
|6|2|2|1|1|1|
|4|9|1|3|2|4|
|7|3|3|2|1|1|

*Table 5.10.3: \ Spearman Rank Correlation Coefficient*

<Br>

$r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})$

$=1-(\frac{6*6}{3(3^2 - 1)})$


<Br>

We can use this value as the calculated r value

The critical value is a two tailed value with n

### 5.11 Partial and Semi-partial Correlations

Partial correlation:

+ Measures the relationship between two variables, controlling for the effect that a third variable has on them both.

Semi-partial correlation:

+ Measures the relationship between two variables controlling for the effect that a third variable has on only one of the others.